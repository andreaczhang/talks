Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{perros2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.05473v1},
author = {Perros, Ioakeim and Papalexakis, Evangelos E and Park, Haesun and Vuduc, Richard and Yan, Xiaowei and Stewart, Walter F and Sun, Jimeng and Tech, Georgia and Riverside, U C and Health, Sutter},
eprint = {arXiv:1803.05473v1},
file = {:Users/andrea/Documents/Papers/perros2018.pdf:pdf},
pages = {1--23},
title = {{SUSTain : Scalable Unsupervised Scoring for Tensors and its Application to Phenotyping}},
year = {2018}
}
@article{huddar2016,
abstract = {Patients in hospitals, particularly in critical care, are susceptible to many complications affecting morbidity and mortality. Digitized clinical data in electronic medical records can be effectively used to develop machine learning models to identify patients at risk of complications early and provide prioritized care to prevent complications. However, clinical data from heterogeneous sources within hospitals pose significant modeling challenges. In particular, unstructured clinical notes are a valuable source of information containing regular assessments of the patient's condition but contain inconsistent abbreviations and lack the structure of formal documents. Our contributions in this paper are twofold. First, we present a new preprocessing technique for extracting features from informal clinical notes that can be used in a classification model to identify patients at risk of developing complications. Second, we explore the use of collective matrix factorization, a multi-view learning technique, to model heterogeneous clinical data-text-based features in combination with other measurements, such as clinical investigations, comorbidites, and demographic data. We present a detailed case study on postoperative respiratory failure using more than 700 patient records from the MIMIC II database. Our experiments demonstrate the efficacy of our preprocessing technique in extracting discriminatory features from clinical notes as well as the benefits of multi-view learning to combine clinical measurements with text data for predicting complications.},
author = {Huddar, Vijay and Desiraju, Bapu Koundinya and Rajan, Vaibhav and Bhattacharya, Sakyajit and Roy, Shourya and Reddy, Chandan K.},
doi = {10.1109/ACCESS.2016.2618775},
file = {:Users/andrea/Documents/Papers/huddar2016.pdf:pdf},
isbn = {2169-3536},
issn = {21693536},
journal = {IEEE Access},
keywords = {Clinical notes,collective matrix factorization,heterogeneous data,multi-view learning,postoperative respiratory failure,topic models},
pages = {7988--8001},
publisher = {IEEE},
title = {{Predicting Complications in Critical Care Using Heterogeneous Clinical Data}},
volume = {4},
year = {2016}
}
@article{Afshar2018,
abstract = {PARAFAC2 has demonstrated success in modeling irregular tensors, where the tensor dimensions vary across one of the modes. An example scenario is modeling treatments across a set of patients with the varying number of medical encounters over time. Despite recent improvements on unconstrained PARAFAC2, its model factors are usually dense and sensitive to noise which limits their interpretability. As a result, the following open challenges remain: a) various modeling constraints, such as temporal smoothness, sparsity and non-negativity, are needed to be imposed for interpretable temporal modeling and b) a scalable approach is required to support those constraints efficiently for large datasets. To tackle these challenges, we propose a {\{}$\backslash$it CO{\}}nstrained {\{}$\backslash$it PA{\}}RAFAC2 (COPA) method, which carefully incorporates optimization constraints such as temporal smoothness, sparsity, and non-negativity in the resulting factors. To efficiently support all those constraints, COPA adopts a hybrid optimization framework using alternating optimization and alternating direction method of multiplier (AO-ADMM). As evaluated on large electronic health record (EHR) datasets with hundreds of thousands of patients, COPA achieves significant speedups (up to 36 times faster) over prior PARAFAC2 approaches that only attempt to handle a subset of the constraints that COPA enables. Overall, our method outperforms all the baselines attempting to handle a subset of the constraints in terms of speed, while achieving the same level of accuracy. Through a case study on temporal phenotyping of medically complex children, we demonstrate how the constraints imposed by COPA reveal concise phenotypes and meaningful temporal profiles of patients. The clinical interpretation of both the phenotypes and the temporal profiles was confirmed by a medical expert.},
archivePrefix = {arXiv},
arxivId = {1803.04572},
author = {Afshar, Ardavan and Perros, Ioakeim and Papalexakis, Evangelos E. and Searles, Elizabeth and Ho, Joyce and Sun, Jimeng},
eprint = {1803.04572},
file = {:Users/andrea/Documents/Papers/Afshar2018.pdf:pdf},
isbn = {9781450360142},
keywords = {Tensor Factorization, Unsupervised Learning, Compu,computational phe-,tensor factorization,unsupervised learning},
pages = {793--802},
title = {{COPA: Constrained PARAFAC2 for Sparse {\&} Large Datasets}},
url = {http://arxiv.org/abs/1803.04572},
year = {2018}
}
@article{Rabanser2017,
abstract = {Tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions. While tensors first emerged in the psychometrics community in the {\$}20{\^{}}{\{}\backslashtext{\{}th{\}}{\}}{\$} century, they have since then spread to numerous other disciplines, including machine learning. Tensors and their decompositions are especially beneficial in unsupervised learning settings, but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis, too. The scope of this paper is to give a broad overview of tensors, their decompositions, and how they are used in machine learning. As part of this, we are going to introduce basic tensor concepts, discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition, explain the most important factorization algorithms and their properties, provide concrete examples of tensor decomposition applications in machine learning, conduct a case study on tensor-based estimation of mixture models, talk about the current state of research, and provide references to available software libraries.},
archivePrefix = {arXiv},
arxivId = {1711.10781},
author = {Rabanser, Stephan and Shchur, Oleksandr and G{\"{u}}nnemann, Stephan},
eprint = {1711.10781},
file = {:Users/andrea/Documents/Papers/Rabanser2017.pdf:pdf},
pages = {1--13},
title = {{Introduction to Tensor Decompositions and their Applications in Machine Learning}},
url = {http://arxiv.org/abs/1711.10781},
year = {2017}
}
@article{Caballero2015,
author = {Caballero, Karla and Akella, Ram},
file = {:Users/andrea/Documents/Papers/Caballero2015.pdf:pdf},
isbn = {9781450336642},
keywords = {Dynamic linear models,MIMIC,Mortality Prediction,Text mining},
mendeley-tags = {MIMIC},
pages = {69--78},
title = {{Dynamically Modeling Patient ' s Health State from Electronic Medical Records : A Time Series Approach}},
year = {2015}
}
@article{Efron2004,
abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a C p estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0406456v2},
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert and Ishwaran, Hemant and Knight, Keith and Loubes, Jean Michel and Massart, Pascal and Madigan, David and Ridgeway, Greg and Rosset, Saharon and Zhu, J. I. and Stine, Robert A. and Turlach, Berwin A. and Weisberg, Sanford and Johnstone, Iain and Tibshirani, Robert},
doi = {10.1214/009053604000000067},
eprint = {0406456v2},
isbn = {00905364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Boosting,Coefficient paths,Lasso,Linear regression,Variable selection},
number = {2},
pages = {407--499},
pmid = {1000198917},
primaryClass = {arXiv:math},
title = {{Least angle regression}},
volume = {32},
year = {2004}
}
@article{Choi2017,
abstract = {Access to electronic health record (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic patient records. Based on input real patient records, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using medGAN.},
archivePrefix = {arXiv},
arxivId = {1703.06490},
author = {Choi, Edward and Biswal, Siddharth and Malin, Bradley and Duke, Jon and Stewart, Walter F. and Sun, Jimeng},
doi = {10.1021/jp408389h},
eprint = {1703.06490},
file = {:Users/andrea/Documents/Papers/Choi2018.pdf:pdf},
issn = {1520-5215},
pages = {1--20},
pmid = {24283465},
title = {{Generating Multi-label Discrete Patient Records using Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1703.06490},
volume = {68},
year = {2017}
}
@article{Ruder2017,
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
archivePrefix = {arXiv},
arxivId = {1706.05098},
author = {Ruder, Sebastian},
doi = {10.1109/CVPR.2015.7299170},
eprint = {1706.05098},
file = {:Users/andrea/Documents/Papers/Ruder2017.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
number = {May},
pmid = {314800},
title = {{An Overview of Multi-Task Learning in Deep Neural Networks}},
url = {http://arxiv.org/abs/1706.05098},
year = {2017}
}
@article{dhamala2019,
author = {Dhamala, Jwala and Azuh, Emmanuel and Al-dujaili, Abdullah and Rubin, Jonathan and Reilly, Una-may O},
doi = {10.1109/LSENS.2018.2877920},
file = {:Users/andrea/Documents/Papers/dhamala2019.pdf:pdf},
journal = {IEEE Sensors Letters},
number = {1},
pages = {1--4},
publisher = {IEEE},
title = {{Sensor data fusion Multivariate Time-Series Similarity Assessment via Unsupervised Representation Learning and Stratified Locality Sensitive Hashing : Application to Early Acute Hypotensive Episode Detection}},
volume = {3},
year = {2019}
}
@article{angus2001,
abstract = {CONTEXT Severe sepsis, defined as infection complicated by acute organ dysfunction, occurs more frequently and leads to more deaths in black than in white individuals. The optimal approach to minimize these disparities is unclear. OBJECTIVE To determine the extent to which higher severe sepsis rates in black than in white patients are due to higher infection rates or to a higher risk of acute organ dysfunction. DESIGN, SETTING, AND PARTICIPANTS Analysis of infection-related hospitalizations from the 2005 hospital discharge data of 7 US states and infection-related emergency department visits from the 2003-2007 National Hospital Ambulatory Care Survey. MAIN OUTCOME MEASURE Age- and sex-standardized severe sepsis and infection hospitalization rates and the risk of acute organ dysfunction. RESULTS Of 8,661,227 non-childbirth-related discharges, 2,261,857 were associated with an infection, and of these, 381,787 (16.8{\%}) had severe sepsis. Black patients had a 67{\%} higher age- and sex-standardized severe sepsis rate than did white patients (9.4; 95{\%} confidence interval [CI], 9.3-9.5 vs 5.6; 95{\%} CI, 5.6-5.6 per 1000 population; P {\textless} .001) and 80{\%} higher standardized mortality (1.8, 95{\%} CI, 1.8-1.9 vs 1.0, 95{\%} CI, 1.0-1.1 per 1000 population; P {\textless} .001). The higher severe sepsis rate was explained by both a higher infection rate in black patients (47.3; 95{\%} CI, 47.1-47.4 vs 34.0; 95{\%} CI, 33.9-34.0 per 1000 population; incidence rate ratio, 1.39; P {\textless} .001) and a higher risk of developing acute organ dysfunction (age- and sex-adjusted odds ratio [OR], 1.29; 95{\%} CI, 1.27-1.30; P {\textless} .001). Differences in infection presented broadly across different sites and etiology of infection and for community- and hospital-acquired infections and occurred despite a lower likelihood of being admitted for infection from the emergency department (adjusted OR, 0.70; 95{\%} CI, 0.64-0.76; P {\textless} .001). The higher risk of organ dysfunction persisted but was attenuated after adjusting for age, sex, comorbid conditions, poverty, and hospital effect (OR, 1.14; 95{\%} CI, 1.13-1.16; P {\textless} .001). Racial disparities in infection and severe sepsis incidence and mortality rates were largest among younger adults (eg, the proportion of invasive pneumococcal disease occurring in adults {\textless} 65 years was 73.9{\%} among black patients vs 44.5{\%} among white patients, P {\textless} .001). CONCLUSION Racial differences in severe sepsis are explained by both a higher infection rate and a higher risk of acute organ dysfunction in black than in white individuals.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Angus, Derek and Linde-Zwirble, Walter and Lidicker, Jeffrey and Clermont, Gilles and Carcillo, Joseph and Pinsky, MIchael},
doi = {10.1016/j.immuni.2014.04.001},
eprint = {NIHMS150003},
file = {:Users/andrea/Documents/Papers/angus2001.pdf:pdf},
isbn = {9780323446112},
issn = {15578216},
journal = {Clinics in Chest Medicine},
keywords = {*Cause of Death,08,10,2,2016,21037,23,59,80 and over,ARTICLE,Acute Disease,Adaptive Immunity,Adjunctive,Adjuvant,Adjuvant therapy,Adolescent,Adult,Advisory Committees,Africa,African Continental Ancestry Group,Age Distribution,Aged,Anergy,Animal,Animals,Anti-Infective Agents,Anti-Infective Agents: therapeutic use,Anti-PD-1,Anti-PD-L1,Anti-inflammatory,Antibiotics,Antibodies,Antibodies: administration {\&} dosage,Anticoagulants,Antigen presentation,Antimicrobial stewardship,Automated alert systems,Bacteremia,Bacteria,Bacterial toxins,Benchmarking,Biologic response modifier,Biomarker,Biomarkers,Biomarkers: blood,Blood Coagulation,Blood Coagulation: immunology,Blood Pressure Determination,Bloodstream infections,C-reactive protein,CARE,CD64,Case-Control Studies,Cell Line,Cellular,Cellular: immunology,Classification,Clinical Trials,Clinical trials,Co-inhibitory molecules,Coagulation,Cohort Studies,Combined Modality Therapy,Comorbidity,Compensatory anti-inflammatory response syndrome,Compensatory anti-inflammatory response syndrome (,Complement,Complementary,Complementary: genetics,Consensus,Critical Care,Critical Care: methods,Critical Care: standards,Critical Care: trends,Critical Illness,Critical care,Cross Infection,Cross Infection: epidemiology,Cross Infection: immunology,Cytokines,Cytokines: secretion,D-dimer,DNA,Databases,Delphi Technique,Diagnosis,Disease Models,Dysfunction,Early Diagnosis,Early goal-directed therapy,EdU,Emergency Service,Emergency services,Endotoxin,Epidemiology,Europe,European Continental Ancestry Group,Exome,Factual,Female,Flow Cytometry,Flow cytometry,Flt3 ligand,Fluid Therapy,Fluids,Genome-wide association study,Global Health,Glycocalyx,Glycosaminoglycans,Guidelines,HLA-DR,HLA-DR Antigens,HLA-DR Antigens: biosynthesis,HMGB1 Protein,HMGB1 Protein: antagonists {\&} inhibitors,HMGB1 Protein: blood,HMGB1 Protein: genetics,Health Promotion,Health Status Disparities,Heart,Heart: physiopathology,Heat-shock proteins,Heparan sulfate,Hospital,Hospital Mortality/*trends,Hospital/utilization,Host response,Human leukocyte antigen,Humans,Hypotension,Hypotension: diagnosis,ICU transfer,IFN-??,IFN-$\gamma$,IL-6,Immune System,Immune Tolerance,Immune cells,Immune dysfunction,Immune system,Immunity,Immunoglobulins,Immunohistochemistry,Immunological,Immunomodulatory agents,Immunoparalysis,Immunosuppression,Immunotherapy,In Vitro Techniques,Incidence,Infection,Infection: complications,Infection: epidemiology,Inflammation,Inflammation: immunology,Innate,Innate immunity,Inpatients,Inpatients: statistics {\&} numerical data,Intensive Care,Intensive Care Units,Intensive Care Units/utilization,Intensive care,Intensive care unit,Interleukin-15,Interleukin-7,Interleukins,International Classification of Diseases,Interventions,Intravital microscopy,Investigational,Kidney,Kidney: physiopathology,Kinetics,Lactate,Lactates,Lactates: blood,Leukocytes,Logistic Models,Lung,Lung: cytology,Lymphocyte,Lymphocytes,MHLA-DR,Macrophages,Male,Mediators,Medical,Methods,Mice,Microarrays,Microcirculation,Middle Aged,Models,Monocytes,Monocytes: immunology,Mortality,Multiple Organ Failure,Multiple Organ Failure: drug therapy,Multiple Organ Failure: etiology,Multiple Organ Failure: immunology,Multiple Organ Failure: mortality,Multiple Organ Failure: therapy,Multiple Trauma,Multiple Trauma: complications,Multiple Trauma: immunology,Multivariate Analysis,Myocardial depression,Neutralization Tests,Neutrophils,North America,Nosocomial infection,Nucleic acid amplification,Observational Studies as Topic,Organ failure,Outcomes,PD-1,Patient outcomes,Pediatric,Peptide Fragments,Peptide Fragments: chemistry,Peptide Fragments: genetics,Physicians,Polymorphism,Polytrauma,Population stratification,Postoperative Complications,Postoperative Complications: immunology,Practice Guidelines as Topic,Procalcitonin,Proinflammatory,Proliferation,Protein C,Protein C: therapeutic use,Randomized Controlled Trials as Topic,Rapid diagnostics,Rare single nucleotide variation,Recombinant Proteins,Recombinant Proteins: therapeutic use,Recommendations,Registries,Registries: statistics {\&} numerical data,Research,Resistance markers,Resource-limited,Respiratory Insufficiency,Resuscitation,Resuscitation: methods,Retrospective Studies,Review,Review Literature as Topic,Rhode Island,Risk Factors,Risk assessment,Risk factors,SIRS,Semantic set covering machine,Sensitivity,Sensitivity and specificity,Sepsis,Sepsis bundles,Sepsis management,Sepsis performance improvement,Sepsis syndrome,Sepsis/diagnosis/*mortality/therapy,Sepsis: blood,Sepsis: complications,Sepsis: diagnosis,Sepsis: drug therapy,Sepsis: epidemiology,Sepsis: ethnology,Sepsis: etiology,Sepsis: immunology,Sepsis: mortality,Sepsis: therapy,Septic,Septic shock,Septic/diagnosis/mortality/therapy,Septic: blood,Septic: diagnosis,Septic: epidemiology,Septic: etiology,Septic: physiopathology,Septic: therapy,Septicemia,Severe sepsis,Sex Distribution,Shock,Societies,Specificity,Spleen,Spleen: cytology,Staging,Study of diagnostic accuracy,Survival Rate,Surviving sepsis campaign,Syndrome,Systemic Inflammatory Response Syndrome,Systemic inflammation,Systemic inflammatory response syndrome,Systemic inflammatory response syndrome (SIRS),T cell,T/B lymphocytes,Terminology,The JAMA Network,Therapeutics,Therapies,Therapy,Toll-like receptors,Treatment,Treatment Outcome,Tregs,Tumor necrosis factor,United States,United States/epidemiology,United States: epidemiology,Universities,Vasoconstrictor Agents,Vasoconstrictor Agents: therapeutic use,Vasodilation,Young Adult,[3H]-Thymidine,abbreviations,accepted for publication aug,acute kidney injury,adenine nucleotide translocase,adenosine diphosphate,adenosine triphosphate,adp,anergy,ant,anti-inflammatory cytokines leukocytes,atm,atp,beneficial effects of cytokine,biogenesis,biomarkers,blockade,burden,cell cycle,compensatory anti-,compensatory anti-inflammatory response syndrome (,complications,cyp6d1 transcription,cytokines,damp,danger associated molecular pattern,depending on a,development,diagnosis,diagnostics,doi,due to a,dx,epidemiology,fadh 2,flavin adenine dinucleotide,flow cytometry,granulocyte-,granulocyte-macrophage colony–stimulating factor,however,hr96,http,ifn- g,immune monitoring,immune system,immune-pathophysiology,immunoparalysis,immunosuppression,in animal models of,infection,inflammation,inflammatory mediators,inflammatory reflex,inflammatory response syndrome,innate immunity,investigated because of the,is an extensive clinical,macrophage colony,microbial infection,microbiology,microcirculation,mitochondria,mitophagy,mof,multi-organ failure,musca domestica,nadh,neutrophils,nicotinamide adenine dinucleotide,nitric oxide,no,org,organ dysfunction,pathogen-host interaction,pgc-1 $\alpha$,phenobarbital induction,ppar $\gamma$ -coactivator-1 $\alpha$,problem with,pyrethroid resistance,reactive oxygen species,review,risk factors,ros,sepsis,sepsis antibiotics treatment therapeutic,sepsis systemic inflammatory response,septic shock,severe sepsis,significant mortality and economic,sirs,stimulating factor,submitted jul 31,syndrome proinflammatory,systemic,systemic inflammatory response syndrome (SIRS),targets,the systemic inflammatory response,the traditional approach,ucp,uncoupling protein,vagus nerve,view this article at},
number = {4},
pages = {463--475},
pmid = {24335434},
title = {{Epidemiology of severe sepsis in the United States: Analysis of incidence, outcome, and associated costs of care}},
url = {http://dx.doi.org/10.1016/j.molmed.2014.02.001{\%}5Cnhttp://dx.doi.org/10.1016/j.immuni.2014.04.001{\%}5Cnhttp://dx.doi.org/10.1016/j.ccm.2016.01.015{\%}5Cnhttp://ccforum.biomedcentral.com/articles/10.1186/s13054-016-1274-9{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/27},
volume = {37},
year = {2016}
}
@article{Ratanamahatana2010,
abstract = {Because time series are a ubiquitous and increasingly prevalent type of data, there has been much research effort devoted to time series data mining in recent years. As with all data mining problems, the key to effective and scalable algorithms is choosing the right representation of the data. Many high level representations of time series have been proposed for data mining, including spectral transforms, wavelets, Singular Value Decomposition (SVD), piecewise polynomial models, symbolic models, etc. In this work, we introduce a new technique based on a bit level approximation of the data. The representation has several important advantages over existing techniques. One unique advantage is that it allows raw data to be directly compared to the reduced representation, while still guaranteeing lower bounds to either Euclidean distance or DTW. This fact can be exploited to produce faster exact algorithms for similarly search. In addition, we demonstrate that our new representation allows time series clustering to scale to much larger datasets.},
author = {Ratanamahatana, Chotirat and Keogh, Eamonn and Bagnall, Anthony J. and Lonardi, Stefano},
doi = {10.1007/11430919_90},
file = {:Users/andrea/Documents/Papers/ratanamahatatna2005.pdf:pdf},
isbn = {0000000000000},
keywords = {similarity,time series representation},
pages = {771--777},
title = {{A Novel Bit Level Time Series Representation with Implication of Similarity Search and Clustering}},
year = {2010}
}
@article{Yoon2013,
abstract = {(Received 2 November 2011; final version received 21 February 2012) Penalized regression methods have recently gained enormous attention in statistics and the field of machine learning due to their ability of reducing the prediction error and identifying important variables at the same time. Numerous studies have been conducted for penalized regression, but most of them are limited to the case when the data are independently observed. In this paper, we study a variable selection problem in penalized regression models with autoregressive (AR) error terms.We consider three estimators, adaptive least absolute shrinkage and selection operator, bridge, and smoothly clipped absolute deviation, and propose a computational algorithm that enables us to select a relevant set of variables and also the order ofAR error terms simultaneously. In addition, we provide their asymptotic properties such as consistency, selection consistency, and asymptotic normality. The performances of the three estimators are compared with one another using simulated and real examples.},
author = {Yoon, Young Joo and Park, Cheolwoo and Lee, Taewook},
doi = {10.1080/00949655.2012.669383},
issn = {00949655},
journal = {Journal of Statistical Computation and Simulation},
keywords = {asymptotic normality,autoregressive error models,consistency,oracle property,penalized regression,variable selection},
number = {9},
pages = {1756--1772},
title = {{Penalized regression models with autoregressive error terms}},
volume = {83},
year = {2013}
}
@article{Simon2012,
abstract = {We re-examine the original Group Lasso paper of Yuan and Lin (2007). The form of penalty in that paper seems to be designed for problems with uncorrelated features, but the statistical community has adopted it for general problems with correlated features. We show that for this general situation, a Group Lasso with a different choice of penalty matrix is generally more effective. We give insight into this formulation and show that it is intimately related to the uniformly most powerful invariant test for inclusion of a group. We demonstrate the efficacy of this method– the “standardized Group Lasso”– over the usual group lasso on real and simulated data sets. We also extend this to the Ridged Group Lasso to provide within group regularization as needed. We discuss a simple algorithm based on group-wise coordinate descent to fit both this standardized Group Lasso and Ridged Group Lasso.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Simon, Noah and Tibshirani, Robert},
doi = {10.5705/ss.2011.075},
eprint = {15334406},
isbn = {0000000000000},
issn = {10170405},
journal = {Statistica Sinica},
number = {3},
pmid = {24655651},
title = {{Standardization and the Group Lasso Penalty}},
url = {http://www3.stat.sinica.edu.tw/statistica/j22n3/J22N34/J22N34.html},
volume = {22},
year = {2012}
}
@article{Prasser2019,
abstract = {Background: Modern data-driven approaches to medical research require patient-level information at comprehensive depth and breadth. To create the required big datasets, information from disparate sources can be integrated into clinical and translational warehouses. This is typically implemented with Extract, Transform, Load (ETL) processes, which access, harmonize and upload data into the analytics platform. Objective: Privacy-protection needs careful consideration when data is pooled or re-used for secondary purposes, and data anonymization is an important protection mechanism. However, common ETL environments do not support anonymization, and common anonymization tools cannot easily be integrated into ETL workflows. The objective of the work described in this article was to bridge this gap. Methods: Our main design goals were (1) to base the anonymization process on expert-level risk assessment methodologies, (2) to use transformation methods which preserve both the truthfulness of data and its schematic properties (e.g. data types), (3) to implement a method which is easy to understand and intuitive to configure, and (4) to provide high scalability. Results: We designed a novel and efficient anonymization process and implemented a plugin for the Pentaho Data Integration (PDI) platform, which enables integrating data anonymization and re-identification risk analyses directly into ETL workflows. By combining different instances into a single ETL process, data can be protected from multiple threats. The plugin supports very large datasets by leveraging the streaming-based processing model of the underlying platform. We present results of an extensive experimental evaluation and discuss successful applications. Conclusions: Our work shows that expert-level anonymization methodologies can be integrated into ETL workflows. Our implementation is available under a non-restrictive open source license and it overcomes several limitations of other data anonymization tools.},
author = {Prasser, Fabian and Spengler, Helmut and Bild, Raffael and Eicher, Johanna and Kuhn, Klaus A.},
doi = {10.1016/j.ijmedinf.2019.03.006},
file = {:Users/andrea/Documents/Papers/prasser2019.pdf:pdf},
issn = {18728243},
journal = {International Journal of Medical Informatics},
keywords = {Anonymization,Clinical data warehousing,Extract Transform Load,Privacy},
number = {November 2018},
pages = {72--81},
title = {{Privacy-enhancing ETL-processes for biomedical data}},
volume = {126},
year = {2019}
}
@article{Raghu2017,
abstract = {Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patient's physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patient's physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6{\%} over observed clinical policies, from a baseline mortality of 13.7{\%}. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.},
archivePrefix = {arXiv},
arxivId = {1705.08422},
author = {Raghu, Aniruddh and Komorowski, Matthieu and Celi, Leo Anthony and Szolovits, Peter and Ghassemi, Marzyeh},
doi = {10.7641/CTA.2016.60173},
eprint = {1705.08422},
file = {:Users/andrea/Documents/Papers/Raghu2017.pdf:pdf},
isbn = {0340-2592 (Print) 0340-2592 (Linking)},
issn = {10008152},
pages = {1--17},
pmid = {9646429},
title = {{Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach}},
url = {http://arxiv.org/abs/1705.08422},
year = {2017}
}
@article{Salciccioli2015,
abstract = {IntroductionThe neutrophil-to-lymphocyte ratio (NLR) is a biological marker that has been shown to be associated with outcomes in patients with a number of different malignancies. The objective of this study was to assess the relationship between NLR and mortality in a population of adult critically ill patients.MethodsWe performed an observational cohort study of unselected intensive care unit (ICU) patients using a large clinical database. We computed individual patient NLR and categorized patients by quartile of this ratio. The association of NLR quartiles and 28-day mortality was assessed using multivariable logistic regression. Secondary outcomes included mortality in the ICU, in-hospital mortality and 1-year mortality. An a priori sub-group analysis of patients with and without sepsis was performed to assess any differences in the relationship between the NLR and outcomes in these cohorts.ResultsA total of 5,056 patients were included with a 28-day mortality rate of 19{\%}. The median age of the cohort was 65 years and 47{\%} were female. The median NLR for the entire cohort was 8.9 (interquartile range: 4.99 to 16.21). Following multivariable adjustments, there was a stepwise increase in mortality with increasing quartiles of NLR (1st quartile: reference category; 2nd quartile odds ratio (OR)¿=¿1.36, 95{\%} confidence interval (CI): 1.06 to 1.74; 3rd quartile OR¿=¿1.47, 95{\%} CI: 1.15 to 1.89; 4th quartile OR 1.78, 95{\%} CI: 1.41 to 2.25). A similar stepwise relationship was identified in the subgroup of patients who presented without sepsis. The NLR was not associated with 28-day mortality in patients with sepsis. Increasing quartile of NLR was statistically significantly associated with secondary outcomes.ConclusionThe neutrophil-to-lymphocyte ratio is associated with outcomes in unselected critically ill patients. In patients with sepsis there was no statistically significant relationship between NLR and mortality. Further investigation is required to understand the pathophysiology of this relationship and to validate these findings with data collected prospectively.},
author = {Salciccioli, Justin D. and Marshall, Dominic C. and Pimentel, Marco A.F. and Santos, Mauro D. and Pollard, Tom and Celi, Anthony A. and Shalhoub, Joseph},
doi = {10.1186/s13054-014-0731-6},
file = {:Users/andrea/Documents/Papers/salciccioli2015.pdf:pdf},
isbn = {1364-8535},
issn = {1466609X},
journal = {Critical Care},
number = {1},
pages = {1--8},
pmid = {25598149},
title = {{The association between the neutrophil-to-lymphocyte ratio and mortality in critical illness: An observational cohort study}},
volume = {19},
year = {2015}
}
@article{Mairal2009,
abstract = {Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.},
annote = {WWHA
What (Contribution, conclusion)
- new online optimisation algo for dictionary learning, based on stochastic approximations
- learning basis sets

Why
- effective in image processing 

How
- new algorithms
- compare with batch learning and stocastic gradient


Application /example: 
- natural image (denoising, debluring, inprinting)
- video restoration
- image classification},
archivePrefix = {arXiv},
arxivId = {0908.0050},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
doi = {10.1145/1553374.1553463},
eprint = {0908.0050},
file = {::},
isbn = {9781605585161},
issn = {0016450X},
journal = {Proceedings of the 26th International Conference on Machine Learning},
pages = {1--8},
pmid = {710806},
title = {{Online dictionary learning for sparse coding}},
url = {http://dl.acm.org/citation.cfm?id=1553463},
year = {2009}
}
@article{karpathy2015,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {1506.02078},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1506.02078},
file = {:Users/andrea/Documents/Papers/Karpathy2015.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
pages = {1--12},
pmid = {26353135},
title = {{Visualizing and Understanding Recurrent Networks}},
url = {http://arxiv.org/abs/1506.02078},
year = {2015}
}
@article{Basu2015,
abstract = {Many scientific and economic problems involve the analysis of high-dimensional time series datasets. However, theoretical studies in high-dimensional statistics to date rely primarily on the assumption of independent and identically distributed (i.i.d.) samples. In this work, we focus on stable Gaussian processes and investigate the theoretical properties of {\$}\backslashell {\_}1{\$}-regularized estimates in two important statistical problems in the context of high-dimensional time series: (a) stochastic regression with serially correlated errors and (b) transition matrix estimation in vector autoregressive (VAR) models. We derive nonasymptotic upper bounds on the estimation errors of the regularized estimates and establish that consistent estimation under high-dimensional scaling is possible via {\$}\backslashell{\_}1{\$}-regularization for a large class of stable processes under sparsity constraints. A key technical contribution of the work is to introduce a measure of stability for stationary processes using their spectral properties that provides insight into the effect of dependence on the accuracy of the regularized estimates. With this proposed stability measure, we establish some useful deviation bounds for dependent data, which can be used to study several important regularized estimates in a time series setting.},
annote = {the effect of both AR error and VAR (dependence) on lasso, hence this is a very important paper, error bound

introduced measure of stability using spectral properties (this not so useful for me)

has cited some interesting applications in biomedicine

consistency of lasso under some restrictions, and the contribution of this paper is to study this with time dependency

inspiring for simulation.},
archivePrefix = {arXiv},
arxivId = {1311.4175},
author = {Basu, Sumanta and Michailidis, George},
doi = {10.1214/15-AOS1315},
eprint = {1311.4175},
isbn = {9823010102},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Covariance estimation,High-dimensional time series,Lasso,Stochastic regression,Vector autoregression},
number = {4},
pages = {1535--1567},
title = {{Regularized estimation in sparse high-dimensional time series models}},
volume = {43},
year = {2015}
}
@article{wang2018,
abstract = {Skin cancer, the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs) show potential for general and highly variable tasks across many fine-grained object categories. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images-two orders of magnitude larger than previous datasets-consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
author = {Wang, Fei and Casalino, Lawrence Peter and Khullar, Dhruv},
doi = {10.1038/nature21056},
file = {:Users/andrea/Documents/Papers/wang2018.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
number = {7639},
pages = {115--118},
pmid = {28117445},
title = {{Deep learning in Medicine - promiss, progress, and challenges}},
volume = {542},
year = {2017}
}
@article{Wu2009,
abstract = {MOTIVATION: In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations. METHOD: The present article evaluates the performance of lasso penalized logistic regression in case-control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression. RESULTS: This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs. AVAILABILITY: The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
archivePrefix = {arXiv},
arxivId = {The Stata Journal},
author = {Wu, Tong Tong and Chen, Yi Fang and Hastie, Trevor and Sobel, Eric and Lange, Kenneth},
doi = {10.1093/bioinformatics/btp041},
eprint = {The Stata Journal},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {6},
pages = {714--721},
pmid = {19176549},
title = {{Genome-wide association analysis by lasso penalized logistic regression}},
volume = {25},
year = {2009}
}
@article{Zhan2018,
abstract = {{\textcopyright} 2018 IEEE. Along with the continuous development of infor- mation technology, massive numbers of detecting instruments or systems in various fields are continuously producing plenty number of streaming time series data. In recent years, many representation approaches for time series has been proposed with the main objective of dimensionality reduction to support various data mining algorithms in the domain of time series data processing. Symbolic Aggregate approXimation(SAX) is a major symbolic representation and dimensionality reduction algorithm which has been widely used in many application scenarios of time series data mining, such as motifs discov- ery,outlier detection, etc. In this paper, we propose a symbolic representation method of streaming time series based on VTP- diving with sliding window and a similarity measurement algorithm for the proposed representation method which lower bounding the Euclidean distance on the original data. Extensive experiments on different kinds of typical time series datasets have been conducted to demonstrate the advantages of our proposed method.},
author = {Zhan, Peng and Hu, Yupeng and Zhang, Qi and Zheng, Jiecai and Li, Xueqing},
doi = {10.1109/ITME.2018.00184},
file = {:Users/andrea/Documents/Papers/zhan2018.pdf:pdf},
isbn = {9781538677438},
journal = {Proceedings - 9th International Conference on Information Technology in Medicine and Education, ITME 2018},
keywords = {Dimensionality Reduction,Streaming Data Processing,Symbolic Aggregate Approximation,Time Series},
pages = {817--823},
publisher = {IEEE},
title = {{Feature-Based Dividing Symbolic Time Series Representation for Streaming Data Processing}},
year = {2018}
}
@article{Ruffini2017,
abstract = {In this paper we present a method for the unsupervised clustering of high-dimensional binary data, with a special focus on electronic healthcare records. We present a robust and efficient heuristic to face this problem using tensor decomposition. We present the reasons why this approach is preferable for tasks such as clustering patient records, to more commonly used distance-based methods. We run the algorithm on two datasets of healthcare records, obtaining clinically meaningful results.},
archivePrefix = {arXiv},
arxivId = {1708.08994},
author = {Ruffini, Matteo and Gavald{\`{a}}, Ricard and Lim{\'{o}}n, Esther},
doi = {10.1002/dei},
eprint = {1708.08994},
file = {:Users/andrea/Documents/Papers/Ruffini2017.pdf:pdf},
issn = {1938-7228},
title = {{Clustering Patients with Tensor Decomposition}},
url = {http://arxiv.org/abs/1708.08994},
volume = {68},
year = {2017}
}
@article{Bernanke1998,
abstract = {We develop a model-based, VAR methodology for measuring innovations in monetary policy and their macroeconomic effects. Using this framework, we are able to compare existing approaches to measuring monetary policy shocks and derive a new measure of policy innovations based directly on (possibly time- varying) estimates of the central bank's operating procedures. We also propose a new measure of the overall stance of policy (including the endogenous or systematic component) that is consistent with our approach.},
annote = {most applications are limited to 6 series},
author = {Bernanke, Ben S. and Mihov, Illian},
doi = {10.1162/003355398555775},
isbn = {0033-5533, 1531-4650},
issn = {0033-5533},
journal = {The Quarterly Journl of Economics},
keywords = {VAR},
mendeley-tags = {VAR},
number = {3},
pages = {869--902},
pmid = {949498},
title = {{Measuring Monetary Policy}},
url = {http://www.jstor.org/stable/pdf/2586876.pdf?{\_}=1459493708610},
volume = {113},
year = {1998}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {:Users/andrea/Documents/Papers/Bengio2014.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
number = {8},
pages = {1798--1828},
pmid = {23459267},
title = {{Representation learning: A review and new perspectives}},
volume = {35},
year = {2013}
}
@article{Rush2017,
abstract = {The impact of chronic exposure to air pollution and outcomes in the acute respiratory distress syndrome (ARDS) is unknown. The Nationwide Inpatient Sample (NIS) from 2011 was utilized for this analysis. The NIS is a national database that captures 20{\%} of all US in-patient hospitalizations from 47 states. Patients with ARDS who underwent mechanical ventilation from the highest 15 ozone pollution cities were compared with the rest of the country. Secondary analyses assessed outcomes of ARDS patients for ozone pollution and particulate matter pollution on a continuous scale by county of residence. A total of 8,023,590 hospital admissions from the 2011 NIS sample were analyzed. There were 93,950 patients who underwent mechanical ventilation for ARDS included in the study. Patients treated in high ozone regions had significantly higher unadjusted hospital mortality (34.9{\%} versus 30.8{\%}, p {\textless} 0.01) than patients in cities with control levels of ozone. After controlling for all variables in the model, treatment in a hospital located in a high ozone pollution area was associated with an increased odds of in-hospital mortality (OR 1.11, 95{\%} CI 1.08–1.15, p {\textless} 0.01). After adjustment for all variables in the model, for each increase in ozone exposure by 0.01 ppm the OR for death was 1.07 (95{\%} CI 1.06–1.08, p {\textless} 0.01). Similarly, for each increase in particulate matter exposure by 10 $\mu$g/m3, the OR for death was 1.08 (95{\%} CI 1.02–1.16, p {\textless} 0.01). Chronic exposure to both ozone and particulate matter pollution is associated with higher rates of mortality in ARDS. These preliminary findings need to be confirmed by further detailed studies.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Rush, Barret and McDermid, Robert C. and Celi, Leo Anthony and Walley, Keith R. and Russell, James A. and Boyd, John H.},
doi = {10.1016/j.envpol.2017.02.014},
eprint = {15334406},
file = {:Users/andrea/Documents/Papers/rush2017.pdf:pdf},
isbn = {1873-64240269-7491},
issn = {18736424},
journal = {Environmental Pollution},
pages = {352--356},
pmid = {28202265},
publisher = {Elsevier Ltd},
title = {{Association between chronic exposure to air pollution and mortality in the acute respiratory distress syndrome}},
url = {http://dx.doi.org/10.1016/j.envpol.2017.02.014},
volume = {224},
year = {2017}
}
@article{Seth2013,
abstract = {Granger causality is a method for identifying directed functional connectivity based on time series analysis of precedence and predictability. The method has been applied widely in neuroscience, however its application to functional MRI data has been particularly controversial, largely because of the suspicion that Granger causal inferences might be easily confounded by inter-regional differences in the hemodynamic response function. Here, we show both theoretically and in a range of simulations, that Granger causal inferences are in fact robust to a wide variety of changes in hemodynamic response properties, including notably their time-to-peak. However, when these changes are accompanied by severe downsampling, and/or excessive measurement noise, as is typical for current fMRI data, incorrect inferences can still be drawn. Our results have important implications for the ongoing debate about lag-based analyses of functional connectivity. Our methods, which include detailed spiking neuronal models coupled to biophysically realistic hemodynamic observation models, provide an important 'analysis-agnostic' platform for evaluating functional and effective connectivity methods. {\textcopyright} 2012 Elsevier Inc..},
annote = {application. just read introduction and conclusion},
author = {Seth, Anil K. and Chorley, Paul and Barnett, Lionel C.},
doi = {10.1016/j.neuroimage.2012.09.049},
isbn = {1095-9572},
issn = {10538119},
journal = {NeuroImage},
keywords = {Computational modeling,Functional MRI,Functional connectivity,Granger causality,Hemodynamic response function},
pages = {540--555},
pmid = {23036449},
title = {{Granger causality analysis of fMRI BOLD signals is invariant to hemodynamic convolution but not downsampling}},
volume = {65},
year = {2013}
}
@article{Karoui2018,
abstract = {We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, wherep {\textless} n but p=n is not close to zero.  We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement:  can the bootstrap give us good condence intervals for a single coordinate of(where is the true regression vector)? We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems.  Both of the most commonly used methods of bootstrapping for regression| residual  bootstrap  and  pairs  bootstrap|give  very  poor  inference  onas  the  ratio p=n grows. We nd that the residual bootstrap tend to give anti-conservative estimates (in
ated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio p=n grows.  We also show that the jackknife resampling technique for estimating the variance of{\^{}} severely overestimates the variance in high dimensions. We  contribute  alternative  procedures  based  on  our  theoretical  results  that  result  in dimensionality adaptive and robust bootstrap methods.},
author = {Karoui, Noureddine El and Purdom, Elizabeth},
file = {:Users/andrea/Documents/Papers/karoui2018.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bootstrap,high-dimensional inference,random matrices,resampling},
number = {5},
pages = {1--66},
title = {{Can We Trust the Bootstrap in High-dimensions? The Case of Linear Models}},
url = {http://jmlr.org/papers/v19/17-006.html},
volume = {19},
year = {2018}
}
@article{Wu2017,
abstract = {Background The widespread adoption of electronic health records allows us to ask evidence-based questions about the need for and benefits of specific clinical interventions in critical-care settings across large populations. Objective We investigated the prediction of vasopressor administration and weaning in the intensive care unit. Vasopressors are commonly used to control hypotension, and changes in timing and dosage can have a large impact on patient outcomes. Materials and Methods We considered a cohort of 15 695 intensive care unit patients without orders for reduced care who were alive 30 days post-discharge. A switching-state autoregressive model (SSAM) was trained to predict the multidimensional physiological time series of patients before, during, and after vasopressor administration. The latent states from the SSAM were used as predictors of vasopressor administration and weaning. Results The unsupervised SSAM features were able to predict patient vasopressor administration and successful patient weaning. Features derived from the SSAM achieved areas under the receiver operating curve of 0.92, 0.88, and 0.71 for predicting ungapped vasopressor administration, gapped vasopressor administration, and vasopressor weaning, respectively. We also demonstrated many cases where our model predicted weaning well in advance of a successful wean. Conclusion Models that used SSAM features increased performance on both predictive tasks. These improvements may reflect an underlying, and ultimately predictive, latent state detectable from the physiological time series.},
author = {Wu, Mike and Ghassemi, Marzyeh and Feng, Mengling and Celi, Leo A. and Szolovits, Peter and Doshi-Velez, Finale},
doi = {10.1093/jamia/ocw138},
file = {:Users/andrea/Documents/Papers/Wu2017.pdf:pdf},
isbn = {1527-974X (Electronic) 1067-5027 (Linking)},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {Autoregressive models,Electronic health records,Latent variable models,MIMIC,Risk prediction},
mendeley-tags = {MIMIC},
number = {3},
pages = {488--495},
pmid = {27707820},
title = {{Understanding vasopressor intervention and weaning: Risk prediction in a public heterogeneous clinical time series database}},
url = {https://watermark.silverchair.com/ocw138.pdf?token=AQECAHi208BE49Ooan9kkhW{\_}Ercy7Dm3ZL{\_}9Cf3qfKAc485ysgAAAkIwggI-BgkqhkiG9w0BBwagggIvMIICKwIBADCCAiQGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMKXlgby1-JPOy1bXwAgEQgIIB9VWMUZM-wJVFvNqU6lkpLKpvvqCNmUUTeYawfaw0H9vTLbB9},
volume = {24},
year = {2017}
}
@inproceedings{Jenatton2011a,
abstract = {Inverse inference, or "brain reading", is a recent paradigm for analyzing functional magnetic resonance imaging (fMRI) data, based on pattern recognition tools. By predicting some cognitive variables related to brain activation maps, this approach aims at decoding brain activity. Inverse inference takes into account the multivariate information between voxels and is currently the only way to assess how precisely some cognitive information is encoded by the activity of neural populations within the whole brain. However, it relies on a prediction function that is plagued by the curse of dimensionality, as we have far more features than samples, i.e., more voxels than fMRI volumes. To address this problem, different methods have been proposed. Among them are univariate feature selection, feature agglomeration and regularization techniques. In this paper, we consider a hierarchical structured regularization. Specifically, the penalization we use is constructed from a tree that is obtained by spatially constrained agglomerative clustering. This approach encodes the spatial prior information in the regularization process, which makes the overall prediction procedure more robust to inter-subject variability. We test our algorithm on a real data acquired for studying the mental representation of objects, and we show that the proposed algorithm yields better prediction accuracy than reference methods.},
archivePrefix = {arXiv},
arxivId = {1105.0363},
author = {Jenatton, Rodolphe and Gramfort, Alexandre and Michel, Vincent and Obozinski, Guillaume and Bach, Francis and Thirion, Bertrand},
booktitle = {Proceedings - International Workshop on Pattern Recognition in NeuroImaging, PRNI 2011},
doi = {10.1109/PRNI.2011.15},
eprint = {1105.0363},
isbn = {9780769543994},
issn = {1936-4954},
pages = {69--72},
title = {{Multi-scale mining of fMRI data with hierarchical structured sparsity}},
year = {2011}
}
@article{Bergmeir2012,
abstract = {In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand. In traditional forecasting, it is common practice to reserve a part from the end of each time series for testing, and to use the rest of the series for training. Thus it is not made full use of the data, but theoretical problems with respect to temporal evolutionary effects and dependencies within the data as well as practical problems regarding missing values are eliminated. On the other hand, when evaluating machine learning and other regression methods used for time series forecasting, often cross-validation is used for evaluation, paying little attention to the fact that those theoretical problems invalidate the fundamental assumptions of cross-validation. To close this gap and examine the consequences of different model selection procedures in practice, we have developed a rigorous and extensive empirical study. Six different model selection procedures, based on (i) cross-validation and (ii) evaluation using the series' last part, are used to assess the performance of four machine learning and other regression techniques on synthetic and real-world time series. No practical consequences of the theoretical flaws were found during our study, but the use of cross-validation techniques led to a more robust model selection. To make use of the "best of both worlds", we suggest that the use of a blocked form of cross-validation for time series evaluation became the standard procedure, thus using all available information and circumventing the theoretical problems. ?? 2012 Elsevier Inc. All rights reserved.},
annote = {blocked form of CV

empirical study

no practical consequences of the theoretical flaws were found, but suggest to use blocked CV},
author = {Bergmeir, Christoph and Ben{\'{i}}tez, Jos{\'{e}} M.},
doi = {10.1016/j.ins.2011.12.028},
issn = {00200255},
journal = {Information Sciences},
keywords = {Cross-validation,Error measures,Machine learning,Predictor evaluation,Regression,Time series},
pages = {192--213},
title = {{On the use of cross-validation for time series predictor evaluation}},
volume = {191},
year = {2012}
}
@article{Mueen2016,
abstract = {Dynamic Time Warping (DTW) is a distance measure that compares two time series after optimally aligning them. DTW is being used for decades in thousands of academic and industrial projects despite the very expensive computational complexity, O(n2). These applications include data mining, image processing, signal processing, robotics and computer graphics among many others. In spite of all this research effort, there are many myths and misunderstanding about DTW in the literature, for example "it is too slow to be useful" or "the warping window size does not matter much." In this tutorial, we correct these misunderstandings and we summarize the research efforts in optimizing both the efficiency and effectiveness of both the basic DTW algorithm, and of the higher-level algorithms that exploit DTW such as similarity search, clustering and classification. We will discuss variants of DTW such as constrained DTW, multidimensional DTW and asynchronous DTW, and optimization techniques such as lower bounding, early abandoning, run-length encoding, bounded approximation and hardware optimization. We will discuss a multitude of application areas including physiological monitoring, social media mining, activity recognition and animal sound processing. The optimization techniques are generalizable to other domains on various data types and problems.},
author = {Mueen, Abdullah and Keogh, Eamonn},
doi = {10.1145/2939672.2945383},
file = {:Users/andrea/Documents/Papers/mueen2016.pdf:pdf},
isbn = {9781450342322},
pages = {2129--2130},
title = {{Extracting Optimal Performance from Dynamic Time Warping}},
year = {2016}
}
@article{ChoiY2016,
author = {Choi, Youngduck and Chiu, Chill Yi-i and Sontag, David},
file = {:Users/andrea/Documents/Papers/ChoiY2016.pdf:pdf},
pages = {41--50},
title = {{Learning Low-Dimensional Representations of Medical Concepts}}
}
@article{Zou2007,
abstract = {We study the effective degrees of freedom of the lasso in the framework of Stein's unbiased risk estimation (SURE). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso--a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria--{\$}C{\_}p{\$}, AIC and BIC--are available, which, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.},
archivePrefix = {arXiv},
arxivId = {0712.0881},
author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1214/009053607000000127},
eprint = {0712.0881},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Degrees of freedom,LARS algorithm,Lasso,Model selection,SURE,Unbiased estimate},
number = {5},
pages = {2173--2192},
title = {{On the "degrees of freedom" of the lasso}},
volume = {35},
year = {2007}
}
@article{Pivovarov2015,
abstract = {We present the Unsupervised Phenome Model (UPhenome), a probabilistic graphical model for large-scale discovery of computational models of disease, or phenotypes. We tackle this challenge through the joint modeling of a large set of diseases and a large set of clinical observations. The observations are drawn directly from heterogeneous patient record data (notes, laboratory tests, medications, and diagnosis codes), and the diseases are modeled in an unsupervised fashion. We apply UPhenome to two qualitatively different mixtures of patients and diseases: records of extremely sick patients in the intensive care unit with constant monitoring, and records of outpatients regularly followed by care providers over multiple years. We demonstrate that the UPhenome model can learn from these different care settings, without any additional adaptation. Our experiments show that (i) the learned phenotypes combine the heterogeneous data types more coherently than baseline LDA-based phenotypes; (ii) they each represent single diseases rather than a mix of diseases more often than the baseline ones; and (iii) when applied to unseen patient records, they are correlated with the patients' ground-truth disorders. Code for training, inference, and quantitative evaluation is made available to the research community.},
author = {Pivovarov, Rimma and Perotte, Adler J. and Grave, Edouard and Angiolillo, John and Wiggins, Chris H. and Elhadad, No{\'{e}}mie},
doi = {10.1016/j.jbi.2015.10.001},
file = {:Users/andrea/Documents/Papers/pivovarov2015.pdf:pdf},
isbn = {1532-0480 (Electronic)1532-0464 (Linking)},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Clinical phenotype modeling,Computational disease models,Electronic health record,Medical information systems,Phenotyping,Probabilistic modeling},
pages = {156--165},
pmid = {26464024},
publisher = {Elsevier Inc.},
title = {{Learning probabilistic phenotypes from heterogeneous EHR data}},
url = {http://dx.doi.org/10.1016/j.jbi.2015.10.001},
volume = {58},
year = {2015}
}
@article{Simon2013,
abstract = {For high dimensional supervised learning problems, often using problem specific assumptions can lead to greater ac- curacy. For problems with grouped covariates, which are believed to have sparse effects both on a group and within group level, we introduce a regularized model for linear regression with ?1 and ?2 penalties. We discuss the sparsity and other regularization prop- erties of the optimal fit for this model, and show that it has the desired effect of group-wise and within group sparsity. We propose an algorithm to fit the model via accelerated generalized gradi- ent descent, and extend this model and algorithm to convex loss functions. We also demonstrate the efficacy of our model and the efficiency of our algorithm on simulated data.},
author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1080/10618600.2012.681250},
isbn = {1061-8600},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Model,Nesterov,Penalize,Regression,Regularize},
number = {2},
pages = {231--245},
title = {{A sparse-group lasso}},
volume = {22},
year = {2013}
}
@article{Zhao2009,
abstract = {Extracting useful information from high-dimensional data is an important focus of today's statistical research and practice. Penalized loss function minimization has been shown to be effective for this task both theoretically and empirically. With the virtues of both regularization and sparsity, the {\$}L{\_}1{\$}-penalized squared error minimization method Lasso has been popular in regression models and beyond. In this paper, we combine different norms including {\$}L{\_}1{\$} to form an intelligent penalty in order to add side information to the fitting of a regression or classification model to obtain reasonable estimates. Specifically, we introduce the Composite Absolute Penalties (CAP) family, which allows given grouping and hierarchical relationships between the predictors to be expressed. CAP penalties are built by defining groups and combining the properties of norm penalties at the across-group and within-group levels. Grouped selection occurs for nonoverlapping groups. Hierarchical variable selection is reached by defining groups with particular overlapping patterns. We propose using the BLASSO and cross-validation to compute CAP estimates in general. For a subfamily of CAP estimates involving only the {\$}L{\_}1{\$} and {\$}L{\_}{\{}\backslashinfty{\}}{\$} norms, we introduce the iCAP algorithm to trace the entire regularization path for the grouped selection problem. Within this subfamily, unbiased estimates of the degrees of freedom (df) are derived so that the regularization parameter is selected without cross-validation. CAP is shown to improve on the predictive performance of the LASSO in a series of simulated experiments, including cases with {\$}p\backslashgg n{\$} and possibly mis-specified groupings. When the complexity of a model is properly calculated, iCAP is seen to be parsimonious in the experiments.},
archivePrefix = {arXiv},
arxivId = {0909.0411},
author = {Zhao, Peng and Rocha, Guilherme and Yu, Bin},
doi = {10.1214/07-AOS584},
eprint = {0909.0411},
file = {:Users/andrea/Documents/Papers/Zhao2009.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Coefficient paths,Grouped selection,Hierarchical models,Linear regression,Penalized regression,Variable selection},
number = {6 A},
pages = {3468--3497},
title = {{The composite absolute penalties family for grouped and hierarchical variable selection}},
volume = {37},
year = {2009}
}
@article{henao2016,
abstract = {Electronic Health Record (EHR) phenotyping utilizes patient data captured through nor-mal medical practice, to identify features that may represent computational medical phe-notypes. These features may be used to identify at-risk patients and improve prediction of patient morbidity and mortality. We present a novel deep multi-modality architecture for EHR analysis (applicable to joint analysis of multiple forms of EHR data), based on Poisson Factor Analysis (PFA) modules. Each modality, composed of observed counts, is represented as a Poisson distribution, parameterized in terms of hidden binary units. In-formation from different modalities is shared via a deep hierarchy of common hidden units. Activation of these binary units occurs with probability characterized as Bernoulli-Poisson link functions, instead of more traditional logistic link functions. In addition, we demon-strate that PFA modules can be adapted to discriminative modalities. To compute model parameters, we derive efficient Markov Chain Monte Carlo (MCMC) inference that scales efficiently, with significant computational gains when compared to related models based on logistic link functions. To explore the utility of these models, we apply them to a subset of patients from the Duke-Durham patient cohort. We identified a cohort of over 12,000 patients with Type 2 Diabetes Mellitus (T2DM) based on diagnosis codes and laboratory},
author = {Henao, Ricardo and Lu, James T and Lucas, Joseph E and Ferranti, Jeffrey and Carin, Lawrence},
file = {:Users/andrea/Documents/Papers/Henao16.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {1--48},
title = {{Electronic Health Record Analysis via Deep Poisson Factor Models}},
volume = {1},
year = {2015}
}
@article{Au2017,
abstract = {One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent "absent levels" problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question's level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's random forests FORTRAN code and the randomForest R package (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model's performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found.},
archivePrefix = {arXiv},
arxivId = {1706.03492},
author = {Au, Timothy C.},
eprint = {1706.03492},
file = {:Users/andrea/Documents/Papers/au2018.pdf:pdf},
keywords = {absent levels,cart,categorical predictors,decision trees,random forests},
pages = {1--30},
title = {{Random Forests, Decision Trees, and Categorical Predictors: The "Absent Levels" Problem}},
url = {http://arxiv.org/abs/1706.03492},
volume = {19},
year = {2017}
}
@article{Deliberato2018,
abstract = {OBJECTIVE Severity of illness scores rest on the assumption that patients have normal physiologic values at baseline and that patients with similar severity of illness scores have the same degree of deviation from their usual state. Prior studies have reported differences in baseline physiology, including laboratory markers, between obese and normal weight individuals, but these differences have not been analyzed in the ICU. We compared deviation from baseline of pertinent ICU laboratory test results between obese and normal weight patients, adjusted for the severity of illness. DESIGN Retrospective cohort study in a large ICU database. SETTING Tertiary teaching hospital. PATIENTS Obese and normal weight patients who had laboratory results documented between 3 days and 1 year prior to hospital admission. INTERVENTIONS None. MEASUREMENTS AND MAIN RESULTS Seven hundred sixty-nine normal weight patients were compared with 1,258 obese patients. After adjusting for the severity of illness score, age, comorbidity index, baseline laboratory result, and ICU type, the following deviations were found to be statistically significant: WBC 0.80 (95{\%} CI, 0.27-1.33) × 10/L; p = 0.003; log (blood urea nitrogen) 0.01 (95{\%} CI, 0.00-0.02); p = 0.014; log (creatinine) 0.03 (95{\%} CI, 0.02-0.05), p {\textless} 0.001; with all deviations higher in obese patients. A logistic regression analysis suggested that after adjusting for age and severity of illness at least one of these deviations had a statistically significant effect on hospital mortality (p = 0.009). CONCLUSIONS Among patients with the same severity of illness score, we detected clinically small but significant deviations in WBC, creatinine, and blood urea nitrogen from baseline in obese compared with normal weight patients. These small deviations are likely to be increasingly important as bigger data are analyzed in increasingly precise ways. Recognition of the extent to which all critically ill patients may deviate from their own baseline may improve the objectivity, precision, and generalizability of ICU mortality prediction and severity adjustment models.},
author = {Deliberato, Rodrigo Oct{\'{a}}vio and Ko, Stephanie and Komorowski, Matthieu and {Armengol De La Hoz}, M. A. and Frushicheva, Maria P. and Raffa, Jesse D. and Johnson, Alistair E.W. and Celi, Leo Anthony and Stone, David J.},
doi = {10.1097/CCM.0000000000002868},
isbn = {0000000000},
issn = {15300293},
journal = {Critical Care Medicine},
keywords = {Critical care,Obesity,Outcome,Severity of illness score},
number = {3},
pages = {394--400},
pmid = {29194147},
title = {{Severity of illness scores may misclassify critically ill obese patients}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/29194147},
volume = {46},
year = {2018}
}
@article{cohen2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.05035v1},
author = {Cohen, Jeremy E and Bro, Rasmus},
eprint = {arXiv:1802.05035v1},
file = {:Users/andrea/Documents/Papers/cohen2018.pdf:pdf},
keywords = {flexible coupling,nonnegativity constraints,parafac2},
title = {{Nonnegative PARAFAC2: a flexible coupling approach}},
year = {2018}
}
@article{magoev2018,
author = {Magoev, Kirill and Krzhizhanovskaya, Valeria V. and Kovalchuk, Sergey V.},
doi = {10.1016/j.procs.2018.08.277},
file = {:Users/andrea/Documents/Papers/Magoev2018.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Acute Coronary Syndrome,Anomaly Detection,Classification,Clustering,Machine Learning},
pages = {370--379},
publisher = {Elsevier B.V.},
title = {{Application of clustering methods for detecting critical acute coronary syndrome patients}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918315837},
volume = {136},
year = {2018}
}
@article{Goldstein2017a,
abstract = {OBJECTIVE Electronic health records (EHRs) are an increasingly common data source for clinical risk prediction, presenting both unique analytic opportunities and challenges. We sought to evaluate the current state of EHR based risk prediction modeling through a systematic review of clinical prediction studies using EHR data. METHODS We searched PubMed for articles that reported on the use of an EHR to develop a risk prediction model from 2009 to 2014. Articles were extracted by two reviewers, and we abstracted information on study design, use of EHR data, model building, and performance from each publication and supplementary documentation. RESULTS We identified 107 articles from 15 different countries. Studies were generally very large (median sample size = 26 100) and utilized a diverse array of predictors. Most used validation techniques (n = 94 of 107) and reported model coefficients for reproducibility (n = 83). However, studies did not fully leverage the breadth of EHR data, as they uncommonly used longitudinal information (n = 37) and employed relatively few predictor variables (median = 27 variables). Less than half of the studies were multicenter (n = 50) and only 26 performed validation across sites. Many studies did not fully address biases of EHR data such as missing data or loss to follow-up. Average c-statistics for different outcomes were: mortality (0.84), clinical prediction (0.83), hospitalization (0.71), and service utilization (0.71). CONCLUSIONS EHR data present both opportunities and challenges for clinical risk prediction. There is room for improvement in designing such studies.},
author = {Goldstein, Benjamin A. and Navar, Ann Marie and Pencina, Michael J. and Ioannidis, John P.A.},
doi = {10.1093/jamia/ocw042},
file = {:Users/andrea/Documents/Papers/goldstein2017.pdf:pdf},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {Electronic medical record,Review,Risk assessment},
number = {1},
pages = {198--208},
title = {{Opportunities and challenges in developing risk prediction models with electronic health records data: A systematic review}},
volume = {24},
year = {2017}
}
@inproceedings{Chiuso2010,
abstract = {Identification of sparse high dimensional linear systems pose sever challenges to off-the-shelf techniques for system identification. This is particularly so when relatively small data sets, as compared to the number of inputs and outputs, have to be used. In this paper we introduce a new nonparametric technique which borrows ideas from a recently introduced Kernel estimator called “stable-spline” as well as from sparsity inducing priors which use ℓ1 penalty. We compare the new method with a group LAR-type of algorithm applied to estimation of sparse Vector Autoregressive models and to standard PEM methods.},
annote = {estimate VARX with lasso and group lasso penalties though do not elaborate on potential group structures 

this could be the most close model apart from VARX-L to me.},
author = {Chiuso, Alessandro and Pillonetto, Gianluigi},
booktitle = {Proceedings of the IEEE Conference on Decision and Control},
doi = {10.1109/CDC.2010.5717169},
isbn = {9781424477456},
issn = {01912216},
pages = {2942--2947},
title = {{Nonparametric sparse estimators for identification of large scale linear systems}},
year = {2010}
}
@misc{Tibshirani1996,
abstract = {Document: Details (1994) Robert Tibshirani CiteSeer.IST - Copyright Penn State and NEC},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
booktitle = {Journal of the Royal Statistical Society B},
doi = {10.2307/2346178},
eprint = {11/73273},
isbn = {0849320240},
issn = {00359246},
number = {1},
pages = {267--288},
pmid = {16272381},
primaryClass = {1369–7412},
title = {{Regression Selection and Shrinkage via the Lasso}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7574},
volume = {58},
year = {1996}
}
@article{Usharani2016,
author = {Usharani, Yelipe},
file = {:Users/andrea/Documents/Papers/usharani2016.pdf:pdf},
keywords = {a very dangerous potential,at,cluster,dimensionality reduction,hazard,imputation,medical records,missing values,nearest,neighbor,prediction,results and thus leading,this research mainly aims,to improper medical treatment,which is},
number = {February},
pages = {23--31},
title = {{An Innovative Imputation and Classification Approach for Accurate Disease Prediction}},
volume = {14},
year = {2016}
}
@article{coates2011,
author = {Coates, Adam and Lee, Honglak and Ng, Andrew Y.},
file = {:Users/andrea/Documents/Papers/coates2011.pdf:pdf},
issn = {1000680X},
number = {2},
pages = {130},
title = {{An Analysis of Single-Layer Networks in Unsupervised Feature Learning Adam}},
volume = {24},
year = {2002}
}
@article{Amodei2016,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
archivePrefix = {arXiv},
arxivId = {1606.06565},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
eprint = {1606.06565},
file = {:Users/andrea/Documents/Papers/amodei2016.pdf:pdf},
pages = {1--29},
title = {{Concrete Problems in AI Safety}},
url = {http://arxiv.org/abs/1606.06565},
year = {2016}
}
@article{Kojima2010,
abstract = {We propose a state space representation of vector autoregressive model and its sparse learning based on L1 regularization to achieve efficient estimation of dynamic gene networks based on time course microarray data. The proposed method can overcome drawbacks of the vector autoregressive model and state space model; the assumption of equal time interval and lack of separation ability of observation and systems noises in the former method and the assumption of modularity of network structure in the latter method. However, in a simple implementation the proposed model requires the calculation of large inverse matrices in a large number of times during parameter estimation process based on EM algorithm. This limits the applicability of the proposed method to a relatively small gene set. We thus introduce a new calculation technique for EM algorithm that does not require the calculation of inverse matrices. The proposed method is applied to time course microarray data of lung cells treated by stimulating EGF receptors and dosing an anticancer drug, Gefitinib. By comparing the estimated network with the control network estimated using non-treated lung cells, perturbed genes by the anticancer drug could be found, whose up- and down-stream genes in the estimated networks may be related to side effects of the anticancer drug.},
annote = {seems that the goal of state space model is to estimate the coeff matrix A of latent variables (state) xt. Then it is not useful for us, since we have xt

but the representation from VAR to SS is interesting. how would it serve us? 

more applications in VAR in biomedicine

time course microarray gene expression

gene regulation network (network can be potentially useful for our future study)},
author = {Kojima, Kaname and Yamaguchi, Rui and Imoto, Seiya and Yamauchi, Mai and Nagasaki, Masao and Yoshida, Ryo and Shimamura, Teppei and Ueno, Kazuko and Higuchi, Tomoyuki and Gotoh, Noriko and Miyano, Satoru},
issn = {0919-9454},
journal = {Genome informatics International Conference on Genome Informatics},
pages = {56--68},
pmid = {20238419},
title = {{A state space representation of VAR models with sparse learning for dynamic gene networks.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=20238419{\&}retmode=ref{\&}cmd=prlinks{\%}5Cnpapers3://publication/uuid/B260A578-2AAC-41D6-9ACC-39B70EA1DD39},
volume = {22},
year = {2010}
}
@article{Abid2019,
abstract = {We introduce the concrete autoencoder, an end-to-end differentiable method for global feature selection, which efficiently identifies a subset of the most informative features and simultaneously learns a neural network to reconstruct the input data from the selected features. Our method is unsupervised, and is based on using a concrete selector layer as the encoder and using a standard neural network as the decoder. During the training phase, the temperature of the concrete selector layer is gradually decreased, which encourages a user-specified number of discrete features to be learned. During test time, the selected features can be used with the decoder network to reconstruct the remaining input features. We evaluate concrete autoencoders on a variety of datasets, where they significantly outperform state-of-the-art methods for feature selection and data reconstruction. In particular, on a large-scale gene expression dataset, the concrete autoencoder selects a small subset of genes whose expression levels can be use to impute the expression levels of the remaining genes. In doing so, it improves on the current widely-used expert-curated L1000 landmark genes, potentially reducing measurement costs by 20{\%}. The concrete autoencoder can be implemented by adding just a few lines of code to a standard autoencoder.},
archivePrefix = {arXiv},
arxivId = {1901.09346},
author = {Abid, Abubakar and Balin, Muhammad Fatih and Zou, James},
eprint = {1901.09346},
file = {:Users/andrea/Documents/Papers/abid2019.pdf:pdf},
title = {{Concrete Autoencoders for Differentiable Feature Selection and Reconstruction}},
url = {http://arxiv.org/abs/1901.09346},
year = {2019}
}
@article{Lipton2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1606.03490v3},
author = {Lipton, Zachary C},
eprint = {arXiv:1606.03490v3},
file = {:Users/andrea/Documents/Papers/Lipton2017.pdf:pdf},
title = {{The Mythos of Model Interpretability}},
url = {https://arxiv.org/pdf/1606.03490.pdf},
year = {2016}
}
@article{Eng2019,
abstract = {{\textless}div{\textgreater}{\textless}p{\textgreater}by Simon W. M. Eng, Florence A. Aeschlimann, Mira van Veenendaal, Roberta A. Berard, Alan M. Rosenberg, Quaid Morris, Rae S. M. Yeung, on behalf of the ReACCh-Out Research Consortium {\textless}/p{\textgreater}
Background {\textless}p{\textgreater}Joint inflammation is the common feature underlying juvenile idiopathic arthritis (JIA). Clinicians recognize patterns of joint involvement currently not part of the International League of Associations for Rheumatology (ILAR) classification. Using unsupervised machine learning, we sought to uncover data-driven joint patterns that predict clinical phenotype and disease trajectories.{\textless}/p{\textgreater} Methods and findings {\textless}p{\textgreater}We analyzed prospectively collected clinical data, including joint involvement using a standard 71-joint homunculus, for 640 discovery patients with newly diagnosed JIA enrolled in a Canada-wide study who were followed serially for five years, treatment-na{\"{i}}ve except for nonsteroidal anti-inflammatory drugs (NSAIDs) and diagnosed within one year of symptom onset. Twenty-one patients had systemic arthritis, 300 oligoarthritis, 125 rheumatoid factor (RF)-negative polyarthritis, 16 RF-positive polyarthritis, 37 psoriatic arthritis, 78 enthesitis-related arthritis (ERA), and 63 undifferentiated arthritis. At diagnosis, we observed global hierarchical groups of co-involved joints.To characterize these patterns, we developed sparse multilayer non-negative matrix factorization (NMF). Model selection by internal bi-cross-validation identified seven joint patterns at presentation, to which all 640 discovery patients were assigned: pelvic girdle (57 patients), fingers (25), wrists (114), toes (48), ankles (106), knees (283), and indistinct (7). Patterns were distinct from clinical subtypes ({\textless}i{\textgreater}P{\textless}/i{\textgreater} {\textless} 0.001 by $\chi${\textless}sup{\textgreater}2{\textless}/sup{\textgreater} test) and reproducible through external data set validation on a 119-patient, prospectively collected independent validation cohort (reconstruction accuracy {\textless}i{\textgreater}Q{\textless}/i{\textgreater}{\textless}sup{\textgreater}2{\textless}/sup{\textgreater} = 0.55 for patterns; 0.35 for groups).Some patients matched multiple patterns. To determine whether their disease outcomes differed, we further subdivided the 640 discovery patients into three subgroups by degree of localization—the percentage of their active joints aligning with their assigned pattern: localized (≥90{\%}; 359 patients), partially localized (60{\%}–90{\%}; 124), or extended ({\textless}60{\%}; 157). Localized patients more often maintained their baseline patterns ({\textless}i{\textgreater}P{\textless}/i{\textgreater} {\textless} 0.05 for five groups by permutation test) than nonlocalized patients ({\textless}i{\textgreater}P{\textless}/i{\textgreater} {\textless} 0.05 for three groups by permutation test) over a five-year follow-up period.We modelled time to zero joints in the discovery cohort using a multivariate Cox proportional hazards model considering joint pattern, degree of localization, and ILAR subtype. Despite receiving more intense treatment, 50{\%} of nonlocalized patients had zero joints at one year compared to six months for localized patients. Overall, localized patients required less time to reach zero joints (partial: {\textless}i{\textgreater}P{\textless}/i{\textgreater} = 0.0018 versus localized by log-rank test; extended: {\textless}i{\textgreater}P{\textless}/i{\textgreater} = 0.0057).Potential limitations include the requirement for patients to be treatment na{\"{i}}ve (except NSAIDs), which may skew the patient cohorts towards milder disease, and the validation cohort size precluded multivariate analyses of disease trajectories.{\textless}/p{\textgreater} Conclusions {\textless}p{\textgreater}Multilayer NMF identified patterns of joint involvement that predicted disease trajectory in children with arthritis. Our hierarchical unsupervised approach identified a new clinical feature, degree of localization, which predicted outcomes in both cohorts. Detailed assessment of every joint is already part of every musculoskeletal exam for children with arthritis. Our study supports both the continued collection of detailed joint involvement and the inclusion of patterns and degrees of localization to stratify patients and inform treatment decisions. This will advance pediatric rheumatology from counting joints to realizing the potential of using data available from uncovering patterns of joint involvement.{\textless}/p{\textgreater}{\textless}/div{\textgreater}},
author = {Eng, Simon W. M. and Aeschlimann, Florence A. and van Veenendaal, Mira and Berard, Roberta A. and Rosenberg, Alan M. and Morris, Quaid and Yeung, Rae S. M. and Consortium, on behalf of the ReACCh-Out Research},
doi = {10.1371/journal.pmed.1002750},
file = {:Users/andrea/Documents/Papers/eng2019.pdf:pdf},
isbn = {1111111111},
issn = {1549-1676},
journal = {PLoS Medicine},
pages = {1--22},
title = {{Patterns of joint involvement in juvenile idiopathic arthritis and prediction of disease course: A prospective study with multilayer non-negative matrix factorization}},
url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002750},
year = {2019}
}
@article{Wang2017,
abstract = {This paper describes a frame{\_}calendar function that organizes and displays temporal data, collected on sub-daily resolution, into a calendar layout. Calendars are broadly used in society to display temporal information, and events. The frame{\_}calendar uses linear algebra on the date variable to create the layout. It utilizes the grammar of graphics to create the plots inside each cell, and thus synchronizes neatly with ggplot2 graphics. The motivating application is studying pedestrian behavior in Melbourne, Australia, based on counts which are captured at hourly intervals by sensors scattered around the city. Faceting by the usual features such as day and month, was insufficient to examine the behavior. Making displays on a monthly calendar format helps to understand pedestrian patterns relative to events such as work days, weekends, holidays, and special events. The layout algorithm has several format options and variations. It is implemented in the R package sugrrants.},
archivePrefix = {arXiv},
arxivId = {1709.04553},
author = {Wang, Earo and Cook, Dianne and Hyndman, Rob J},
doi = {10.18637/jss.v000.i00},
eprint = {1709.04553},
journal = {Journal of Statistical Software},
keywords = {data visualization,grammar of graph-,r package,statistical graphics,time series},
number = {II},
pages = {1--19},
title = {{Calendar-based graphics for visualizing people's daily schedules}},
url = {http://arxiv.org/abs/1709.04553},
volume = {VV},
year = {2017}
}
@article{Sims1980,
abstract = {Existing strategies for econometric analysis related to macroeconomics are subject to a$\backslash$r$\backslash$n number of serious objections, some recently formulated, some old. These objections are$\backslash$r$\backslash$n summarized in this paper, and it is argued that taken together they make it unlikely that$\backslash$r$\backslash$n macroeconomic models are in fact over identified, as the existing statistical theory usually$\backslash$r$\backslash$n assumes. The implications of this conclusion are explored, and an example of econometric$\backslash$r$\backslash$n work in a non-standard style, taking account of the objections to the standard style, is$\backslash$r$\backslash$n presented},
annote = {popularised VAR in 1980s},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sims, Christopher A.},
doi = {10.2307/1912017},
eprint = {arXiv:1011.1669v3},
isbn = {00129682},
issn = {00129682},
journal = {Econometrica},
keywords = {VAR},
mendeley-tags = {VAR},
number = {1},
pages = {1},
pmid = {25246403},
title = {{Macroeconomics and Reality}},
url = {http://www.jstor.org/stable/1912017?origin=crossref},
volume = {48},
year = {1980}
}
@article{Rubin2018,
abstract = {Background: Early deterioration indicators have the potential to alert hospital care staff in advance of adverse events, such as patients requiring an increased level of care, or the need for rapid response teams to be called. Our work focuses on the problem of predicting the transfer of pediatric patients from the general ward of a hospital to the pediatric intensive care unit. Objectives: The development of a data-driven pediatric early deterioration indicator for use by clinicians with the purpose of predicting encounters where transfer from the general ward to the PICU is likely. Methods: Using data collected over 5.5 years from the electronic health records of two medical facilities, we develop machine learning classifiers based on adaptive boosting and gradient tree boosting. We further combine these learned classifiers into an ensemble model and compare its performance to a modified pediatric early warning score (PEWS) baseline that relies on expert defined guidelines. To gauge model generalizability, we perform an inter-facility evaluation where we train our algorithm on data from one facility and perform evaluation on a hidden test dataset from a separate facility. Results: We show that improvements are witnessed over the modified PEWS baseline in accuracy (0.77 vs. 0.69), sensitivity (0.80 vs. 0.68), specificity (0.74 vs. 0.70) and AUROC (0.85 vs. 0.73). Conclusions: Data-driven, machine learning algorithms can improve PICU transfer prediction accuracy compared to expertly defined systems, such as a modified PEWS, but care must be taken in the training of such approaches to avoid inadvertently introducing bias into the outcomes of these systems.},
author = {Rubin, Jonathan and Potes, Cristhian and Xu-Wilson, Minnan and Dong, Junzi and Rahman, Asif and Nguyen, Hiep and Moromisato, David},
doi = {10.1016/j.ijmedinf.2018.01.001},
file = {:Users/andrea/Documents/Papers/rubin2018.pdf:pdf},
issn = {18728243},
journal = {International Journal of Medical Informatics},
keywords = {Early deterioration indicator,Early warning systems,Machine learning,Pediatrics},
number = {November 2017},
pages = {15--20},
publisher = {Elsevier},
title = {{An ensemble boosting model for predicting transfer to the pediatric intensive care unit}},
url = {https://doi.org/10.1016/j.ijmedinf.2018.01.001},
volume = {112},
year = {2018}
}
@article{Pollard2018,
abstract = {Critical care patients are monitored closely through the course of their illness. As a result of this monitoring, large amounts of data are routinely collected for these patients. Philips Healthcare has developed a telehealth system, the eICU Program, which leverages these data to support management of critically ill patients. Here we describe the eICU Collaborative Research Database, a multi-center intensive care unit (ICU)database with high granularity data for over 200,000 admissions to ICUs monitored by eICU Programs across the United States. The database is deidentified, and includes vital sign measurements, care plan documentation, severity of illness measures, diagnosis information, treatment information, and more. Data are publicly available after registration, including completion of a training course in research with human subjects and signing of a data use agreement mandating responsible handling of the data and adhering to the principle of collaborative research. The freely available nature of the data will support a number of applications including the development of machine learning algorithms, decision support tools, and clinical research.ch},
author = {Pollard, Tom J. and Johnson, Alistair E.W. and Raffa, Jesse D. and Celi, Leo A. and Mark, Roger G. and Badawi, Omar},
doi = {10.1038/sdata.2018.178},
file = {:Users/andrea/Documents/Papers/Pollard2018.pdf:pdf},
issn = {20524463},
journal = {Scientific Data},
pages = {1--13},
pmid = {30204154},
publisher = {The Author(s)},
title = {{The eICU collaborative research database, a freely available multi-center database for critical care research}},
url = {http://dx.doi.org/10.1038/sdata.2018.178},
volume = {5},
year = {2018}
}
@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models re-main mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation tech-nique that explains the predictions of any classifier in an in-terpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative indi-vidual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization prob-lem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.1145/2939672.2939778},
eprint = {1602.04938},
file = {:Users/andrea/Documents/Papers/Ribeiro2016.pdf:pdf},
isbn = {9781450321389},
issn = {9781450321389},
pmid = {214160309},
title = {{'Why should I Trust You?' Explaining the Predictions of Any Classifier}},
year = {2016}
}
@article{Fiorini2017,
abstract = {Multiple Sclerosis is a degenerative condition of the central nervous system that affects nearly 2.5 million of individuals in terms of their physical, cognitive, psychological and social capabilities. Despite the high variability of its clinical presentation, relapsing and progressive multiple sclerosis are considered the two main disease types, with the former possibly evolving into the latter. Recently, the attention of the medical community toward the use of patient-centered outcomes in multiple sclerosis has significantly increased. Such patient-friendly measures are devoted to the assessment of the impact of the disease on several domains of the patient life. In this work, we investigate on use of patient-centered outcomes to predict the evolution of the disease and to assess its impact on patients' lives. To this aim, we build a novel temporal model based on gradient boosting classification and multiple-output elastic-net regression. The model provides clinically interpretable results along with accurate predictions of the disease course evolution.},
author = {Fiorini, Samuele and Verri, Alessandro and Barla, Annalisa and Tacchino, Andrea and Brichetto, Giampaolo},
file = {:Users/andrea/Documents/Papers/Fiorini2017.pdf:pdf},
issn = {1938-7228},
pages = {112--125},
title = {{Temporal prediction of multiple sclerosis evolution from patient-centered outcomes}},
url = {http://proceedings.mlr.press/v68/fiorini17a.html},
volume = {68},
year = {2017}
}
@article{Che2015,
abstract = {We apply deep learning to the problem of discovery and detection of characteristic patterns of physiology in clinical time series data. We propose two novel modifications to standard neural net training that address challenges and ex-ploit properties that are peculiar, if not exclusive, to medical data. First, we examine a general framework for using prior knowledge to regularize parameters in the topmost layers. This framework can leverage priors of any form, ranging from formal ontologies (e.g., ICD9 codes) to data-derived similarity. Second, we describe a scalable procedure for training a collection of neural networks of di↵erent sizes but with partially shared architectures. Both of these innova-tions are well-suited to medical applications, where available data are not yet Internet scale and have many sparse out-puts (e.g., rare diagnoses) but which have exploitable struc-ture (e.g., temporal order and relationships between labels). However, both techniques are suciently general to be ap-plied to other problems and domains. We demonstrate the empirical ecacy of both techniques on two real-world hos-pital data sets and show that the resulting neural nets learn interpretable and clinically relevant features.},
author = {Che, Zhengping and Kale, David and Bahadori, Mohammad Taha and Liu, Yan and Li, Wenzhe},
doi = {10.1145/2783258.2783365},
file = {:Users/andrea/Documents/Papers/che2015.pdf:pdf},
isbn = {9781450336642},
keywords = {ate time series,deep learning,healthcare,medical informatics,multi-label classification,multivari-,phenotyping},
pages = {507--516},
title = {{Deep Computational Phenotyping}},
year = {2015}
}
@article{Liu2018a,
abstract = {The availability of a large amount of electronic health records (EHR) provides huge opportunities to improve health care service by mining these data. One important application is clinical endpoint prediction, which aims to predict whether a disease, a symptom or an abnormal lab test will happen in the future according to patients' history records. This paper develops deep learning techniques for clinical endpoint prediction, which are effective in many practical applications. However, the problem is very challenging since patients' history records contain multiple heterogeneous temporal events such as lab tests, diagnosis, and drug administrations. The visiting patterns of different types of events vary significantly, and there exist complex nonlinear relationships between different events. In this paper, we propose a novel model for learning the joint representation of heterogeneous temporal events. The model adds a new gate to control the visiting rates of different events which effectively models the irregular patterns of different events and their nonlinear correlations. Experiment results with real-world clinical data on the tasks of predicting death and abnormal lab tests prove the effectiveness of our proposed approach over competitive baselines.},
archivePrefix = {arXiv},
arxivId = {1803.04837},
author = {Liu, Luchen and Shen, Jianhao and Zhang, Ming and Wang, Zichang and Tang, Jian},
eprint = {1803.04837},
file = {:Users/andrea/Documents/Papers/liu2018c.pdf:pdf},
number = {1},
title = {{Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction}},
url = {http://arxiv.org/abs/1803.04837},
year = {2018}
}
@article{Liu2018,
abstract = {Early detection of preventable diseases is important for better disease management, improved inter-ventions, and more efficient health-care resource allocation. Various machine learning approacheshave been developed to utilize information in Electronic Health Record (EHR) for this task. Majorityof previous attempts, however, focus on structured fields and lose the vast amount of information inthe unstructured notes. In this work we propose a general multi-task framework for disease onsetprediction that combines both free-text medical notes and structured information. We compareperformance of different deep learning architectures including CNN, LSTM and hierarchical models.In contrast to traditional text-based prediction models, our approach does not require disease specificfeature engineering, and can handle negations and numerical values that exist in the text. Ourresults on a cohort of about 1 million patients show that models using text outperform modelsusing just structured data, and that models capable of using numerical values and negations in thetext, in addition to the raw text, further improve performance. Additionally, we compare differentvisualization methods for medical professionals to interpret model predictions.},
archivePrefix = {arXiv},
arxivId = {1808.04928},
author = {Liu, Jingshu and Zhang, Zachariah and Razavian, Narges},
doi = {arXiv:1808.04928v1},
eprint = {1808.04928},
title = {{Deep EHR: Chronic Disease Prediction Using Medical Notes}},
url = {http://arxiv.org/abs/1808.04928},
year = {2018}
}
@misc{Schweinberger2017,
abstract = {High-dimensional multivariate time series are challenging due to the dependent and high-dimensional nature of the data, but in many applications there is additional structure that can be exploited to reduce computing time along with statistical error. We consider high-dimensional vector autoregressive processes with spatial structure, a simple and common form of additional structure. We propose novel high-dimensional methods that take advantage of such structure without making model assumptions about how distance affects dependence. We provide non-asymptotic bounds on the statistical error of parameter estimators in high-dimensional settings and show that the proposed approach reduces the statistical error. An application to air pollu-tion in the U.S.A. demonstrates that the estimation approach reduces both computing time and prediction error and gives rise to results that are meaningful from a scientific point of view, in contrast to high-dimensional methods that ignore spatial structure. In practice, these high-dimensional methods can be used to decompose high-dimensional multivariate time series into lower-dimensional multivariate time series that can be studied by other methods in more depth.},
annote = {spatial structure

application: air pollution},
archivePrefix = {arXiv},
arxivId = {arXiv:1510.02159v3},
author = {Schweinberger, Michael and Babkin, Sergii and Ensor, Katherine B.},
booktitle = {Journal of Computational and Graphical Statistics},
doi = {10.1080/10618600.2016.1265528},
eprint = {arXiv:1510.02159v3},
issn = {15372715},
keywords = {Dependent data,High-dimensional data,Spatial dependence,Vector autoregressive process},
pages = {1--13},
title = {{High-Dimensional Multivariate Time Series With Additional Structure}},
year = {2017}
}
@article{Helwig2017,
abstract = {Longitudinal data are inherently multimode in the sense that such data are often collected across multiple modes of variation, for example, time × variables × subjects. In many longitudinal studies, multiple variables are collected to measure some latent construct(s) of interest. In such cases, the goal is to understand temporal trends in the latent variables, as well as individual differences in the trends. Multimode component analysis models provide a powerful framework for discovering latent trends in longitudinal data. However, classic implementations of multimode models do not take into consideration functional information (i.e., the temporal sequence of the collected data) or structural information (i.e., which variables load onto which latent factors) about the study design. In this paper, we reveal how functional and structural constraints can be imposed inmultimode models (Parafac and Parafac2) in order to elucidate trends in longitudinal data. As a motivating example, we consider a longitudinal study on per capita alcohol consumption trends conducted from1970 to 2013 by theU.S. National Institute on Alcohol Abuse and Alcoholism.We demonstrate how functional and structural information about the study design can be incorporated into the Parafac and Parafac2 alternating least squares algorithms to understand temporal and regional trends in three latent constructs: beer consumption, spirits consumption, and wine consumption. Our results reveal that Americans consume more than the recommended amount of alcohol, and total alcohol consumption trends show no signs of decreasing in the last decade.},
author = {Helwig, Nathaniel E.},
doi = {10.1002/bimj.201600045},
file = {:Users/andrea/Documents/Papers/helwig2017.pdf:pdf},
issn = {15214036},
journal = {Biometrical Journal},
keywords = {Latent trends,Longitudinal data,Parafac2,Parallel Factor Analysis},
number = {4},
pages = {783--803},
title = {{Estimating latent trends in multivariate longitudinal data via Parafac2 with functional and structural constraints}},
volume = {59},
year = {2017}
}
@unpublished{Bergmeir2015,
abstract = {One of the most widely used standard procedures for model evaluation in classification and regression is K-fold cross-validation (CV). However, when it comes to time series forecasting, because of the inherent serial correlation and potential non-stationarity of the data, its application is not straightforward and often omitted by practitioners in favor of an out-of-sample (OOS) evaluation. In this paper, we show that the particular setup in which time series forecasting is usually performed using Machine Learning methods renders the use of standard K-fold CV possible. We present theoretical insights supporting our arguments. Furthermore, we present a simulation study where we show empirically that K-fold CV performs favorably compared to both OOS evaluation and other time-series-specific techniques such as non-dependent cross-validation},
annote = {A purely autoregressive model, the use of standard k-fold CV is possible as the models considered have uncorrelated errors},
author = {Bergmeir, Christoph and Hyndman, Rob J and Koo, Bonsoo},
booktitle = {Monash University, Department of Econometrics and Business Statistics},
keywords = {auto regression,cross-validation,time series},
number = {April},
title = {{A Note on the Validity of Cross-Validation for Evaluating Time Series Prediction}},
year = {2015}
}
@article{Ren2013a,
abstract = {Subset selection is a critical component of vector autoregressive (VAR) modeling. This paper proposes simple and hybrid subset selection procedures for VAR models via the adaptive Lasso. By a proper choice of tuning parameters, one can identify the correct subset and obtain the asymptotic normality of the nonzero parameters with probability tending to one. Simulation results show that for small samples, a particular hybrid procedure has the best performance in terms of prediction mean squared errors, estimation errors and subset selection accuracy under various settings. The proposed method is also applied to modeling the IS-LM data for illustration. ?? 2010 Elsevier B.V.},
author = {Ren, Yunwen and Zhang, Xinsheng},
doi = {10.1080/03610926.2011.611317},
isbn = {0167-7152},
issn = {03610926},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Adaptive lasso,Bayesian information criterion,Least angle regression algorithm,Oracle property,Order selection,Subset selection,Vector autoregressive models},
number = {13},
pages = {2423--2436},
title = {{Model selection for vector autoregressive processes via adaptive lasso}},
volume = {42},
year = {2013}
}
@article{Yoon2017,
abstract = {{\textcopyright} 2017 Taylor  {\&}  Francis Group, LLC. The linear regression models with the autoregressive moving average (ARMA) errors (REGARMA models) are often considered, in order to reflect a serial correlation among observations. In this article, we focus on an adaptive least absolute shrinkage and selection operator (LASSO) (ALASSO) method for the variable selection of the REGARMA models and extend it to the linear regression models with the ARMA-generalized autoregressive conditional heteroskedasticity (ARMA-GARCH) errors (REGARMA-GARCH models). This attempt is an extension of the existing ALASSO method for the linear regression models with the AR errors (REGAR models) proposed by Wang et al. in 2007. New ALASSO algorithms are proposed to determine important predictors for the REGARMA and REGARMA-GARCH models. Finally, we provide the simulation results and real data analysis to illustrate our findings.},
archivePrefix = {arXiv},
arxivId = {10.1080/03610918.2015.1096372},
author = {Yoon, Young Joo and Lee, Sooyong and Lee, Taewook},
doi = {10.1080/03610918.2015.1096372},
eprint = {03610918.2015.1096372},
issn = {15324141},
journal = {Communications in Statistics: Simulation and Computation},
keywords = {ARMA error models,ARMA-GARCH error models,Adaptive LASSO,Penalized regression,Variable selection},
number = {5},
pages = {3479--3490},
primaryClass = {10.1080},
publisher = {Taylor {\&} Francis},
title = {{Adaptive LASSO for linear regression models with ARMA-GARCH errors}},
url = {https://doi.org/10.1080/03610918.2015.1096372},
volume = {46},
year = {2017}
}
@article{Kim2019,
author = {Kim, Eugene and Torous, John and Horng, Steven and Grossestreuer, Anne V. and Rodriguez, Jorge and Lee, Terrance and Nathanson, Larry A.},
doi = {10.1016/j.ijmedinf.2019.03.020},
file = {:Users/andrea/Documents/Papers/kim2019.pdf:pdf},
issn = {13865056},
journal = {International Journal of Medical Informatics},
keywords = {Emergency medicine,Smartphone,Tablet,mobile application},
number = {March},
pages = {114--117},
publisher = {Elsevier},
title = {{Mobile device ownership among emergency department patients}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1386505618310323},
volume = {126},
year = {2019}
}
@article{Guyon2003,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@article{Goldstein2017,
abstract = {OBJECTIVE Electronic health records (EHRs) are an increasingly common data source for clinical risk prediction, presenting both unique analytic opportunities and challenges. We sought to evaluate the current state of EHR based risk prediction modeling through a systematic review of clinical prediction studies using EHR data. METHODS We searched PubMed for articles that reported on the use of an EHR to develop a risk prediction model from 2009 to 2014. Articles were extracted by two reviewers, and we abstracted information on study design, use of EHR data, model building, and performance from each publication and supplementary documentation. RESULTS We identified 107 articles from 15 different countries. Studies were generally very large (median sample size = 26 100) and utilized a diverse array of predictors. Most used validation techniques (n = 94 of 107) and reported model coefficients for reproducibility (n = 83). However, studies did not fully leverage the breadth of EHR data, as they uncommonly used longitudinal information (n = 37) and employed relatively few predictor variables (median = 27 variables). Less than half of the studies were multicenter (n = 50) and only 26 performed validation across sites. Many studies did not fully address biases of EHR data such as missing data or loss to follow-up. Average c-statistics for different outcomes were: mortality (0.84), clinical prediction (0.83), hospitalization (0.71), and service utilization (0.71). CONCLUSIONS EHR data present both opportunities and challenges for clinical risk prediction. There is room for improvement in designing such studies.},
author = {Goldstein, Benjamin A and Navar, Ann Marie and Pencina, Michael J and Ioannidis, John P A},
doi = {10.1093/jamia/ocw042},
file = {:Users/andrea/Documents/Papers/Goldstein2016.pdf:pdf},
isbn = {1067-5027},
issn = {1067-5027},
journal = {Journal of the American Medical Informatics Association},
keywords = {electronic medical record,review,risk assessment},
number = {1},
pages = {198--208},
pmid = {27189013},
title = {{Opportunities and challenges in developing risk prediction models with electronic health records data: a systematic review}},
url = {https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocw042},
volume = {24},
year = {2017}
}
@article{Wang2013,
abstract = {We study sparse principal component analy-sis (sparse PCA) for high dimensional multi-variate vector autoregressive (VAR) time se-ries. By treating the transition matrix as a nuisance parameter, we show that sparse PCA can be directly applied on analyzing multivariate time series as if the data are i.i.d. generated. Under a double asymp-totic framework in which both the length of the sample period T and dimensionality d of the time series can increase (with possi-bly d T), we provide explicit rates of con-vergence of the angle between the estimated and population leading eigenvectors of the time series covariance matrix. Our results suggest that the spectral norm of the tran-sition matrix plays a pivotal role in deter-mining the final rates of convergence. Im-plications of such a general result is further illustrated using concrete examples. The re-sults of this paper have impacts on different applications, including financial time series, biomedical imaging, and social media, etc.},
annote = {special attention to the application},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.0164v1},
author = {Wang, Zhaoran and Han, Fang and Liu, Han},
eprint = {arXiv:1307.0164v1},
issn = {15337928},
journal = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
pages = {48--56},
title = {{Sparse Principal Component Analysis for High Dimensional Multivariate Time Series}},
url = {http://jmlr.org/proceedings/papers/v31/wang13a.html},
volume = {31},
year = {2013}
}
@article{Komorowski2018,
author = {Komorowski, Matthieu and Celi, Leo A and Badawi, Omar and Gordon, Anthony C},
doi = {10.1038/s41591-018-0213-5},
file = {:Users/andrea/Documents/Papers/Komorowski2018.pdf:pdf},
issn = {1546-170X},
journal = {Nature Medicine},
number = {November},
publisher = {Springer US},
title = {{Treatment strategies for sepsis in intensive care}},
url = {http://dx.doi.org/10.1038/s41591-018-0213-5},
volume = {24},
year = {2018}
}
@article{srivastava2015,
archivePrefix = {arXiv},
arxivId = {1502.04681},
author = {Srivastava, Nitish and Mansimov, Elman and Salakhutdinov, Ruslan},
doi = {citeulike-article-id:13519737},
eprint = {1502.04681},
file = {:Users/andrea/Documents/Papers/srivastava15.pdf:pdf},
isbn = {9781510810587},
issn = {1938-7228},
title = {{Unsupervised learning of video representations using LSTMs}},
url = {http://search.proquest.com.proxy.bibliotheques.uqam.ca:2048/docview/927664756?rfr{\_}id=info{\%}3Axri{\%}2Fsid{\%}3Aprimo},
volume = {37},
year = {2015}
}
@article{Danziger2016,
author = {Burgess, Alison and Shah, Kairavi and Hough, Olivia and Hynynen, Kullervo},
doi = {10.1586/14737175.2015.1028369.Focused},
file = {:Users/andrea/Documents/Papers/danziger2016.pdf:pdf},
isbn = {4164805765},
issn = {1744-8360},
number = {5},
pages = {477--491},
pmid = {25936845},
title = {{Obesity, acute kidney injury, and mortality in critical illness}},
volume = {15},
year = {2016}
}
@article{Zadeh2017,
abstract = {Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.},
archivePrefix = {arXiv},
arxivId = {1707.07250},
author = {Zadeh, Amir and Chen, Minghai and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
doi = {10.18653/v1/D17-1115},
eprint = {1707.07250},
file = {:Users/andrea/Documents/Papers/Zadeh2017.pdf:pdf},
isbn = {9781577355687},
issn = {20904479},
keywords = {ek laboratories},
pmid = {28316638},
title = {{Tensor Fusion Network for Multimodal Sentiment Analysis}},
url = {http://arxiv.org/abs/1707.07250},
year = {2017}
}
@article{Kim2012,
abstract = {We consider the problem of learning a sparse multi-task regression with an application to a genetic association mapping problem for discovering genetic markers that influence expression levels of multiple genes jointly. In particular, we consider the case where the structure over the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity, and aim to recover the common set of relevant inputs for each output cluster. Assuming that the tree structure is available as a prior knowledge, we formulate this problem as a new multi-task regularized regression called tree-guided group lasso. Our structured regularization is based on a group-lasso penalty, where the group is defined with respect to the tree structure. We describe a systematic weighting scheme for the groups in the penalty such that each output variable is penalized in a balanced manner even if the groups overlap. We present an efficient optimization method that can handle a large-scale problem as is typically the case in association mapping that involve thousands of genes as outputs and millions of genetic markers as inputs. Using simulated and yeast datasets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns, compared to other methods for multi-task learning.},
archivePrefix = {arXiv},
arxivId = {0909.1373},
author = {Kim, Seyoung and Xing, Eric P.},
doi = {10.1214/12-AOAS549},
eprint = {0909.1373},
isbn = {9781605589077},
issn = {19326157},
journal = {Annals of Applied Statistics},
number = {3},
pages = {1095--1117},
title = {{Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eQTL mapping}},
volume = {6},
year = {2012}
}
@article{Komorowski2018,
author = {Komorowski, Matthieu and Celi, Leo A and Badawi, Omar and Gordon, Anthony C},
file = {:Users/andrea/Documents/Papers/Komorowski2018.pdf:pdf},
title = {{Treatment Strategies for Sepsis in Intensive Care}},
url = {https://www.nature.com/articles/s41591-018-0213-5}
}
@article{Giordano2017,
abstract = {Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior inference technique that is increasingly popular due to its fast runtimes on large-scale datasets. However, even when MFVB provides accurate posterior means for certain parameters, it often mis-estimates variances and covariances. Furthermore, prior robustness measures have remained undeveloped for MFVB. By deriving a simple formula for the effect of infinitesimal model perturbations on MFVB posterior means, we provide both improved covariance estimates and local robustness measures for MFVB, thus greatly expanding the practical usefulness of MFVB posterior approximations. The estimates for MFVB posterior covariances rely on a result from the classical Bayesian robustness literature relating derivatives of posterior expectations to posterior covariances and include the Laplace approximation as a special case. Our key condition is that the MFVB approximation provides good estimates of a select subset of posterior means---an assumption that has been shown to hold in many practical settings. In our experiments, we demonstrate that our methods are simple, general, and fast, providing accurate posterior uncertainty estimates and robustness measures with runtimes that can be an order of magnitude faster than MCMC.},
archivePrefix = {arXiv},
arxivId = {1709.02536},
author = {Giordano, Ryan and Broderick, Tamara and Jordan, Michael I.},
eprint = {1709.02536},
file = {:Users/andrea/Documents/Papers/giordano2018.pdf:pdf},
keywords = {automatic differentiation,bayesian robustness,laplace approximation,linear response theory,mean field approximation,variational bayes},
pages = {1--49},
title = {{Covariances, Robustness, and Variational Bayes}},
url = {http://arxiv.org/abs/1709.02536},
volume = {19},
year = {2017}
}
@article{Bien2013,
abstract = {We add a set of convex constraints to the lasso to produce sparse interaction models that honor the hierarchy restriction that an interaction only be included in a model if one or both variables are marginally important. We give a precise characterization of the effect of this hierarchy constraint, prove that hierarchy holds with probability one and derive an unbiased estimate for the degrees of freedom of our estimator. A bound on this estimate reveals the amount of fitting "saved" by the hierarchy constraint. We distinguish between parameter sparsity - the number of nonzero coefficients - and practical sparsity - the number of raw variables one must measure to make a new prediction. Hierarchy focuses on the latter, which is more closely tied to important data collection concerns such as cost, time and effort. We develop an algorithm, available in the R package hierNet, and perform an empirical study of our method.},
archivePrefix = {arXiv},
arxivId = {1205.5050},
author = {Bien, Jacob and Taylor, Jonathan and Tibshirani, Robert},
doi = {10.1214/13-AOS1096},
eprint = {1205.5050},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Convexity,Hierarchical sparsity,Interactions,Lasso,Regularized regression},
number = {3},
pages = {1111--1141},
pmid = {26257447},
title = {{A lasso for hierarchical interactions}},
volume = {41},
year = {2013}
}
@article{Tibshirani2011,
abstract = {We propose a new method for estimation in linear models. The "lasso" minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described. Keywords: regression, subset selection, shrinkage, quadratic programming.},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2011.00771.x},
eprint = {11/73273},
isbn = {0035-9246},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Penalization,Regularization,l1-penalty},
number = {3},
pages = {273--282},
pmid = {16272381},
primaryClass = {1369–7412},
title = {{Regression shrinkage and selection via the lasso: A retrospective}},
volume = {73},
year = {2011}
}
@article{Mei2016,
abstract = {Multivariate time series (MTS) datasets broadly exist in numerous fields, including health care, multimedia, finance, and biometrics. How to classify MTS accurately has become a hot research topic since it is an important element in many computer vision and pattern recognition applications. In this paper, we propose a Mahalanobis distance-based dynamic time warping (DTW) measure for MTS classification. The Mahalanobis distance builds an accurate relationship between each variable and its corresponding category. It is utilized to calculate the local distance between vectors in MTS. Then we use DTW to align those MTS which are out of synchronization or with different lengths. After that, how to learn an accurate Mahalanobis distance function becomes another key problem. This paper establishes a LogDet divergence-based metric learning with triplet constraint model which can learn Mahalanobis matrix with high precision and robustness. Furthermore, the proposed method is applied on nine MTS datasets selected from the University of California, Irvine machine learning repository and Robert T. Olszewski's homepage, and the results demonstrate the improved performance of the proposed approach.},
author = {Mei, Jiangyuan and Liu, Meizhu and Wang, Yuan Fang and Gao, Huijun},
doi = {10.1109/TCYB.2015.2426723},
file = {:Users/andrea/Documents/Papers/mei2015.pdf:pdf},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Dynamic time warping (DTW),Mahalanobis distance,metric learning,multivariate time series (MTS)},
number = {6},
pages = {1363--1374},
publisher = {IEEE},
title = {{Learning a Mahalanobis Distance-Based Dynamic Time Warping Measure for Multivariate Time Series Classification}},
volume = {46},
year = {2016}
}
@article{Miotto2016,
abstract = {Secondary use of electronic health records (EHRs) promises to advance clinical research and better inform clinical decision making. Challenges in summarizing and representing patient data prevent widespread practice of predictive modeling using EHRs. Here we present a novel unsupervised deep feature learning method to derive a general-purpose patient representation from EHR data that facilitates clinical predictive modeling. In particular, a three-layer stack of denoising autoencoders was used to capture hierarchical regularities and dependencies in the aggregated EHRs of about 700,000 patients from the Mount Sinai data warehouse. The result is a representation we name "deep patient". We evaluated this representation as broadly predictive of health states by assessing the probability of patients to develop various diseases. We performed evaluation using 76,214 test patients comprising 78 diseases from diverse clinical domains and temporal windows. Our results significantly outperformed those achieved using representations based on raw EHR data and alternative feature learning strategies. Prediction performance for severe diabetes, schizophrenia, and various cancers were among the top performing. These findings indicate that deep learning applied to EHRs can derive patient representations that offer improved clinical predictions, and could provide a machine learning framework for augmenting clinical decision systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4290v2},
author = {Miotto, Riccardo and Li, Li and Kidd, Brian A. and Dudley, Joel T.},
doi = {10.1038/srep26094},
eprint = {arXiv:1401.4290v2},
file = {:Users/andrea/Documents/Papers/Miotto2016.pdf:pdf},
isbn = {2045-2322 (Electronic)$\backslash$r2045-2322 (Linking)},
issn = {20452322},
journal = {Scientific Reports},
number = {May},
pages = {1--10},
pmid = {27185194},
title = {{Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records}},
volume = {6},
year = {2016}
}
@article{Bellot2018,
abstract = {We present a new approach to ensemble learning for risk prognosis in heterogeneous medical populations. Our aim is to improve overall prognosis by focusing on under-represented patient subgroups with an atypical disease presentation; with current prognostic tools, these subgroups are being consistently mis-estimated. Our method proceeds sequentially by learning nonparametric survival estimators which iteratively learn to improve predictions of previously misdiagnosed patients-a process called boosting. This results in fully nonparametric survival estimates, that is, constrained neither by assumptions regarding the baseline hazard nor assumptions regarding the underlying covariate interactions-and thus differentiating our approach from existing boosting methods for survival analysis. In addition, our approach yields a measure of the relative covariate importance that accurately identifies relevant covariates within complex survival dynamics, thereby informing further medical understanding of disease interactions. We study the properties of our approach on a variety of heterogeneous medical datasets, demonstrating significant performance improvements over existing survival and ensemble methods.},
author = {Bellot, Alexis and van der Schaar, Mihaela},
file = {:Users/andrea/Documents/Papers/Bellot2018.pdf:pdf},
journal = {Proceedings of Machine Learning Research},
pages = {1--15},
title = {{Boosted Trees for Risk Prognosis}},
url = {https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/5b7371e44d7a9cee0868f69f/1534292453620/1.pdf},
volume = {85},
year = {2018}
}
@article{Zhao2016,
author = {Zhao, Jing and Papapetrou, Panagiotis and Asker, Lars and Bostr{\"{o}}m, Henrik},
doi = {10.1016/j.jbi.2016.11.006},
file = {:Users/andrea/Documents/Papers/Zhao2016.pdf:pdf},
issn = {1532-0464},
journal = {Journal of Biomedical Informatics},
pages = {105--119},
publisher = {The Author(s)},
title = {{Learning from heterogeneous temporal data in electronic health records}},
url = {http://dx.doi.org/10.1016/j.jbi.2016.11.006},
volume = {65},
year = {2017}
}
@article{Wong2017,
abstract = {Most existing theoretical results for the lasso require the samples to be iid. Recent work has provided guarantees for the lasso assuming that the time series is generated by a sparse Vector Auto-Regressive (VAR) model with Gaussian innovations. Proofs of these results rely critically on the fact that the true data generating mechanism (DGM) is a finite-order Gaussian VAR. This assumption is quite brittle: linear transformations, including selecting a subset of variables, can lead to the violation of this assumption. In order to break free from such assumptions, we derive non-asymptotic inequalities for estimation error and prediction error of the lasso estimate of the best linear predictor without assuming any special parametric form of the DGM. Instead, we rely only on (strict) stationarity and geometrically decaying {\$}\backslashbeta{\$}-mixing coefficients to establish error bounds for the lasso for subweibull random vectors. The class of subweibull random variables that we introduce includes subgaussian and subexponential random variables but also includes random variables with tails heavier than an exponential. We also show that, for Gaussian processes, the {\$}\backslashbeta{\$}-mixing condition can be relaxed to summability of the {\$}\backslashalpha{\$}-mixing coefficients. Our work provides an alternative proof of the consistency of the lasso for sparse Gaussian VAR models. But the applicability of our results extends to non-Gaussian and non-linear times series models as the examples we provide demonstrate.},
archivePrefix = {arXiv},
arxivId = {1708.01505},
author = {Wong, Kam Chung and Tewari, Ambuj},
eprint = {1708.01505},
title = {{Lasso Guarantees for {\$} \backslashbeta {\$}-Mixing Heavy Tailed Time Series}},
url = {http://arxiv.org/abs/1708.01505},
year = {2017}
}
@article{yadav2017,
abstract = {The continuously increasing cost of the US healthcare system has received significant attention. Central to the ideas aimed at curbing this trend is the use of technology, in the form of the mandate to implement electronic health records (EHRs). EHRs consist of patient information such as demographics, medications, laboratory test results, diagnosis codes and procedures. Mining EHRs could lead to improvement in patient health management as EHRs contain detailed information related to disease prognosis for large patient populations. In this manuscript, we provide a structured and comprehensive overview of data mining techniques for modeling EHR data. We first provide a detailed understanding of the major application areas to which EHR mining has been applied and then discuss the nature of EHR data and its accompanying challenges. Next, we describe major approaches used for EHR mining, the metrics associated with EHRs, and the various study designs. With this foundation, we then provide a systematic and methodological organization of existing data mining techniques used to model EHRs and discuss ideas for future research. We conclude this survey with a comprehensive summary of clinical data mining applications of EHR data, as illustrated in the online supplement.},
archivePrefix = {arXiv},
arxivId = {1702.03222},
author = {Yadav, Pranjul and Steinbach, Michael and Kumar, Vipin and Simon, Gyorgy},
doi = {10.1145/3127881},
eprint = {1702.03222},
file = {:Users/andrea/Documents/Papers/Yadav2017.pdf:pdf},
issn = {03600300},
number = {1},
pages = {1--41},
title = {{Mining Electronic Health Records: A Survey}},
url = {http://arxiv.org/abs/1702.03222},
volume = {1},
year = {2017}
}
@article{Ho2014,
abstract = {The rapidly increasing availability of electronic health records (EHRs) from multiple heterogeneous sources has spearheaded the adoption of data-driven approaches for improved clinical research, decision making, prognosis, and patient management. Unfortunately, EHR data do not always directly and reliably map to medical concepts that clinical researchers need or use. Some recent studies have focused on EHR-derived phenotyping, which aims at mapping the EHR data to specific medical concepts; however, most of these approaches require labor intensive supervision from experienced clinical professionals. Furthermore, existing approaches are often disease-centric and specialized to the idiosyncrasies of the information technology and/or business practices of a single healthcare organization. In this paper, we propose Limestone, a nonnegative tensor factorization method to derive phenotype candidates with virtually no human supervision. Limestone represents the data source interactions naturally using tensors (a generalization of matrices). In particular, we investigate the interaction of diagnoses and medications among patients. The resulting tensor factors are reported as phenotype candidates that automatically reveal patient clusters on specific diagnoses and medications. Using the proposed method, multiple phenotypes can be identified simultaneously from data. We demonstrate the capability of Limestone on a cohort of 31,815 patient records from the Geisinger Health System. The dataset spans 7. years of longitudinal patient records and was initially constructed for a heart failure onset prediction study. Our experiments demonstrate the robustness, stability, and the conciseness of Limestone-derived phenotypes. Our results show that using only 40 phenotypes, we can outperform the original 640 features (169 diagnosis categories and 471 medication types) to achieve an area under the receiver operator characteristic curve (AUC) of 0.720 (95{\%} CI 0.715 to 0.725). Moreover, in consultation with a medical expert, we confirmed 82{\%} of the top 50 candidates automatically extracted by Limestone are clinically meaningful.},
author = {Ho, Joyce C. and Ghosh, Joydeep and Steinhubl, Steve R. and Stewart, Walter F. and Denny, Joshua C. and Malin, Bradley A. and Sun, Jimeng},
doi = {10.1016/j.jbi.2014.07.001},
file = {:Users/andrea/Documents/Papers/ho2014.pdf:pdf},
isbn = {1532-0480 (Electronic)$\backslash$r1532-0464 (Linking)},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Dimensionality reduction,EHR phenotyping,Nonnegative tensor factorization},
pages = {199--211},
pmid = {25038555},
publisher = {Elsevier Inc.},
title = {{Limestone: High-throughput candidate phenotype generation via tensor factorization}},
url = {http://dx.doi.org/10.1016/j.jbi.2014.07.001},
volume = {52},
year = {2014}
}
@article{Huang2018,
abstract = {We propose a constructive approach to estimating sparse, high-dimensional linear regression models. The approach is a computational algorithm motivated from the KKT conditions for the 0-penalized least squares solutions. It generates a sequence of solutions iteratively, based on support detection using primal and dual information and root finding. We refer to the algorithm as SDAR for brevity. Under a sparse Riesz condition on the design matrix and certain other conditions, we show that with high probability, the 2 estimation error of the solution sequence decays exponentially to the minimax error bound in O(log(R √ J)) iterations, where J is the number of important predictors and R is the relative magnitude of the nonzero target coefficients; and under a mutual coherence condition and certain other conditions, the ∞ estimation error decays to the optimal error bound in O(log(R)) iterations. Moreover the SDAR solution recovers the oracle least squares estimator within a finite number of iterations with high probability if the sparsity level is known. Computational complexity analysis shows that the cost of SDAR is O(np) per iteration. We also consider an adaptive version of SDAR for use in practical applications where the true sparsity level is unknown. Simulation studies demonstrate that SDAR outperforms Lasso, MCP and two greedy methods in accuracy and efficiency.},
author = {Huang, J. and Jiao, Y. and Liu, Y. and Lu, X.},
file = {:Users/andrea/Documents/Papers/huang2018.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Geometrical convergence,KKT conditions,Nonasymptotic error bounds,Oracle property,Root finding,Support detection},
pages = {1--37},
title = {{A constructive approach to L{\textless}inf{\textgreater}0{\textless}/inf{\textgreater}penalized regression}},
volume = {19},
year = {2018}
}
@article{Shickel2018,
abstract = {The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHR). While primarily designed for archiving patient clinical information and administrative healthcare tasks, many researchers have found secondary use of these records for various clinical informatics tasks. Over the same period, the machine learning community has seen widespread advances in deep learning techniques, which also have been successfully applied to the vast amount of EHR data. In this paper, we review these deep EHR systems, examining architectures, technical aspects, and clinical applications. We also identify shortcomings of current techniques and discuss avenues of future research for EHR-based deep learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.03446v2},
author = {Shickel, Benjamin and Tighe, Patrick James and Bihorac, Azra and Rashidi, Parisa},
doi = {10.1109/JBHI.2017.2767063},
eprint = {arXiv:1706.03446v2},
file = {:Users/andrea/Documents/Papers/shickel2018.pdf:pdf},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Clinical informatics,deep learning,electronic health records,machine learning,survey},
number = {5},
pages = {1589--1604},
title = {{Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis}},
volume = {22},
year = {2018}
}
@article{Saddiki2018,
abstract = {Many questions in Data Science are fundamentally causal in that our objective is to learn the effect of some exposure (randomized or not) on an outcome interest. Even studies that are seemingly non-causal (e.g. prediction or prevalence estimation) have causal elements, such as differential censoring or measurement. As a result, we, as Data Scientists, need to consider the underlying causal mechanisms that gave rise to the data, rather than simply the pattern or association observed in the data. In this work, we review the "Causal Roadmap", a formal framework to augment our traditional statistical analyses in an effort to answer the causal questions driving our research. Specific steps of the Roadmap include clearly stating the scientific question, defining of the causal model, translating the scientific question into a causal parameter, assessing the assumptions needed to translate the causal parameter into a statistical estimand, implementation of statistical estimators including parametric and semi-parametric methods, and interpretation of our findings. Throughout we focus on the effect of an exposure occurring at a single time point and provide extensions to more advanced settings.},
archivePrefix = {arXiv},
arxivId = {1809.02408},
author = {Saddiki, Hachem and Balzer, Laura B.},
eprint = {1809.02408},
file = {:Users/andrea/Documents/Papers/saddiki2019.pdf:pdf},
keywords = {argeted maximum likelihood estimation,causal inference,causal models,dags,directed acyclic graphs,observational studies,structural,targeted learning,tmle},
pages = {1--26},
title = {{A Primer on Causality in Data Science}},
url = {http://arxiv.org/abs/1809.02408},
year = {2018}
}
@article{Chandrashekar2014,
abstract = {Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1606.03476},
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
eprint = {1606.03476},
isbn = {0045-7906},
issn = {00457906},
journal = {Computers and Electrical Engineering},
number = {1},
pages = {16--28},
pmid = {17720704},
title = {{A survey on feature selection methods}},
volume = {40},
year = {2014}
}
@article{Mikalsen2018,
abstract = {Similarity-based approaches represent a promising direction for time series analysis. However, many such methods rely on parameter tuning, and some have shortcomings if the time series are multivariate (MTS), due to dependencies between attributes, or the time series contain missing data. In this paper, we address these challenges within the powerful context of kernel methods by proposing the robust time series cluster kernel (TCK). The approach taken leverages the missing data handling properties of Gaussian mixture models (GMM) augmented with informative prior distributions. An ensemble learning approach is exploited to ensure robustness to parameters by combining the clustering results of many GMM to form the final kernel. We evaluate the TCK on synthetic and real data and compare to other state-of-the-art techniques. The experimental results demonstrate that the TCK is robust to parameter choices, provides competitive results for MTS without missing data and outstanding results for missing data.},
author = {Mikalsen, Karl {\O}yvind and Bianchi, Filippo Maria and Soguero-Ruiz, Cristina and Jenssen, Robert},
doi = {10.1016/j.patcog.2017.11.030},
file = {:Users/andrea/Documents/Papers/mikalsen2017.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Ensemble learning,Gaussian mixture models,Kernel methods,Missing data,Multivariate time series,Similarity measures},
pages = {569--581},
title = {{Time series cluster kernel for learning similarities between multivariate time series with missing data}},
volume = {76},
year = {2018}
}
@article{Karlsson2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.05183v1},
author = {Karlsson, Isak and Rebane, Jonathan and Papapetrou, Panagiotis and Gionis, Aristides},
eprint = {arXiv:1809.05183v1},
file = {:Users/andrea/Documents/Papers/Karlsson2018.pdf:pdf},
title = {{Explainable time series tweaking via irreversible and reversible temporal transformations}},
url = {https://arxiv.org/pdf/1809.05183.pdf},
year = {2018}
}
@article{Ho2014b,
abstract = {The rapidly increasing availability of electronic health records (EHRs) from multiple heterogeneous sources has spearheaded the adoption of data-driven approaches for improved clinical research, decision making, prognosis, and patient management. Unfortunately},
author = {Ho, Joyce and Ghosh, Joydeep and Sun, Jimeng},
doi = {10.1145/2623330.2623658},
file = {:Users/andrea/Documents/Papers/ho2014b.pdf:pdf},
isbn = {9781450329569},
issn = {0148-639X},
journal = {Kdd},
keywords = {ble,health records via sparse,high-throughput phenotyping from electronic,nonnegative tensor},
pages = {115--124},
title = {{Marble : High-throughput Phenotyping from Electronic Health Records via Sparse Nonnegative Tensor Factorization}},
year = {2014}
}
@article{suresh2018,
abstract = {Machine learning approaches have been effective in predicting adverse outcomes in different clinical settings. These models are often developed and evaluated on datasets with heterogeneous patient populations. However, good predictive performance on the aggregate population does not imply good performance for specific groups. In this work, we present a two-step framework to 1) learn relevant patient subgroups, and 2) predict an outcome for separate patient populations in a multi-task framework, where each population is a separate task. We demonstrate how to discover relevant groups in an unsupervised way with a sequence-to-sequence autoencoder. We show that using these groups in a multi-task framework leads to better predictive performance of in-hospital mortality both across groups and overall. We also highlight the need for more granular evaluation of performance when dealing with heterogeneous populations.},
archivePrefix = {arXiv},
arxivId = {1806.02878},
author = {Suresh, Harini and Gong, Jen J. and Guttag, John},
doi = {10.1145/3219819.3219930},
eprint = {1806.02878},
file = {:Users/andrea/Documents/Papers/suresh2018.pdf:pdf},
isbn = {9781450355520},
keywords = {clinical risk models,multi-task learning,patient subpopulation},
title = {{Learning Tasks for Multitask Learning: Heterogenous Patient Populations in the ICU}},
url = {http://arxiv.org/abs/1806.02878{\%}0Ahttp://dx.doi.org/10.1145/3219819.3219930},
year = {2018}
}
@misc{Friston2009,
abstract = {Recent advances in data analysis and modeling allow the use of fMRI data to ask not just which brain regions are involved in various cognitive and perceptual tasks, but also how they communicate with each other. Karl Friston examines two different state-of-the-art approaches to modeling brain connectivity using neuroimaging.},
annote = {Granger causal modelling (GCM) is what we call VAR, dynamic causal modelling (DCM)

generic introduction and distinction of these two models, no specific model or problem},
author = {Friston, Karl},
booktitle = {PLoS Biology},
doi = {10.1371/journal.pbio.1000033},
isbn = {1545-7885 (Electronic)$\backslash$r1544-9173 (Linking)},
issn = {15449173},
number = {2},
pages = {0220--0225},
pmid = {19226186},
title = {{Causal modelling and brain connectivity in functional magnetic resonance imaging}},
volume = {7},
year = {2009}
}
@article{Reis2003,
abstract = {BACKGROUND: Emergency department (ED) based syndromic surveillance systems identify abnormally high visit rates that may be an early signal of a bioterrorist attack. For example, an anthrax outbreak might first be detectable as an unusual increase in the number of patients reporting to the ED with respiratory symptoms. Reliably identifying these abnormal visit patterns requires a good understanding of the normal patterns of healthcare usage. Unfortunately, systematic methods for determining the expected number of (ED) visits on a particular day have not yet been well established. We present here a generalized methodology for developing models of expected ED visit rates.$\backslash$n$\backslash$nMETHODS: Using time-series methods, we developed robust models of ED utilization for the purpose of defining expected visit rates. The models were based on nearly a decade of historical data at a major metropolitan academic, tertiary care pediatric emergency department. The historical data were fit using trimmed-mean seasonal models, and additional models were fit with autoregressive integrated moving average (ARIMA) residuals to account for recent trends in the data. The detection capabilities of the model were tested with simulated outbreaks.$\backslash$n$\backslash$nRESULTS: Models were built both for overall visits and for respiratory-related visits, classified according to the chief complaint recorded at the beginning of each visit. The mean absolute percentage error of the ARIMA models was 9.37{\%} for overall visits and 27.54{\%} for respiratory visits. A simple detection system based on the ARIMA model of overall visits was able to detect 7-day-long simulated outbreaks of 30 visits per day with 100{\%} sensitivity and 97{\%} specificity. Sensitivity decreased with outbreak size, dropping to 94{\%} for outbreaks of 20 visits per day, and 57{\%} for 10 visits per day, all while maintaining a 97{\%} benchmark specificity.$\backslash$n$\backslash$nCONCLUSIONS: Time series methods applied to historical ED utilization data are an important tool for syndromic surveillance. Accurate forecasting of emergency department total utilization as well as the rates of particular syndromes is possible. The multiple models in the system account for both long-term and recent trends, and an integrated alarms strategy combining these two perspectives may provide a more complete picture to public health authorities. The systematic methodology described here can be generalized to other healthcare settings to develop automated surveillance systems capable of detecting anomalies in disease patterns and healthcare utilization.},
author = {Reis, Ben Y. and Mandl, Kenneth D.},
doi = {10.1186/1472-6947-3-1},
isbn = {1472-6947 (Electronic)$\backslash$n1472-6947 (Linking)},
issn = {14726947},
journal = {BMC Medical Informatics and Decision Making},
pages = {1--11},
pmid = {12542838},
title = {{Time series modeling for syndromic surveillance}},
volume = {3},
year = {2003}
}
@article{Goldstein2017,
abstract = {OBJECTIVE Electronic health records (EHRs) are an increasingly common data source for clinical risk prediction, presenting both unique analytic opportunities and challenges. We sought to evaluate the current state of EHR based risk prediction modeling through a systematic review of clinical prediction studies using EHR data. METHODS We searched PubMed for articles that reported on the use of an EHR to develop a risk prediction model from 2009 to 2014. Articles were extracted by two reviewers, and we abstracted information on study design, use of EHR data, model building, and performance from each publication and supplementary documentation. RESULTS We identified 107 articles from 15 different countries. Studies were generally very large (median sample size = 26 100) and utilized a diverse array of predictors. Most used validation techniques (n = 94 of 107) and reported model coefficients for reproducibility (n = 83). However, studies did not fully leverage the breadth of EHR data, as they uncommonly used longitudinal information (n = 37) and employed relatively few predictor variables (median = 27 variables). Less than half of the studies were multicenter (n = 50) and only 26 performed validation across sites. Many studies did not fully address biases of EHR data such as missing data or loss to follow-up. Average c-statistics for different outcomes were: mortality (0.84), clinical prediction (0.83), hospitalization (0.71), and service utilization (0.71). CONCLUSIONS EHR data present both opportunities and challenges for clinical risk prediction. There is room for improvement in designing such studies.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.2609v5},
author = {Goldstein, Benjamin A. and Navar, Ann Marie and Pencina, Michael J. and Ioannidis, John P.A.},
doi = {10.1093/jamia/ocw042},
eprint = {arXiv:1301.2609v5},
file = {:Users/andrea/Documents/Papers/Goldstein2016.pdf:pdf},
isbn = {1067-5027},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {Electronic medical record,Review,Risk assessment},
number = {1},
pages = {198--208},
pmid = {27189013},
title = {{Opportunities and challenges in developing risk prediction models with electronic health records data: A systematic review}},
url = {https://watermark.silverchair.com/ocw042.pdf?token=AQECAHi208BE49Ooan9kkhW{\_}Ercy7Dm3ZL{\_}9Cf3qfKAc485ysgAAAkIwggI-BgkqhkiG9w0BBwagggIvMIICKwIBADCCAiQGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM-XHZWKawFeG-05ZNAgEQgIIB9VoYaTQaLtGFEZWqnSCEGzErkb4vn2ccku-4RiA4QASZlFg4},
volume = {24},
year = {2017}
}
@article{Feng2018,
abstract = {PURPOSE While the use of transthoracic echocardiography (TTE) in the ICU is rapidly expanding, the contribution of TTE to altering patient outcomes among ICU patients with sepsis has not been examined. This study was designed to examine the association of TTE with 28-day mortality specifically in that population. METHODS AND RESULTS The MIMIC-III database was employed to identify patients with sepsis who had and had not received TTE. The statistical approaches utilized included multivariate regression, propensity score analysis, doubly robust estimation, the gradient boosted model, and an inverse probability-weighting model to ensure the robustness of our findings. Significant benefit in terms of 28-day mortality was observed among the TTE patients compared to the control (no TTE) group (odds ratio = 0.78, 95{\%} CI 0.68-0.90, p {\textless} 0.001). The amount of fluid administered (2.5 vs. 2.1 L on day 1, p {\textless} 0.001), use of dobutamine (2{\%} vs. 1{\%}, p = 0.007), and the maximum dose of norepinephrine (1.4 vs. 1 mg/min, p = 0.001) were significantly higher for the TTE patients. Importantly, the TTE patients were weaned off vasopressors more quickly than those in the no TTE group (vasopressor-free days on day 28 of 21 vs. 19, p = 0.004). CONCLUSION In a general population of critically ill patients with sepsis, use of TTE is associated with an improvement in 28-day mortality.},
author = {Feng, Mengling and McSparron, Jakob I. and Kien, Dang Trung and Stone, David J. and Roberts, David H. and Schwartzstein, Richard M. and Vieillard-Baron, Antoine and Celi, Leo Anthony},
doi = {10.1007/s00134-018-5208-7},
file = {:Users/andrea/Documents/Papers/Feng2018.pdf:pdf},
issn = {14321238},
journal = {Intensive Care Medicine},
keywords = {Critical care,Echocardiography,Sepsis,Value},
number = {6},
pages = {884--892},
pmid = {29806057},
publisher = {Springer Berlin Heidelberg},
title = {{Transthoracic echocardiography and mortality in sepsis: analysis of the MIMIC-III database}},
url = {https://doi.org/10.1007/s00134-018-5208-7},
volume = {44},
year = {2018}
}
@article{liu2918b,
archivePrefix = {arXiv},
arxivId = {arXiv:1811.11400v2},
author = {Liu, Dianbo and Miller, Timothy and Sayeed, Raheel and Mandl, Kenneth D and Health, Computational and Program, Informatics and Autumn, One and Boston, Street},
eprint = {arXiv:1811.11400v2},
file = {:Users/andrea/Documents/Papers/liu2018b.pdf:pdf},
title = {{FADL : Federated-Autonomous Deep Learning for Distributed Electronic Health Record}},
year = {2018}
}
@article{choi2018b,
abstract = {Deep learning models exhibit state-of-the-art performance for many predictive healthcare tasks using electronic health records (EHR) data, but these models typically require training data volume that exceeds the capacity of most healthcare systems. External resources such as medical ontologies are used to bridge the data volume constraint, but this approach is often not directly applicable or useful because of inconsistencies with terminology. To solve the data insufficiency challenge, we leverage the inherent multilevel structure of EHR data and, in particular, the encoded relationships among medical codes. We propose Multilevel Medical Embedding (MiME) which learns the multilevel embedding of EHR data while jointly performing auxiliary prediction tasks that rely on this inherent EHR structure without the need for external labels. We conducted two prediction tasks, heart failure prediction and sequential disease prediction, where MiME outperformed baseline methods in diverse evaluation settings. In particular, MiME consistently outperformed all baselines when predicting heart failure on datasets of different volumes, especially demonstrating the greatest performance improvement (15{\%} relative gain in PR-AUC over the best baseline) on the smallest dataset, demonstrating its ability to effectively model the multilevel structure of EHR data.},
archivePrefix = {arXiv},
arxivId = {1810.09593},
author = {Choi, Edward and Xiao, Cao and Stewart, Walter F. and Sun, Jimeng},
eprint = {1810.09593},
file = {:Users/andrea/Documents/Papers/choi2018b.pdf:pdf},
number = {Nips},
title = {{MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare}},
url = {http://arxiv.org/abs/1810.09593},
year = {2018}
}
@article{Senin2013,
author = {Senin, Pavel and Malinchik, Sergey},
file = {:Users/andrea/Documents/Papers/senin2013.pdf:pdf},
title = {{Interpretable Time Series Classification Using SAX and Vector Space Model}}
}
@article{Hu2016,
abstract = {In this paper, we investigate a group sparse optimization problem via {\$}\backslashell{\_}{\{}p,q{\}}{\$} regularization in three aspects: theory, algorithm and application. In the theoretical aspect, by introducing a notion of group restricted eigenvalue condition, we establish some oracle property and a global recovery bound of order {\$}O(\backslashlambda{\^{}}\backslashfrac{\{}2{\}}{\{}2-q{\}}){\$} for any point in a level set of the {\$}\backslashell{\_}{\{}p,q{\}}{\$} regularization problem, and by virtue of modern variational analysis techniques, we also provide a local analysis of recovery bound of order {\$}O(\backslashlambda{\^{}}2){\$} for a path of local minima. In the algorithmic aspect, we apply the well-known proximal gradient method to solve the {\$}\backslashell{\_}{\{}p,q{\}}{\$} regularization problems, either by analytically solving some specific {\$}\backslashell{\_}{\{}p,q{\}}{\$} regularization subproblems, or by using the Newton method to solve general {\$}\backslashell{\_}{\{}p,q{\}}{\$} regularization subproblems. In particular, we establish the linear convergence rate of the proximal gradient method for solving the {\$}\backslashell{\_}{\{}1,q{\}}{\$} regularization problem under some mild conditions. As a consequence, the linear convergence rate of proximal gradient method for solving the usual {\$}\backslashell{\_}{\{}q{\}}{\$} regularization problem ({\$}0{\textless}q{\textless}1{\$}) is obtained. Finally in the aspect of application, we present some numerical results on both the simulated data and the real data in gene transcriptional regulation.},
archivePrefix = {arXiv},
arxivId = {1601.07779},
author = {Hu, Yaohua and Li, Chong and Meng, Kaiwen and Qin, Jing and Yang, Xiaoqi},
eprint = {1601.07779},
issn = {15337928},
pages = {1--52},
title = {{Group sparse optimization via {\$}\backslashell{\_}{\{}p,q{\}}{\$} regularization}},
url = {http://arxiv.org/abs/1601.07779},
volume = {18},
year = {2016}
}
@article{Oliveira2019,
abstract = {Objective
The collaboration and knowledge exchange between researchers are often hindered by the nonexistence of accurate information about which databases may support research studies. Even though a considerable amount of patient health information does exist, it is usually distributed and hidden in many institutions. The goal of this project is to provide, for any research community, a holistic view of biomedical datasets of interests, from which researchers can explore several distinct levels of granularity.
Methods
We developed a community-centered approach to facilitate data sharing while ensuring privacy. A dynamic schema allows exposing any metadata model about existing repositories. The framework was developed following a modular plugin-based architecture that facilitates the integration of internal and external tools.
Results
The EMIF Catalogue, a web platform for sharing and reusing biomedical data. Through this system, data custodians can publish and share different levels of information, while the researchers can search for databases that fulfill research requirements.
Conclusions
The EMIF Catalogue currently fosters several distinct research communities, with different levels of data governance, combining, for instance, data available in pan-European EHR and Alzheimer cohorts. This portal is publicly available at https://emif-catalogue.eu.},
author = {Oliveira, Jos{\'{e}} Lu{\'{i}}s and Trifan, Alina and {Basti{\~{a}}o Silva}, Lu{\'{i}}s A.},
doi = {10.1016/j.ijmedinf.2019.02.006},
file = {:Users/andrea/Documents/Papers/oliverira2019.pdf:pdf},
issn = {13865056},
journal = {International Journal of Medical Informatics},
keywords = {Biomedical data integration,Data catalogue,Data discovery,Data reuse,Data sharing,Research study,biomedical data integration},
number = {February},
pages = {35--45},
publisher = {Elsevier},
title = {{EMIF Catalogue: A collaborative platform for sharing and reusing biomedical data}},
url = {https://doi.org/10.1016/j.ijmedinf.2019.02.006},
volume = {126},
year = {2019}
}
@article{Tonekaboni2018,
author = {Tonekaboni, Sana and Mazwi, Mjaye and Laussen, Peter and Eytan, Danny and Greer, Robert and Goodfellow, Sebastian D and Goodwin, Andrew and Brudno, Michael and Goldenberg, Anna},
file = {:Users/andrea/Documents/Papers/Tonekaboshi.pdf:pdf},
pages = {1--16},
title = {{Prediction of Cardiac Arrest from Physiological Signals in the Pediatric ICU}},
url = {https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/5b73738d88251beb6017ede8/1534343143656/Tonekaboni{\_}S}
}
@article{Makridakis2018,
abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1371/journal.pone.0194889},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {3},
pages = {1--26},
pmid = {29584784},
title = {{Statistical and Machine Learning forecasting methods: Concerns and ways forward}},
volume = {13},
year = {2018}
}
@article{Wood2009,
abstract = {Saber rattling is a prominent tool of the {\{}U.S.{\}} president's foreign$\backslash$npolicy leadership. Yet there has been no study of how presidential$\backslash$nsaber rattling affects international or domestic political outcomes.$\backslash$nThis study evaluates how presidential saber rattling affects {\{}U.S.{\}}$\backslash$neconomic behavior and performance. Theoretically, the study demonstrates$\backslash$nthat presidential rhetoric affects the risks that economic actors$\backslash$nare willing to take, as well as the consequences of these resulting$\backslash$nbehaviors for {\{}U.S.{\}} economic performance. Using monthly time series$\backslash$nrunning from January 1978 through January 2005, vector autoregression$\backslash$nmethods are applied to show that increased presidential saber rattling$\backslash$nproduces increased perceptions of negative economic news, declining$\backslash$nconsumer confidence, lower personal consumption expenditures, less$\backslash$ndemand for money, and slower economic growth. More broadly, the study$\backslash$ndemonstrates an important linkage between the president's two most$\backslash$nimportant roles: foreign and economic policy leadership. The president's$\backslash$nforeign policy pronouncements not only impact other nations, but$\backslash$nalso affect domestic economic outcomes.},
annote = {application 
VAR in political science},
author = {Wood, B. Dan},
doi = {10.1111/j.1540-5907.2009.00395.x},
isbn = {0092-5853},
issn = {00925853},
journal = {American Journal of Political Science},
number = {3},
pages = {695--709},
title = {{Presidential saber rattling and the economy}},
volume = {53},
year = {2009}
}
@article{Xiao2018,
abstract = {Objective: To conduct a systematic review of deep learning models for electronic health record (EHR) data, and illustrate various deep learning architectures for analyzing different data sources and their target applications. We also highlight ongoing research and identify open challenges in building deep learning models of EHRs. Design/method: We searched PubMed and Google Scholar for papers on deep learning studies using EHR data published between January 1, 2010, and January 31, 2018. We summarize them according to these axes: types of analytics tasks, types of deep learning model architectures, special challenges arising from health data and tasks and their potential solutions, as well as evaluation strategies. Results: We surveyed and analyzed multiple aspects of the 98 articles we found and identified the following an-alytics tasks: disease detection/classification, sequential prediction of clinical events, concept embedding, data augmentation, and EHR data privacy. We then studied how deep architectures were applied to these tasks. We also discussed some special challenges arising from modeling EHR data and reviewed a few popular approaches. Finally, we summarized how performance evaluations were conducted for each task. Discussion: Despite the early success in using deep learning for health analytics applications, there still exist a number of issues to be addressed. We discuss them in detail including data and label availability, the interpret-ability and transparency of the model, and ease of deployment.},
author = {Xiao, Cao and Choi, Edward and Sun, Jimeng},
doi = {10.1093/jamia/ocy068},
file = {:Users/andrea/Documents/Papers/xiao2018.pdf:pdf},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {deep learning,electronic health records,neural networks,systematic review},
number = {10},
pages = {1419--1428},
title = {{Opportunities and challenges in developing deep learning models using electronic health records data: A systematic review}},
volume = {25},
year = {2018}
}
@article{Nicholson2017,
abstract = {The vector autoregression (VAR) has long proven to be an effective method for modeling the joint dynamics of macroeconomic time series, as well as for forecasting. One major shortcoming of the VAR that has limited its applicability is its heavy parameterization: the parameter space grows quadratically with the number of series included, quickly exhausting the available degrees of freedom. Consequently, using VARs for forecasting is intractable for low-frequency, high-dimensional macroeconomic data. However, empirical evidence suggests that VARs that incorporate more component series tend to result in more accurate forecasts. Most conventional methods that allow for the estimation of large VARs either require ad hoc subjective specifications or are computationally infeasible. Moreover, as global economies become more intricately intertwined, there has been a substantial interest in incorporating the impact of stochastic, unmodeled exogenous variables. Vector autoregression with exogenous variables (VARX) extends the VAR to allow for the inclusion of unmodeled variables, but faces similar dimensionality challenges. This paper introduces the VARX-L framework, a structured family of VARX models, and provides a methodology that allows for both efficient estimation and accurate forecasting in high-dimensional analysis. VARX-L adapts several prominent scalar regression regularization techniques to a vector time series context, which greatly reduces the parameter space of VAR and VARX models. We also highlight a compelling extension that allows for shrinking toward reference models, such as a vector random walk. We demonstrate the efficacy of VARX-L in both low- and high-dimensional macroeconomic forecasting applications and simulated data examples. Our methodology is easy to reproduce in a publicly available R package.},
archivePrefix = {arXiv},
arxivId = {1508.07497},
author = {Nicholson, William B. and Matteson, David S. and Bien, Jacob},
doi = {10.1016/j.ijforecast.2017.01.003},
eprint = {1508.07497},
file = {::},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Big data,Forecasting,Group lasso,Macroeconometrics,Time series},
number = {3},
pages = {627--651},
title = {{VARX-L: Structured regularization for large vector autoregressions with exogenous variables}},
volume = {33},
year = {2017}
}
@article{Morel2018,
abstract = {In this paper, we propose an innovative averaging of a set of time-series based on the Dynamic Time Warping (DTW). The DTW is widely used in data mining since it provides not only a similarity measure, but also a temporal alignment of time-series. However, its use is often restricted to the case of a pair of signals. In this paper, we propose to extend its application to a set of signals by providing an average time-series that opens a wide range of applications in data mining process. Starting with an existing well-established method called DBA (for DTW Barycenter Averaging), this paper points out its limitations and suggests an alternative based on a Constrained Dynamic Time Warping. Secondly, an innovative tolerance is added to take into account the admissible variability around the average signal. This new modeling of time-series is evaluated on a classification task applied on several datasets and results show that it outperforms state of the art methods.},
author = {Morel, Marion and Achard, Catherine and Kulpa, Richard and Dubuisson, S{\'{e}}verine},
doi = {10.1016/j.patcog.2017.08.015},
file = {:Users/andrea/Documents/Papers/morel2017.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Constrained DTW barycenter averaging,Dynamic time warping,Local constraints,Time-series averaging,Time-series classification},
pages = {77--89},
publisher = {Elsevier Ltd},
title = {{Time-series averaging using constrained dynamic time warping with tolerance}},
volume = {74},
year = {2018}
}
@article{Estiri2019,
author = {Estiri, Hossein and Mccoy, Thomas H and Wagholikar, Kavishwar B and Goodson, Alyssa P and Murphy, Katie and Murphy, N},
file = {:Users/andrea/Documents/Papers/estiri2019.pdf:pdf},
title = {{High-throughput Phenotyping with Temporal Sequences}},
year = {2019}
}
@article{Zheng2017,
abstract = {Objective To discover diverse genotype-phenotype associations affiliated with Type 2 Diabetes Mellitus (T2DM) via genome-wide association study (GWAS) and phenome-wide association study (PheWAS), more cases (T2DM subjects) and controls (subjects without T2DM) are required to be identified (e.g., via Electronic Health Records (EHR)). However, existing expert based identification algorithms often suffer in a low recall rate and could miss a large number of valuable samples under conservative filtering standards. The goal of this work is to develop a semi-automated framework based on machine learning as a pilot study to liberalize filtering criteria to improve recall rate with a keeping of low false positive rate. Materials and methods We propose a data informed framework for identifying subjects with and without T2DM from EHR via feature engineering and machine learning. We evaluate and contrast the identification performance of widely-used machine learning models within our framework, including k-Nearest-Neighbors, Na{\"{i}}ve Bayes, Decision Tree, Random Forest, Support Vector Machine and Logistic Regression. Our framework was conducted on 300 patient samples (161 cases, 60 controls and 79 unconfirmed subjects), randomly selected from 23,281 diabetes related cohort retrieved from a regional distributed EHR repository ranging from 2012 to 2014. Results We apply top-performing machine learning algorithms on the engineered features. We benchmark and contrast the accuracy, precision, AUC, sensitivity and specificity of classification models against the state-of-the-art expert algorithm for identification of T2DM subjects. Our results indicate that the framework achieved high identification performances (∼0.98 in average AUC), which are much higher than the state-of-the-art algorithm (0.71 in AUC). Discussion Expert algorithm-based identification of T2DM subjects from EHR is often hampered by the high missing rates due to their conservative selection criteria. Our framework leverages machine learning and feature engineering to loosen such selection criteria to achieve a high identification rate of cases and controls. Conclusions Our proposed framework demonstrates a more accurate and efficient approach for identifying subjects with and without T2DM from EHR.},
author = {Zheng, Tao and Xie, Wei and Xu, Liling and He, Xiaoying and Zhang, Ya and You, Mingrong and Yang, Gong and Chen, You},
doi = {10.1016/j.ijmedinf.2016.09.014},
isbn = {9783319626970},
issn = {18728243},
journal = {International Journal of Medical Informatics},
keywords = {Data mining,Electronic health records,Feature engineering,Machine learning,Type 2 diabetes},
pages = {120--127},
pmid = {27919371},
publisher = {Elsevier Ireland Ltd},
title = {{A machine learning-based framework to identify type 2 diabetes through electronic health records}},
url = {http://dx.doi.org/10.1016/j.ijmedinf.2016.09.014},
volume = {97},
year = {2017}
}
@article{Komorowski2017,
abstract = {Abstract unavailable for this article.},
author = {Komorowski, Matthieu and Celi, Leo Anthony},
doi = {10.1097/CCM.0000000000002351},
file = {:Users/andrea/Documents/Papers/komorowski2017.pdf:pdf},
isbn = {0090-3493},
issn = {15300293},
journal = {Critical Care Medicine},
number = {5},
pages = {912--913},
pmid = {28410309},
title = {{Will Artificial Intelligence Contribute to Overuse in Healthcare?*}},
url = {http://insights.ovid.com/crossref?an=00003246-201705000-00024},
volume = {45},
year = {2017}
}
@article{Poucke2016,
abstract = {With the accumulation of large amounts of health related data, predictive analytics could stimulate the transformation of reactive medicine towards Predictive, Preventive and Personalized (PPPM) Medicine, ultimately affecting both cost and quality of care. However, high-dimensionality and high-complexity of the data involved, prevents data-driven methods from easy translation into clinically relevant models. Additionally, the application of cutting edge predictive methods and data manipulation require substantial programming skills, limiting its direct exploitation by medical domain experts. This leaves a gap between potential and actual data usage. In this study, the authors address this problem by focusing on open, visual environments, suited to be applied by the medical community. Moreover, we review code free applications of big data technologies. As a showcase, a framework was developed for the meaningful use of data from critical care patients by integrating the MIMIC-II database in a data mining environment (RapidMiner) supporting scalable predictive analytics using visual tools (RapidMiner's Radoop extension). Guided by the CRoss-Industry Standard Process for Data Mining (CRISP-DM), the ETL process (Extract, Transform, Load) was initiated by retrieving data from the MIMIC-II tables of interest. As use case, correlation of platelet count and ICU survival was quantitatively assessed. Using visual tools for ETL on Hadoop and predictive modeling in RapidMiner, we developed robust processes for automatic building, parameter optimization and evaluation of various predictive models, under different feature selection schemes. Because these processes can be easily adopted in other projects, this environment is attractive for scalable predictive analytics in health research.},
author = {Poucke, Sven Van and Zhang, Zhongheng and Schmitz, Martin and Vukicevic, Milan and Laenen, Margot Vander and Celi, Leo Anthony and {De Deyne}, Cathy},
doi = {10.1371/journal.pone.0145791},
file = {:Users/andrea/Documents/Papers/poucke2016.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
pages = {1--21},
pmid = {26731286},
title = {{Scalable predictive analysis in critically ill patients using a visual open data analysis platform}},
volume = {11},
year = {2016}
}
@article{Lee2014,
abstract = {Recently, to solve large-scale lasso and group lasso problems, screening rules have been developed, the goal of which is to reduce the problem size by efficiently discarding zero coefficients using simple rules independently of the others. However, screening for overlapping group lasso remains an open challenge because the overlaps between groups make it infeasible to test each group independently. In this paper, we develop screening rules for overlapping group lasso. To address the challenge arising from groups with overlaps, we take into account overlapping groups only if they are inclusive of the group being tested, and then we derive screening rules, adopting the dual polytope projection approach. This strategy allows us to screen each group independently of each other. In our experiments, we demonstrate the efficiency of our screening rules on various datasets.},
archivePrefix = {arXiv},
arxivId = {1410.6880},
author = {Lee, Seunghak and Xing, EP},
eprint = {1410.6880},
journal = {arXiv preprint arXiv:1410.6880},
title = {{Screening Rules for Overlapping Group Lasso}},
url = {http://arxiv.org/abs/1410.6880},
year = {2014}
}
@article{Harris2018,
abstract = {Objective: To build and curate a linkable multi-centre database of high resolution longitudinal electronic health records (EHR) from adult Intensive Care Units (ICU). To develop a set of open-source tools to make these data ‘research ready' while protecting patient's privacy with a particular focus on anonymisation. Materials and methods: We developed a scalable EHR processing pipeline for extracting, linking, normalising and curating and anonymising EHR data. Patient and public involvement was sought from the outset, and approval to hold these data was granted by the NHS Health Research Authority's Confidentiality Advisory Group (CAG). The data are held in a certified Data Safe Haven. We followed sustainable software development principles throughout, and defined and populated a common data model that links to other clinical areas. Results: Longitudinal EHR data were loaded into the CCHIC database from eleven adult ICUs at 5 UK teaching hospitals. From January 2014 to January 2017, this amounted to 21,930 and admissions (18,074 unique patients). Typical admissions have 70 data-items pertaining to admission and discharge, and a median of 1030 (IQR 481–2335) time-varying measures. Training datasets were made available through virtual machine images emulating the data processing environment. An open source R package, cleanEHR, was developed and released that transforms the data into a square table readily analysable by most statistical packages. A simple language agnostic configuration file will allow the user to select and clean variables, and impute missing data. An audit trail makes clear the provenance of the data at all times. Discussion: Making health care data available for research is problematic. CCHIC is a unique multi-centre longitudinal and linkable resource that prioritises patient privacy through the highest standards of data security, but also provides tools to clean, organise, and anonymise the data. We believe the development of such tools are essential if we are to meet the twin requirements of respecting patient privacy and working for patient benefit. Conclusion: The CCHIC database is now in use by health care researchers from academia and industry. The ‘research ready' suite of data preparation tools have facilitated access, and linkage to national databases of secondary care is underway.},
author = {Harris, Steve and Shi, Sinan and Brealey, David and MacCallum, Niall S. and Denaxas, Spiros and Perez-Suarez, David and Ercole, Ari and Watkinson, Peter and Jones, Andrew and Ashworth, Simon and Beale, Richard and Young, Duncan and Brett, Stephen and Singer, Mervyn},
doi = {10.1016/j.ijmedinf.2018.01.006},
file = {:Users/andrea/Documents/Papers/harris2018.pdf:pdf},
issn = {18728243},
journal = {International Journal of Medical Informatics},
keywords = {Clinical decision support,Critical care,Database,Electronic health records,Reproducibility},
number = {January},
pages = {82--89},
publisher = {Elsevier},
title = {{Critical Care Health Informatics Collaborative (CCHIC): Data, tools and methods for reproducible research: A multi-centre UK intensive care database}},
url = {https://doi.org/10.1016/j.ijmedinf.2018.01.006},
volume = {112},
year = {2018}
}
@article{Wang2007,
abstract = {The least absolute shrinkage and selection operator (‘lasso') has been widely used in regression shrinkage and selection.We extend its application to the regression model with autoregressive errors. Two types of lasso estimators are carefully studied. The first is similar to the traditional lasso estimator with only two tuning parameters (one for regression coeffi- cients and the other for autoregression coefficients). These tuning parameters can be easily calculated via a data-driven method, but the resulting lasso estimator may not be fully efficient. To overcome this limitation, we propose a second lasso estimator which uses different tuning parameters for each coefficient.We show that this modified lasso can produce the estimator as efficiently as the oracle. Moreover, we propose an algorithm for tuning parameter estimates to obtain the modified lasso estimator. Simulation studies demonstrate that the modified estimator is superior to the traditional estimator. One empirical example is also presented to illustrate the usefulness of lasso estimators.The extension of the lasso to the autoregression with exogenous variables model is briefly discussed.},
author = {Wang, Hansheng and Li, Guodong and Tsai, Chin Ling},
doi = {10.1111/j.1467-9868.2007.00577.x},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Autoregression with exogenous variables,Lasso,Oracle estimator,Regression model with autoregressive errors},
number = {1},
pages = {63--78},
title = {{Regression coefficient and autoregressive order shrinkage and selection via the lasso}},
volume = {69},
year = {2007}
}
@article{Ustun2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04269v3},
author = {Ustun, Berk and Rudin, Cynthia},
eprint = {arXiv:1502.04269v3},
number = {1502},
title = {{Supersparse Linear Integer Models for Optimized Medical Scoring Systems}},
year = {2001}
}
@article{lim2018,
abstract = {Joint models for longitudinal and time-to-event data are commonly used in longitudinal studies to forecast disease trajectories over time. While there are many advantages to joint modeling, the standard forms suffer from limitations that arise from a fixed model specification, and computational difficulties when applied to high-dimensional datasets. In this paper, we propose a deep learning approach to address these limitations, enhancing existing methods with the inherent flexibility and scalability of deep neural networks, while retaining the benefits of joint modeling. Using longitudinal data from a real-world medical dataset, we demonstrate improvements in performance and scalability, as well as robustness in the presence of irregularly sampled data.},
archivePrefix = {arXiv},
arxivId = {1803.10254},
author = {Lim, Bryan and van der Schaar, Mihaela},
eprint = {1803.10254},
file = {:Users/andrea/Documents/Papers/lim2018.pdf:pdf},
pages = {1--23},
title = {{Disease-Atlas: Navigating Disease Trajectories with Deep Learning}},
url = {http://arxiv.org/abs/1803.10254},
year = {2018}
}
@article{Fiterau2017,
abstract = {In healthcare applications, temporal variables that encode movement, health status and longitudinal patient evolution are often accompanied by rich structured information such as demographics, diagnostics and medical exam data. However, current methods do not jointly optimize over structured covariates and time series in the feature extraction process. We present ShortFuse, a method that boosts the accuracy of deep learning models for time series by explicitly modeling temporal interactions and dependencies with structured covariates. ShortFuse introduces hybrid convolutional and LSTM cells that incorporate the covariates via weights that are shared across the temporal domain. ShortFuse outperforms competing models by 3{\%} on two biomedical applications, forecasting osteoarthritis-related cartilage degeneration and predicting surgical outcomes for cerebral palsy patients, matching or exceeding the accuracy of models that use features engineered by domain experts.},
archivePrefix = {arXiv},
arxivId = {1705.04790},
author = {Fiterau, Madalina and Bhooshan, Suvrat and Fries, Jason and Bournhonesque, Charles and Hicks, Jennifer and Halilaj, Eni and R{\'{e}}, Christopher and Delp, Scott},
eprint = {1705.04790},
keywords = {NotMIMIC},
mendeley-tags = {NotMIMIC},
pages = {1--15},
title = {{ShortFuse: Biomedical Time Series Representations in the Presence of Structured Information}},
url = {http://arxiv.org/abs/1705.04790},
year = {2017}
}
@article{Liu2010,
abstract = {The group Lasso is an extension of the Lasso for feature selection on (predefined) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efficient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results demonstrate the efficiency and effectiveness of the proposed algorithm.},
archivePrefix = {arXiv},
arxivId = {1009.0306},
author = {Liu, Jun and Ye, Jieping},
eprint = {1009.0306},
journal = {Arxiv preprint arXiv:1009.0306},
pages = {1--14},
title = {{Fast Overlapping Group Lasso}},
url = {http://arxiv.org/abs/1009.0306},
year = {2010}
}
@article{Jacobson2016,
abstract = {Detecting healthcare-associated infections pose a major challenge in healthcare. Us-ing natural language processing and ma-chine learning applied on electronic pa-tient records is one approach that has been shown to work. However the results indi-cate that there was room for improvement and therefore we have applied deep learn-ing methods. Specifically we implemented a network of stacked sparse auto encoders and a network of stacked restricted Boltz-mann machines. Our best results were ob-tained using the stacked restricted Boltz-mann machines with a precision of 0.79 and a recall of 0.88.},
author = {Jacobson, Olof and Dalianis, Hercules},
doi = {10.18653/v1/w16-2926},
file = {:Users/andrea/Documents/Papers/Jacobson2016.pdf:pdf},
pages = {191--195},
title = {{Applying deep learning on electronic health records in Swedish to predict healthcare-associated infections}},
year = {2016}
}
@inproceedings{Shojaie2011,
abstract = {MOTIVATION: Components of biological systems interact with each other in order to carry out vital cell functions. Such information can be used to improve estimation and inference, and to obtain better insights into the underlying cellular mechanisms. Discovering regulatory interactions among genes is therefore an important problem in systems biology. Whole-genome expression data over time provides an opportunity to determine how the expression levels of genes are affected by changes in transcription levels of other genes, and can therefore be used to discover regulatory interactions among genes.$\backslash$n$\backslash$nRESULTS: In this article, we propose a novel penalization method, called truncating lasso, for estimation of causal relationships from time-course gene expression data. The proposed penalty can correctly determine the order of the underlying time series, and improves the performance of the lasso-type estimators. Moreover, the resulting estimate provides information on the time lag between activation of transcription factors and their effects on regulated genes. We provide an efficient algorithm for estimation of model parameters, and show that the proposed method can consistently discover causal relationships in the large p, small n setting. The performance of the proposed model is evaluated favorably in simulated, as well as real, data examples.$\backslash$n$\backslash$nAVAILABILITY: The proposed truncating lasso method is implemented in the R-package 'grangerTlasso' and is freely available at http://www.stat.lsa.umich.edu/{\~{}}shojaie/.},
annote = {Proposed a novel penalisation method (truncating lasso) to estimate causal relationships from time-course gene expression data

The method can correctly determine the order of underlying time series, and improve the performance of lasso-type estimators

estimation of DAG, graphical granger causality

they're still doing VAR but in a graphical setting... strange!},
archivePrefix = {arXiv},
arxivId = {1007.0499},
author = {Shojaie, Ali and Michailidis, George},
booktitle = {Bioinformatics},
doi = {10.1093/bioinformatics/btq377},
eprint = {1007.0499},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {14602059},
number = {13},
pages = {i517--i523},
pmid = {20823316},
title = {{Discovering graphical Granger causality using the truncating lasso penalty}},
volume = {27},
year = {2011}
}
@article{Negahban2017,
abstract = {When tracking user-specific online activities, each user's preference is revealed in the form of choices and comparisons. For example, a user's purchase history tracks her choices, i.e. which item was chosen among a subset of offerings. A user's comparisons are observed either explicitly as in movie ratings or implicitly as in viewing times of news articles. Given such individualized ordinal data, we address the problem of collaboratively learning representations of the users and the items. The learned features can be used to predict a user's preference of an unseen item to be used in recommendation systems. This also allows one to compute similarities among users and items to be used for categorization and search. Motivated by the empirical successes of the MultiNomial Logit (MNL) model in marketing and transportation, and also more recent successes in word embedding and crowdsourced image embedding, we pose this problem as learning the MNL model parameters that best explains the data. We propose a convex optimization for learning the MNL model, and show that it is minimax optimal up to a logarithmic factor by comparing its performance to a fundamental lower bound. This characterizes the minimax sample complexity of the problem, and proves that the proposed estimator cannot be improved upon other than by a logarithmic factor. Further, the analysis identifies how the accuracy depends on the topology of sampling via the spectrum of the sampling graph. This provides a guideline for designing surveys when one can choose which items are to be compared. This is accompanies by numerical simulations on synthetic and real datasets confirming our theoretical predictions.},
archivePrefix = {arXiv},
arxivId = {1704.07228},
author = {Negahban, Sahand and Oh, Sewoong and Thekumparampil, Kiran K. and Xu, Jiaming},
eprint = {1704.07228},
file = {:Users/andrea/Documents/Papers/negahban2018.pdf:pdf},
pages = {1--95},
title = {{Learning from Comparisons and Choices}},
url = {http://arxiv.org/abs/1704.07228},
volume = {19},
year = {2017}
}
@article{Kruse2016,
abstract = {Background: Big data analytics offers promise in many business sectors, and health care is looking at big data to provide answers to many age-related issues, particularly dementia and chronic disease management.},
author = {Kruse, Clemens Scott and Goswamy, Rishi and Raval, Yesha and Marawi, Sarah},
doi = {10.2196/medinform.5359},
file = {:Users/andrea/Documents/Papers/kruse2016.pdf:pdf},
journal = {JMIR Medical Informatics},
keywords = {analytics,big data,electronic medical record,health care,human genome},
number = {4},
pages = {e38},
title = {{Challenges and Opportunities of Big Data in Health Care: A Systematic Review}},
volume = {4},
year = {2016}
}
@article{johnson2018b,
abstract = {Real-time prediction of mortality for intensive care unit patients has the potential to provide physicians with a simple and easily interpretable synthesis of patient acuity. Here we extract data from a random time during each patient's ICU stay. We believe this sampling scheme allows for the application of the model(s) across a future patient's entire ICU stay. The AUROC of a Gradient Boosting model was high (AUROC=0.920), even though no information about diagnosis or comorbid burden was utilized. We also compare models using data from the first 24 hours of a patient's stay against published severity of illness scores, and find the Gradient Boosting model greatly outperformed the frequently used Simplified Acute Physiology Score II (AUROC = 0.927 vs. 0.809). We nuance this performance with comparison to the literature, provide our interpretation, and discuss potential avenues for improvement.},
author = {Johnson, Alistair E W and Mark, Roger G},
file = {:Users/andrea/Documents/Papers/johnson2018b.pdf:pdf},
issn = {1942-597X},
journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
pages = {994--1003},
pmid = {29854167},
title = {{Real-time mortality prediction in the Intensive Care Unit.}},
url = {https://pdfs.semanticscholar.org/d6c6/4ab57ed8341bb92cdaefd71019b866018073.pdf{\%}0Ahttp://www.ncbi.nlm.nih.gov/pubmed/29854167{\%}0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5977709},
volume = {2017},
year = {2017}
}
@article{Salehinejad2017,
abstract = {Recurrent neural networks (RNNs) are capable of learning features and long term dependencies from sequential and time-series data. The RNNs have a stack of non-linear units where at least one connection between units forms a directed cycle. A well-trained RNN can model any dynamical system; however, training RNNs is mostly plagued by issues in learning long-term dependencies. In this paper, we present a survey on RNNs and several new advances for newcomers and professionals in the field. The fundamentals and recent advances are explained and the research challenges are introduced.},
archivePrefix = {arXiv},
arxivId = {1801.01078},
author = {Salehinejad, Hojjat and Sankar, Sharan and Barfett, Joseph and Colak, Errol and Valaee, Shahrokh},
doi = {10.1162/089976600300015015},
eprint = {1801.01078},
file = {:Users/andrea/Documents/Papers/Salehinejad2018.pdf:pdf},
issn = {1930-739X; 1930-7381},
pages = {1--21},
pmid = {11252508},
title = {{Recent Advances in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1801.01078},
year = {2017}
}
@article{Jenatton2010,
abstract = {We propose to combine two approaches formod- eling data admitting sparse representations: on the one hand, dictionary learning has proven ef- fective for various signal processing tasks. On the other hand, recent work on structured spar- sity provides a natural framework for modeling dependencies between dictionary elements. We thus consider a tree-structured sparse regulariza- tion to learn dictionaries embedded in a hierar- chy. The involved proximal operator is com- putable exactly via a primal-dual method, allow- ing the use of accelerated gradient techniques. Experiments showthat for natural image patches, learned dictionary elements organize themselves in such a hierarchical structure, leading to an im- proved performance for restoration tasks. When applied to text documents, our method learns hi- erarchies of topics, thus providing a competitive alternative to probabilistic topic models.},
archivePrefix = {arXiv},
arxivId = {0909.0844},
author = {Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume and Bach, Francis and Fr, Inria},
doi = {10.1.1.173.1120},
eprint = {0909.0844},
isbn = {9781605589077},
journal = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
pages = {487--494},
title = {{Proximal Methods for Sparse Hierarchical Dictionary Learning}},
url = {http://www.icml2010.org/papers/416.pdf},
year = {2010}
}
@article{hripscsak2017,
abstract = {Electronic health record phenotyping is the use of raw electronic health record data to assert characterizations about patients. Researchers have been doing it since the beginning of biomedical informatics, under different names. Phenotyping will benefit from an increasing focus on fidelity, both in the sense of increasing richness, such as measured levels, degree or severity, timing, probability, or conceptual relationships, and in the sense of reducing bias. Research agendas should shift from merely improving binary assignment to studying and improving richer representations. The field is actively researching new temporal directions and abstract representations, including deep learning. The field would benefit from research in nonlinear dynamics, in combining mechanistic models with empirical data, including data assimilation, and in topology. The health care process produces substantial bias, and studying that bias explicitly rather than treating it as merely another source of noise would facilitate addressing it.},
author = {Hripcsak, George and Albers, David J.},
doi = {10.1093/jamia/ocx110},
file = {:Users/andrea/Documents/Papers/hripscsak2017.pdf:pdf},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
number = {3},
pages = {289--294},
title = {{High-fidelity phenotyping: Richness and freedom from bias}},
volume = {25},
year = {2018}
}
@article{Ustun2016a,
abstract = {Scoring systems are linear classification models that only require users to add, subtract and multiply a few small numbers in order to make a prediction. These models are in widespread use by the medical community, but are difficult to learn from data because they need to be accurate and sparse, have coprime integer coefficients, and satisfy multiple operational constraints. We present a new method for creating data-driven scoring systems called a Supersparse Linear Integer Model (SLIM). SLIM scoring systems are built by solving an integer program that directly encodes measures of accuracy (the 0-1 loss) and sparsity (the {\$}\backslashell{\_}0{\$}-seminorm) while restricting coefficients to coprime integers. SLIM can seamlessly incorporate a wide range of operational constraints related to accuracy and sparsity, and can produce highly tailored models without parameter tuning. We provide bounds on the testing and training accuracy of SLIM scoring systems, and present a new data reduction technique that can improve scalability by eliminating a portion of the training data beforehand. Our paper includes results from a collaboration with the Massachusetts General Hospital Sleep Laboratory, where SLIM was used to create a highly tailored scoring system for sleep apnea screening},
archivePrefix = {arXiv},
arxivId = {1502.04269},
author = {Ustun, Berk and Rudin, Cynthia},
doi = {10.1007/s10994-015-5528-6},
eprint = {1502.04269},
file = {:Users/andrea/Documents/Papers/Ustun2016.pdf:pdf},
isbn = {9781577356288},
issn = {15730565},
journal = {Machine Learning},
keywords = {0–1 Loss,Discrete linear classification,Integer programming,Interpretability,Medical scoring systems,Sleep apnea,Sparsity,Supervised Classification},
number = {3},
pages = {349--391},
title = {{Supersparse linear integer models for optimized medical scoring systems}},
volume = {102},
year = {2016}
}
@article{Ghassemi2016,
abstract = {The ability to determine patient acuity (or severity of illness) has immediate practical use for clinicians. We evaluate the use of multivariate timeseries modeling with the multi-task Gaussian process (GP) models using noisy, incomplete, sparse, heterogeneous and unevenly- sampled clinical data, including both physiological sig- nals and clinical notes. The learned multi-task GP (MTGP) hyperparameters are then used to assess and forecast patient acuity. Experiments were conducted with two real clinical data sets acquired from ICU pa- tients: firstly, estimating cerebrovascular pressure reac- tivity, an important indicator of secondary damage for traumatic brain injury patients, by learning the inter- actions between intracranial pressure and mean arterial blood pressure signals, and secondly, mortality predic- tion using clinical progress notes. In both cases, MTGPs provided improved results: an MTGP model provided better results than single-task GP models for signal in- terpolation and forecasting (0.91 vs 0.69 RMSE), and the use of MTGP hyperparameters obtained improved results when used as additional classification features (0.812 vs 0.788 AUC). 1},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Ghassemi, Marzyeh and Pimentel, Marco and Naumann, Tristan and Brennan, Thomas and Clifton, David and Szolovitz, Peter and Feng, Mengling},
doi = {10.1007/s12671-013-0269-8.Moving},
eprint = {15334406},
file = {:Users/andrea/Documents/Papers/Ghassemi2015.pdf:pdf},
isbn = {9781577356998},
issn = {2159-5399 (Print)},
journal = {Proc Conf AAAI Artif Intell.},
keywords = {ghassemi2015},
pages = {446--453},
pmid = {27182460},
title = {{A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in ICU with Sparse, Heterogeneous Clinical Data}},
volume = {January},
year = {2016}
}
@article{Xu2018,
abstract = {With the improvement of medical data capturing, vast amount of continuous patient monitoring data, e.g., electrocardiogram (ECG), real-time vital signs and medications, become available for clinical decision support at intensive care units (ICUs). However, it becomes increasingly challenging to model such data, due to high density of the monitoring data, heterogeneous data types and the requirement for interpretable models. Integration of these high-density monitoring data with the discrete clinical events (including diagnosis, medications, labs) is challenging but potentially rewarding since richness and granularity in such multimodal data increase the possibilities for accurate detection of complex problems and predicting outcomes (e.g., length of stay and mortality). We propose Recurrent Attentive and Intensive Model (RAIM) for jointly analyzing continuous monitoring data and discrete clinical events. RAIM introduces an efficient attention mechanism for continuous monitoring data (e.g., ECG), which is guided by discrete clinical events (e.g, medication usage). We apply RAIM in predicting physiological decompensation and length of stay in those critically ill patients at ICU. With evaluations on MIMIC- III Waveform Database Matched Subset, we obtain an AUC-ROC score of 90.18{\%} for predicting decompensation and an accuracy of 86.82{\%} for forecasting length of stay with our final model, which outperforms our six baseline models.},
archivePrefix = {arXiv},
arxivId = {1807.08820},
author = {Xu, Yanbo and Biswal, Siddharth and Deshpande, Shriprasad R and Maher, Kevin O and Sun, Jimeng},
doi = {10.1145/3219819.3220051},
eprint = {1807.08820},
file = {:Users/andrea/Documents/Papers/Xu2018.pdf:pdf},
isbn = {9781450355520},
title = {{RAIM: Recurrent Attentive and Intensive Model of Multimodal Patient Monitoring Data}},
url = {http://arxiv.org/abs/1807.08820},
year = {2018}
}
@article{Suresh2019,
abstract = {As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of how and why unwanted consequences arise. For instance, downstream harms to particular groups are often blamed on "biased data," but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of downstream harm in machine learning into five distinct categories spanning the data generation and machine learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an understanding of application-specific populations and data generation processes, rather than relying on general claims about what may or may not be "fair."},
archivePrefix = {arXiv},
arxivId = {1901.10002},
author = {Suresh, Harini and Guttag, John V.},
eprint = {1901.10002},
file = {:Users/andrea/Documents/Papers/suresh2019.pdf:pdf},
title = {{A Framework for Understanding Unintended Consequences of Machine Learning}},
url = {http://arxiv.org/abs/1901.10002},
year = {2019}
}
@inproceedings{Bontempi2013,
abstract = {The increasing availability of large amounts of historical data and the need of performing accurate forecasting of future behavior in several scientific and applied domains demands the definition of robust and efficient techniques able to infer from observations the stochastic dependency between past and future. The forecasting domain has been influenced, from the 1960s on, by linear statistical methods such as ARIMA models. More recently, machine learning models have drawn attention and have established themselves as serious contenders to classical statistical models in the forecasting community. This chapter presents an overview of machine learning techniques in time series forecasting by focusing on three aspects: the formalization of one-step forecasting problems as supervised learning tasks, the discussion of local learning techniques as an effective tool for dealing with temporal data and the role of the forecasting strategy when we move from one-step to multiple-step forecasting. {\textcopyright} 2013 Springer-Verlag.},
annote = {this could be interesting...ML + TS},
author = {Bontempi, Gianluca and {Ben Taieb}, Souhaib and {Le Borgne}, Yann A{\"{e}}l},
booktitle = {Lecture Notes in Business Information Processing},
doi = {10.1007/978-3-642-36318-4_3},
isbn = {9783642363177},
issn = {18651348},
keywords = {MIMO,Time series forecasting,lazy learning,local learning,machine learning},
pages = {62--77},
title = {{Machine learning strategies for time series forecasting}},
volume = {138 LNBIP},
year = {2013}
}
@article{Dunlavy2010,
abstract = {The data in many disciplines such as social networks, web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this paper, we consider the problem of temporal link prediction: Given link data for times 1 through T, can we predict the links at time T+1? If our data has underlying periodic structure, can we predict out even further in time, i.e., links at time T+2, T+3, etc.? In this paper, we consider bipartite graphs that evolve over time and consider matrix- and tensor-based methods for predicting future links. We present a weight-based method for collapsing multi-year data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix- and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem. Additionally, we show that tensor-based techniques are particularly effective for temporal data with varying periodic patterns.},
archivePrefix = {arXiv},
arxivId = {1005.4006},
author = {Dunlavy, Daniel M. and Kolda, Tamara G. and Acar, Evrim},
doi = {10.1145/1921632.1921636},
eprint = {1005.4006},
file = {:Users/andrea/Documents/Papers/dunlavy2011.pdf:pdf},
isbn = {1556-4681},
issn = {15564681},
number = {2},
pages = {1--27},
pmid = {14340949184692942955},
title = {{Temporal Link Prediction using Matrix and Tensor Factorizations}},
url = {http://arxiv.org/abs/1005.4006{\%}0Ahttp://dx.doi.org/10.1145/1921632.1921636},
volume = {5},
year = {2010}
}
@article{Liu2014,
abstract = {Much recent scholarship has emphasized institutional differences in corporate governance, capital markets, and law among European, American, and Japanese companies.' Despite very real differences in the corporate systems, the deeper tendency is toward convergence, as it has been since the nineteenth century. The basic law of corporate governance-indeed, most of corporate law-has achieved a high degree of uniformity across developed market jurisdictions, and continu- ing convergence toward a single, standard model is likely. The core legal features of the corporate form were already well established in advanced jurisdictions one hundred years ago, at the turn of the twentieth century. Although there remained considerable room for variation in governance prac- tices and in the fine structure of corporate law throughout the twentieth century, the pressures for further convergence are now rapidly growing. Chief among these pressures is the recent dominance of a shareholder-centered ideology of corporate law among the business, government, and legal elites in key commer- cial jurisdictions. There is no longer any serious competitor to the view that corporate law should principally strive to increase long-term shareholder value. This emergent consensus has already profoundly affected corporate governance practices throughout the world. It is only a matter of time before its influence is felt in the reform of corporate law as well.},
author = {Liu, Chenghao and Hoi, Steven C H and Zhao, Peilin and Sun, Jianling},
file = {:Users/andrea/Documents/Papers/Liu2016.pdf:pdf},
isbn = {9781577357605},
journal = {Aaai},
keywords = {Technical Papers: Machine Learning Methods},
pages = {1867--1873},
title = {{Online ARIMA Algorithms for Time Series Prediction}},
year = {2014}
}
@article{beaulieujones2016,
author = {Beaulieu-jones, Brett K and Greene, Casey S and Als, Resource Open-access and Trials, Clinical},
doi = {10.1016/j.jbi.2016.10.007},
file = {:Users/andrea/Documents/Papers/beaulier-jones2016.pdf:pdf},
issn = {1532-0464},
journal = {Journal of Biomedical Informatics},
keywords = {denoising autoencoder,electronic health record},
pages = {168--178},
publisher = {The Author(s)},
title = {{Semi-supervised learning of the electronic health record for phenotype stratification}},
url = {http://dx.doi.org/10.1016/j.jbi.2016.10.007},
volume = {64},
year = {2016}
}
@article{Haque2017,
abstract = {One in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection. If we can intelligently track healthcare staff, patients, and visitors, we can better understand the sources of such infections. We envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending. In this paper, we propose a non-intrusive vision-based system for tracking people's activity in hospitals. We evaluate our method for the problem of measuring hand hygiene compliance. Empirically, our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies. We present intuitive, qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method's interpretability. This work is a step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections.},
archivePrefix = {arXiv},
arxivId = {1708.00163},
author = {Haque, Albert and Guo, Michelle and Alahi, Alexandre and Yeung, Serena and Luo, Zelun and Rege, Alisha and Jopling, Jeffrey and Downing, Lance and Beninati, William and Singh, Amit and Platchek, Terry and Milstein, Arnold and Fei-Fei, Li},
eprint = {1708.00163},
file = {:Users/andrea/Documents/Papers/Haque2018.pdf:pdf},
issn = {1938-7228},
pages = {1--13},
title = {{Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance}},
url = {http://arxiv.org/abs/1708.00163},
year = {2017}
}
@article{Candes2018,
abstract = {Many contemporary large-scale applications involve building interpretable models linking a large set of potential covariates to a response in a nonlinear fashion, such as when the response is binary. Although this modeling problem has been extensively studied, it remains unclear how to effectively control the fraction of false discoveries even in high-dimensional logistic regression, not to mention general high-dimensional nonlinear models. To address such a practical problem, we propose a new framework of {\$}model{\$}-{\$}X{\$} knockoffs, which reads from a different perspective the knockoff procedure (Barber and Cand$\backslash$`es, 2015) originally designed for controlling the false discovery rate in linear models. Whereas the knockoffs procedure is constrained to homoscedastic linear models with {\$}n\backslashge p{\$}, the key innovation here is that model-X knockoffs provide valid inference from finite samples in settings in which the conditional distribution of the response is arbitrary and completely unknown. Furthermore, this holds no matter the number of covariates. Correct inference in such a broad setting is achieved by constructing knockoff variables probabilistically instead of geometrically. To do this, our approach requires the covariates be random (independent and identically distributed rows) with a distribution that is known, although we provide preliminary experimental evidence that our procedure is robust to unknown/estimated distributions. To our knowledge, no other procedure solves the {\$}controlled{\$} variable selection problem in such generality, but in the restricted settings where competitors exist, we demonstrate the superior power of knockoffs through simulations. Finally, we apply our procedure to data from a case-control study of Crohn's disease in the United Kingdom, making twice as many discoveries as the original analysis of the same data.},
archivePrefix = {arXiv},
arxivId = {1610.02351},
author = {Cand{\`{e}}s, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
doi = {10.1111/rssb.12265},
eprint = {1610.02351},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {False discovery rate,Generalized linear models,Genomewide association study,Knockoff filter,Logistic regression,Markov blanket,Testing for conditional independence in non-linear},
number = {3},
pages = {551--577},
title = {{Panning for gold: ‘model-X' knockoffs for high dimensional controlled variable selection}},
volume = {80},
year = {2018}
}
@article{perros2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.04219v1},
author = {Perros, Ioakeim and Papalexakis, Evangelos E and Wang, Fei and Vuduc, Richard and Searles, Elizabeth and Thompson, Michael and Sun, Jimeng},
doi = {10.475/123},
eprint = {arXiv:1703.04219v1},
file = {:Users/andrea/Documents/Papers/perros2017.pdf:pdf},
isbn = {1234567245},
keywords = {parafac2,phenotyping,sparse tensor factorization,unsuper-},
title = {{SPARTan : Scalable PARAFAC2 for Large {\&} Sparse Data}}
}
@article{Lin2003,
abstract = {The parallel explosions of interest in streaming data, and data mining of time series have had surprisingly little intersection. This is in spite of the fact that time series data are typically streaming data. The main reason for this apparent paradox is the fact that the vast majority of work on streaming data explicitly assumes that the data is discrete, whereas the vast majority of time series data is real valued.Many researchers have also considered transforming real valued time series into symbolic representations, nothing that such representations would potentially allow researchers to avail of the wealth of data structures and algorithms from the text processing and bioinformatics communities, in addition to allowing formerly "batch-only" problems to be tackled by the streaming community. While many symbolic representations of time series have been introduced over the past decades, they all suffer from three fatal flaws. Firstly, the dimensionality of the symbolic representation is the same as the original data, and virtually all data mining algorithms scale poorly with dimensionality. Secondly, although distance measures can be defined on the symbolic approaches, these distance measures have little correlation with distance measures defined on the original time series. Finally, most of these symbolic approaches require one to have access to all the data, before creating the symbolic representation. This last feature explicitly thwarts efforts to use the representations with streaming algorithms.In this work we introduce a new symbolic representation of time series. Our representation is unique in that it allows dimensionality/numerosity reduction, and it also allows distance measures to be defined on the symbolic approach that lower bound corresponding distance measures defined on the original series. As we shall demonstrate, this latter feature is particularly exciting because it allows one to run certain data mining algorithms on the efficiently manipulated symbolic representation, while producing identical results to the algorithms that operate on the original data. Finally, our representation allows the real valued data to be converted in a streaming fashion, with only an infinitesimal time and space overhead.We will demonstrate the utility of our representation on the classic data mining tasks of clustering, classification, query by content and anomaly detection.},
author = {Lin, Jessica and Keogh, Eamonn and Lonardi, Stefano and Chiu, Bill},
doi = {10.1145/882082.882086},
file = {:Users/andrea/Documents/Papers/lin2003.pdf:pdf},
keywords = {data mining,data streams,discretize,symbolic,time series},
pages = {2},
title = {{A symbolic representation of time series, with implications for streaming algorithms}},
year = {2003}
}
@article{Lyndon2018,
abstract = {Objective: Machine learning in healthcare, and innovative healthcare technology in general, require complex interactions within multidisciplinary teams. Healthcare hackathons are being increasingly used as a model for cross-disciplinary collaboration and learning. The aim of this study is to explore high school student learning experiences during a healthcare hackathon. By optimizing their learning experiences, we hope to prepare a future workforce that can bridge technical and health fields and work seamlessly across disciplines. Methods: A qualitative exploratory study utilizing focus group interviews was conducted. Eight high school students from the hackathon were invited to participate in this study through convenience sampling Participating students (n = 8) were allocated into three focus groups. Semi structured interviews were completed, and transcripts evaluated using inductive thematic analysis. Findings: Through the structured analysis of focus group transcripts three major themes emerged from the data: (1) Collaboration, (2) Transferable knowledge and skills, and (3) Expectations about hackathons. These themes highlight strengths and potential barriers when bringing this multidisciplinary approach to high school students and the healthcare community. Conclusion: This study found that students were empowered by the interdisciplinary experience during a hackathon and felt that the knowledge and skills gained could be applied in real world settings. However, addressing student expectations of hackathons prior to the event is an area for improvement. These findings have implications for future hackathons and can spur further research into using the hackathon model as an educational experience for learners of all ages.},
author = {Lyndon, Mataroria P. and Cassidy, Michael P. and Celi, Leo Anthony and Hendrik, Luk and Kim, Yoon Jeon and Gomez, Nicholas and Baum, Nathaniel and Bulgarelli, Lucas and Paik, Kenneth E. and Dagan, Alon},
doi = {10.1016/j.ijmedinf.2017.12.020},
file = {:Users/andrea/Documents/Papers/lyndon2018.pdf:pdf},
issn = {18728243},
journal = {International Journal of Medical Informatics},
keywords = {Hackathons healthcare interprofessional education},
number = {October 2017},
pages = {1--5},
publisher = {Elsevier},
title = {{Hacking Hackathons: Preparing the next generation for the multidisciplinary world of healthcare technology}},
url = {https://doi.org/10.1016/j.ijmedinf.2017.12.020},
volume = {112},
year = {2018}
}
@article{Choi2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.05942v11},
author = {Choi, Edward and Bahadori, Mohammad Taha and Shuetz, Andy and Steward, Walter F and Sun, Jimeng},
eprint = {arXiv:1511.05942v11},
keywords = {MIMIC},
mendeley-tags = {MIMIC},
title = {{Doctor AI : Predicting Clinical Events}},
volume = {56},
year = {2016}
}
@article{Shen2019,
abstract = {Efficient representations of drugs provide important support for healthcare analytics, such as drug–drug interaction (DDI) prediction and drug–drug similarity (DDS) computation. However, incomplete annotated data and drug feature sparseness create substantial barriers for drug representation learning, making it difficult to accurately identify new drug properties prior to public release. To alleviate these deficiencies, we propose KMR, a knowledge-oriented feature-driven method which can learn drug related knowledge with an accurate representation. We conduct series of experiments on real-world medical datasets to demonstrate that KMR is capable of drug representation learning. KMR can support to discover meaningful DDI with an accuracy rate of 92.19{\%}, demonstrating that techniques developed in KMR significantly improve the prediction quality for new drugs not seen at training. Experimental results also indicate that KMR can identify DDS with an accuracy rate of 88.7{\%} by facilitating drug knowledge, outperforming existing state-of-the-art drug similarity measures.},
author = {Shen, Ying and Yuan, Kaiqi and Yang, Min and Tang, Buzhou and Li, Yaliang and Du, Nan and Lei, Kai},
doi = {10.1186/s13321-019-0342-y},
file = {:Users/andrea/Documents/Papers/shen2019.pdf:pdf},
issn = {1758-2946},
journal = {Journal of Cheminformatics},
keywords = {Drug embeddings,Knowledge representation,Drug–drug,drug,drug embeddings,drug interaction,drug similarity,feature,knowledge representation},
number = {1},
pages = {1--16},
publisher = {Springer International Publishing},
title = {{KMR: knowledge-oriented medicine representation learning for drug–drug interaction and similarity computation}},
url = {https://doi.org/10.1186/s13321-019-0342-y},
volume = {11},
year = {2019}
}
@article{choi2016b,
abstract = {Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation.},
archivePrefix = {arXiv},
arxivId = {1602.05568},
author = {Choi, Edward and Bahadori, Mohammad Taha and Searles, Elizabeth and Coffey, Catherine and Thompson, Michael and Bost, James and Tejedor-sojo, Javier and Sun, Jimeng},
doi = {10.1248/cpb.58.1555},
eprint = {1602.05568},
file = {:Users/andrea/Documents/Papers/choi2016b.pdf:pdf},
isbn = {9781450342322},
issn = {0009-2363},
journal = {Chemical {\&} Pharmaceutical Bulletin},
keywords = {alytics,healthcare an-,medical concepts,neural networks,representation learning},
number = {12},
pages = {1555--1564},
pmid = {21139254},
title = {{Multi-layer Representation Learning for Medical Concepts}},
url = {http://joi.jlc.jst.go.jp/JST.JSTAGE/cpb/58.1555?from=CrossRef},
volume = {58},
year = {2010}
}
@article{Chandra2012,
abstract = {Cooperative coevolution decomposes a problem into subcomponents and employs evolutionary algorithms for solving them. Cooperative coevolution has been effective for evolving neural networks. Different problem decomposition methods in cooperative coevolution determine how a neural network is decomposed and encoded which affects its performance. A good problem decomposition method should provide enough diversity and also group interacting variables which are the synapses in the neural network. Neural networks have shown promising results in chaotic time series prediction. This work employs two problem decomposition methods for training Elman recurrent neural networks on chaotic time series problems. The Mackey-Glass, Lorenz and Sunspot time series are used to demonstrate the performance of the cooperative neuro-evolutionary methods. The results show improvement in performance in terms of accuracy when compared to some of the methods from literature. {\textcopyright} 2012 Elsevier B.V.},
author = {Chandra, Rohitash and Zhang, Mengjie},
doi = {10.1016/j.neucom.2012.01.014},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Chaotic time series prediction,Cooperative coevolution,Evolutionary algorithms,Neuro-evolution,Recurrent neural networks},
pages = {116--123},
title = {{Cooperative coevolution of Elman recurrent neural networks for chaotic time series prediction}},
volume = {86},
year = {2012}
}
@article{Suresh2017,
abstract = {Real-time prediction of clinical interventions remains a challenge within intensive care units (ICUs). This task is complicated by data sources that are noisy, sparse, heterogeneous and outcomes that are imbalanced. In this paper, we integrate data from all available ICU sources (vitals, labs, notes, demographics) and focus on learning rich representations of this data to predict onset and weaning of multiple invasive interventions. In particular, we compare both long short-term memory networks (LSTM) and convolutional neural networks (CNN) for prediction of five intervention tasks: invasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. Our predictions are done in a forward-facing manner to enable "real-time" performance, and predictions are made with a six hour gap time to support clinically actionable planning. We achieve state-of-the-art results on our predictive tasks using deep architectures. We explore the use of feature occlusion to interpret LSTM models, and compare this to the interpretability gained from examining inputs that maximally activate CNN outputs. We show that our models are able to significantly outperform baselines in intervention prediction, and provide insight into model learning, which is crucial for the adoption of such models in practice.},
archivePrefix = {arXiv},
arxivId = {1705.08498},
author = {Suresh, Harini and Hunt, Nathan and Johnson, Alistair and Celi, Leo Anthony and Szolovits, Peter and Ghassemi, Marzyeh},
eprint = {1705.08498},
file = {:Users/andrea/Documents/Papers/Suresh2017.pdf:pdf},
keywords = {MIMIC},
mendeley-tags = {MIMIC},
pages = {1--16},
pmid = {175432},
title = {{Clinical Intervention Prediction and Understanding using Deep Networks}},
url = {http://arxiv.org/abs/1705.08498},
year = {2017}
}
@article{George2008,
abstract = {We propose a Bayesian stochastic search approach to selecting restrictions for vector autoregressive (VAR) models. For this purpose, we develop a Markov chain Monte Carlo (MCMC) algorithm that visits high posterior probability restrictions on the elements of both the VAR regression coefficients and the error variance matrix. Numerical simulations show that stochastic search based on this algorithm can be effective at both selecting a satisfactory model and improving forecasting performance. To illustrate the potential of our approach, we apply our stochastic search to VAR modeling of inflation transmission from producer price index (PPI) components to the consumer price index (CPI). {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
annote = {stochastic search variable selection},
author = {George, Edward I. and Sun, Dongchu and Ni, Shawn},
doi = {10.1016/j.jeconom.2007.08.017},
isbn = {03044076},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Bayesian VAR,Markov chain Monte Carlo,Stochastic search},
number = {1},
pages = {553--580},
title = {{Bayesian stochastic search for VAR model restrictions}},
volume = {142},
year = {2008}
}
@article{suresh2017b,
abstract = {We use autoencoders to create low-dimensional embeddings of underlying patient phenotypes that we hypothesize are a governing factor in determining how different patients will react to different interventions. We compare the performance of autoencoders that take fixed length sequences of concatenated timesteps as input with a recurrent sequence-to-sequence autoencoder. We evaluate our methods on around 35,500 patients from the latest MIMIC III dataset from Beth Israel Deaconess Hospital.},
archivePrefix = {arXiv},
arxivId = {1703.07004},
author = {Suresh, Harini and Szolovits, Peter and Ghassemi, Marzyeh},
doi = {10.1093/jhered/esp021},
eprint = {1703.07004},
file = {:Users/andrea/Documents/Papers/suresh2017b.pdf:pdf},
isbn = {9781845933821},
issn = {1520-8532},
number = {Nips},
pmid = {10850481},
title = {{The Use of Autoencoders for Discovering Patient Phenotypes}},
url = {http://arxiv.org/abs/1703.07004},
year = {2017}
}
@article{mcdermott2018,
abstract = {The biomedical field offers many learning tasks that share unique challenges: large amounts of unpaired data, and a high cost to generate labels. In this work, we develop a method to address these issues with semi-supervised learning in regression tasks (e.g., translation from source to target). Our model uses adversarial signals to learn from unpaired datapoints, and imposes a cycle-loss reconstruction error penalty to regularize mappings in either direction against one another. We first evaluate our method on synthetic experiments, demonstrating two primary advantages of the system: 1) distribution matching via the adversarial loss and 2) regularization towards invertible mappings via the cycle loss. We then show a regularization effect and improved performance when paired data is supplemented by additional unpaired data on two real biomedical regression tasks: estimating the physiological effect of medical treatments, and extrapolating gene expression (transcriptomics) signals. Our proposed technique is a promising initial step towards more robust use of adversarial signals in semi-supervised regression, and could be useful for other tasks (e.g., causal inference or modality translation) in the biomedical field. Copyright {\textcopyright} 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
author = {McDermott, M B A and Yan, T and Naumann, T and Hunt, N and Suresh, H and Szolovits, P and Ghassemi, M},
file = {:Users/andrea/Documents/Papers/mcdermott2018.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
pages = {2363--2370},
title = {{Semi-supervised biomedical translation with cycle Wasserstein regression GaNs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060492931{\&}partnerID=40{\&}md5=e23e0a3d37a7ac5fb92fe746799456f1},
year = {2018}
}
@article{Nardi2011,
abstract = {The Lasso is a popular model selection and estimation procedure for linear models that enjoys nice theoretical properties. In this paper, we study the Lasso estimator for fitting autoregressive time series models. We adopt a double asymptotic framework where the maximal lag may increase with the sample size. We derive theoretical results establishing various types of consistency. In particular, we derive conditions under which the Lasso estimator for the autoregressive coefficients is model selection consistent, estimation consistent and prediction consistent. Simulation study results are reported. {\textcopyright} 2010 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {0805.1179},
author = {Nardi, Y. and Rinaldo, A.},
doi = {10.1016/j.jmva.2010.10.012},
eprint = {0805.1179},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Autoregressive model,Estimation consistency,Lasso procedure,Model selection,Prediction consistency},
number = {3},
pages = {528--549},
title = {{Autoregressive process modeling via the Lasso procedure}},
volume = {102},
year = {2011}
}
@article{Che2018,
abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.},
archivePrefix = {arXiv},
arxivId = {1606.01865},
author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
doi = {10.1038/s41598-018-24271-9},
eprint = {1606.01865},
isbn = {4159801824271},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--14},
pmid = {29666385},
title = {{Recurrent Neural Networks for Multivariate Time Series with Missing Values}},
volume = {8},
year = {2018}
}
@article{Lee2015,
abstract = {Objective—Although the consequences of chronic fluid retention are well known, those of iatrogenic fluid retention that occurs during critical illness have not been fully determined. Therefore we investigated the association between fluid balance and survival in a cohort of almost 16,000 individuals who survived an intensive care unit (ICU) stay in a large, urban, tertiary medical centre. Design—Longitudinal analysis of fluid balance at ICU discharge and 90-day post-ICU survival. Measurements—Associations between fluid balance during the ICU stay, determined from the electronic bedside record, and survival were tested using Cox proportional hazard models adjusted for severity of critical illness. Results—There were 1827 deaths in the first 90 days after ICU discharge. Compared to the lowest quartile of discharge fluid balance [median (interquartile range) −1.5 (−3.1, −0.7) L], the highest quartile [7.6 (5.7, 10.8) L] was associated with a 35{\%} [95{\%} confidence interval (CI) 1.13– 1.61)] higher adjusted risk of death. Fluid balance was not associated with outcome amongst individuals without congestive heart failure or renal dysfunction. Amongst patients with either comorbidity, however, fluid balance was strongly associated with outcome, with the highest quartile having a 55{\%} (95{\%} CI 1.24–1.95) higher adjusted risk of death than the lowest quartile. Isotonic fluid balance, defined as the difference between intravenous isotonic fluid administration and urine output, was similarly associated with 90-day outcomes. Conclusion—Positive fluid balance at the time of ICU discharge is associated with increased risk of death, after adjusting for markers of illness severity and chronic medical conditions, particularly in patients with underlying heart or kidney disease. Restoration of euvolaemia prior to discharge may improve survival after acute illness.},
author = {Lee, Joon and {De Louw}, Emma and Niemi, Matthew and Nelson, Rachel and Mark, Roger G and Celi, Leo Anthony and Mukamal, Kenneth J and Danziger, John},
doi = {10.1111/joim.12274},
file = {:Users/andrea/Documents/Papers/lee2015.pdf:pdf},
journal = {J Intern Med},
keywords = {diuresis,fluid balance,heart failure,oedema,survival,weight},
number = {4},
pages = {468--477},
title = {{Association between fluid balance and survival in critically ill patients HHS Public Access}},
volume = {277},
year = {2015}
}
@article{Pollard2018,
abstract = {The eICU Collaborative Research Database, a freely available multi-center database for critical care research},
author = {Pollard, Tom J. and Johnson, Alistair E.W. and Raffa, Jesse D. and Celi, Leo A. and Mark, Roger G. and Badawi, Omar},
doi = {10.1038/sdata.2018.178},
issn = {20524463},
journal = {Scientific Data},
pages = {1--13},
publisher = {The Author(s)},
title = {{The eICU collaborative research database, a freely available multi-center database for critical care research}},
url = {http://dx.doi.org/10.1038/sdata.2018.178},
volume = {5},
year = {2018}
}
@article{banda2018,
author = {Banda, Juan M and Seneviratne, Martin and Hernandez-boussard, Tina and Shah, Nigam H},
file = {:Users/andrea/Documents/Papers/banda2018.pdf:pdf},
keywords = {cohort building,electronic health records,electronic phenotyping},
title = {{Advances in Electronic Phenotyping : From Rule-Based Definitions to Machine Learning Models}},
year = {2018}
}
@article{schulam2017,
abstract = {Decision-makers are faced with the challenge of estimating what is likely to happen when they take an action. For instance, if I choose not to treat this patient, are they likely to die? Practitioners commonly use supervised learning algorithms to fit predictive models that help decision-makers reason about likely future outcomes, but we show that this approach is unreliable, and sometimes even dangerous. The key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data, which causes the model to capture relationships that do not generalize. We propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning. To support decision-making in temporal settings, we introduce the Counterfactual Gaussian Process (CGP) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions. We demonstrate the benefits of the CGP on two important decision-support tasks: risk prediction and "what if?" reasoning for individualized treatment planning.},
archivePrefix = {arXiv},
arxivId = {1703.10651},
author = {Schulam, Peter and Saria, Suchi},
doi = {10.1007/978-3-7091-0956-4-34},
eprint = {1703.10651},
file = {:Users/andrea/Documents/Papers/schulam2017.pdf:pdf},
isbn = {9783709109557},
issn = {10495258},
pmid = {22327688},
title = {{What-if reasoning with Counterfactual Gaussian Processes}},
url = {http://arxiv.org/abs/1703.10651},
year = {2017}
}
@misc{Faraway1998,
abstract = {This case-study fits a variety of neural network (NN) models to the well-known airline data and compares the resulting forecasts with those obtained from the Box-Jenkins and Holt-Winters methods. Many potential problems in fitting NN models were revealed such as the possibility that the fitting routine may not converge or may converge to a local minimum. Moreover it was found that an NN model which fits well may give poor out-of-sample forecasts. Thus we think it is unwise to apply NN models blindly in 'black box' mode as has sometimes been suggested. rather, the wise analyst needs to use traditional modelling skills to select a good NN model, e.g. to select appropriate lagged variables as the 'inputs'. The Bayesian information criterion is preferred to Akaike's information criterion for comparing different models. Methods of examining the response surface implied by an NN model are examined and compared with the results of alternative non-parametric procedures using generalized additive models and prjection pursuit regression. The latter imposes less structure on the model and is arguably easier to understand.},
author = {Faraway, J and Chatfield, C},
booktitle = {{\ldots} of the Royal Statistical Society: Series {\ldots}},
keywords = {Airline model,Akaike information criterion,Autoregressive integrated moving average model,Bayesian information criterion,Box-Jenkins forecasting,Generalized additive model,Holt-Winters forecasting,Projection pursuit regression},
pages = {231--250},
title = {{Time series forecasting with neural networks: a comparative study using the air line data}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9876.00109/abstract},
year = {1998}
}
@article{Izakian2015,
abstract = {Clustering is a powerful vehicle to reveal and visualize structure of data. When dealing with time series, selecting a suitable measure to evaluate the similarities/dissimilarities within the data becomes necessary and subsequently it exhibits a significant impact on the results of clustering. This selection should be based upon the nature of time series and the application itself. When grouping time series based on their shape information is of interest (shape-based clustering), using a Dynamic Time Warping (DTW) distance is a desirable choice. Using stretching or compressing segments of temporal data, DTW determines an optimal match between any two time series. In this way, time series exhibiting similar patterns occurring at different time periods, are considered as being similar. Although DTW is a suitable choice for comparing data with respect to their shape information, calculating the average of a collection of time series (which is required in clustering methods) based on this distance becomes a challenging problem. As the result, employing clustering techniques like K-Means and Fuzzy C-Means (where the cluster centers - prototypes are calculated through averaging the data) along with the DTW distance is a challenging task and may produce unsatisfactory results. In this study, three alternatives for fuzzy clustering of time series using DTW distance are proposed. In the first method, a DTW-based averaging technique proposed in the literature, has been applied to the Fuzzy C-Means clustering. The second method considers a Fuzzy C-Medoids clustering, while the third alternative comes as a hybrid technique, which exploits the advantages of both the Fuzzy C-Means and Fuzzy C-Medoids when clustering time series. Experimental studies are reported over a set of time series coming from the UCR time series database.},
author = {Izakian, Hesam and Pedrycz, Witold and Jamal, Iqbal},
doi = {10.1016/j.engappai.2014.12.015},
file = {:Users/andrea/Documents/Papers/izakian2015.pdf:pdf},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Clustering time series,Dynamic Time Warping (DTW),Fuzzy clustering,Hybrid approach},
pages = {235--244},
publisher = {Elsevier},
title = {{Fuzzy clustering of time series data using dynamic time warping distance}},
url = {http://dx.doi.org/10.1016/j.engappai.2014.12.015},
volume = {39},
year = {2015}
}
@article{Johnson2017,
author = {Johnson, Alistair E W and Pollard, Tom J and Mark, Roger G},
file = {:Users/andrea/Documents/Papers/Johnson2017.pdf:pdf},
title = {{Reproducibility in critical care : a mortality prediction case study}},
volume = {68},
year = {2017}
}
@article{Channouf2007,
abstract = {We develop and evaluate time-series models of call volume to the emergency medical service of a major Canadian city. Our objective is to offer simple and effective models that could be used for realistic simulation of the system and for forecasting daily and hourly call volumes. Notable features of the analyzed time series are: a positive trend, daily, weekly, and yearly seasonal cycles, special-day effects, and positive autocorrelation. We estimate models of daily volumes via two approaches: (1) autoregressive models of data obtained after eliminating trend, seasonality, and special-day effects; and (2) doubly-seasonal ARIMA models with special-day effects. We compare the estimated models in terms of goodness-of-fit and forecasting accuracy. We also consider two possibilities for the hourly model: (3) a multinomial distribution for the vector of number of calls in each hour conditional on the total volume of calls during the day and (4) fitting a time series to the data at the hourly level. For our data, (1) and (3) are superior.},
author = {Channouf, Nabil and L'Ecuyer, Pierre and Ingolfsson, Armann and Avramidis, Athanassios N.},
doi = {10.1007/s10729-006-9006-3},
isbn = {1386-9620},
issn = {13869620},
journal = {Health Care Management Science},
keywords = {Arrivals,Emergency medical service,Forecasting,Simulation,Time series},
number = {1},
pages = {25--45},
pmid = {17323653},
title = {{The application of forecasting techniques to modeling emergency medical system calls in Calgary, Alberta}},
volume = {10},
year = {2007}
}
@article{hunt2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.02442v1},
author = {Hunt, Xin J and Carolina, North and Emrani, Saba and Carolina, North and Silva, Jorge and Carolina, North and Carolina, North},
eprint = {arXiv:1807.02442v1},
file = {:Users/andrea/Documents/Papers/Hunt2018.pdf:pdf},
isbn = {1234567245},
keywords = {2018,acm reference format,alzheimer,and jorge silva,hunt,ilknur kaynar kabul,lasso,missing data,multi-,multi-task learning,s disease,saba emrani,xin j},
number = {August},
title = {{Multi-Task Learning with Incomplete Data for Healthcare}},
year = {2018}
}
@article{Ghassemi2017,
abstract = {The impact of many intensive care unit interventions has not been fully quantified, especially in heterogeneous patient populations. We train unsupervised switching state autoregressive models on vital signs from the public MIMIC-III database to capture patient movement between physiological states. We compare our learned states to static demographics and raw vital signs in the prediction of five ICU treatments: ventilation, vasopressor administra tion, and three transfusions. We show that our learned states, when combined with demographics and raw vital signs, improve prediction for most interventions even 4 or 8 hours ahead of onset. Our results are competitive with existing work while using a substantially larger and more diverse cohort of 36,050 patients. While custom classifiers can only target a specific clinical event, our model learns physiological states which can help with many interventions. Our robust patient state representations provide a path towards evidence-driven administration of clinical interventions.},
author = {Ghassemi, Marzyeh and Wu, Mike and Hughes, Michael C and Szolovits, Peter and Doshi-Velez, Finale},
file = {:Users/andrea/Documents/Papers/ghassemi2017.pdf:pdf},
issn = {2153-4063},
journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},
pages = {82--91},
pmid = {28815112},
title = {{Predicting intervention onset in the ICU with switching state space models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28815112{\%}0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5543372},
volume = {2017},
year = {2017}
}
@inproceedings{Kock2015,
abstract = {This paper establishes non-asymptotic oracle inequalities for the prediction error and estimation accuracy of the LASSO in stationary vector autoregressive models. These inequalities are used to establish consistency of the LASSO even when the number of parameters is of a much larger order of magnitude than the sample size. We also state conditions under which no relevant variables are excluded. Next, non-asymptotic probabilities are given for the adaptive LASSO to select the correct sparsity pattern. We then provide conditions under which the adaptive LASSO reveals the correct sparsity pattern asymptotically. We establish that the estimates of the non-zero coefficients are asymptotically equivalent to the oracle assisted least squares estimator. This is used to show that the rate of convergence of the estimates of the non-zero coefficients is identical to the one of least squares only including the relevant covariates.},
annote = {non-asymptotic probabilities given for adaLasso to select the correct sparsity pattern

theoretical},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.0811v1},
author = {Kock, Anders Bredahl and Callot, Laurent},
booktitle = {Journal of Econometrics},
doi = {10.1016/j.jeconom.2015.02.013},
eprint = {arXiv:1311.0811v1},
issn = {18726895},
keywords = {Adaptive LASSO,High-dimensional data,LASSO,Oracle inequality,VAR},
number = {2},
pages = {325--344},
title = {{Oracle inequalities for high dimensional vector autoregressions}},
volume = {186},
year = {2015}
}
@article{Csiba2016,
abstract = {Minibatching is a very well studied and highly popular technique in supervised learning, used by practitioners due to its ability to accelerate training through better utilization of parallel processing power and reduction of stochastic variance. Another popular technique is importance sampling -- a strategy for preferential sampling of more important examples also capable of accelerating the training process. However, despite considerable effort by the community in these areas, and due to the inherent technical difficulty of the problem, there is no existing work combining the power of importance sampling with the strength of minibatching. In this paper we propose the first {\{}$\backslash$em importance sampling for minibatches{\}} and give simple and rigorous complexity analysis of its performance. We illustrate on synthetic problems that for training data of certain properties, our sampling can lead to several orders of magnitude improvement in training time. We then test the new sampling on several popular datasets, and show that the improvement can reach an order of magnitude.},
archivePrefix = {arXiv},
arxivId = {1602.02283},
author = {Csiba, Dominik and Richt{\'{a}}rik, Peter},
eprint = {1602.02283},
file = {:Users/andrea/Documents/Papers/csiba2018.pdf:pdf},
keywords = {convex optimization,empirical risk minimization,importance sampling,minibatching,reduced methods,variance-},
pages = {1--21},
title = {{Importance Sampling for Minibatches}},
url = {http://arxiv.org/abs/1602.02283},
volume = {19},
year = {2016}
}
@article{Bernanke2005,
abstract = {Bernanke, Ben S., Jean Boivin and Piotr Eliasz. "Measuring The Effects Of Monetary Policy: A Factor-Augmented Vector Autoregressive (FAVAR) Approach," Quarterly Journal of Economics, 2005, v120(1,Feb), 387-422.},
author = {Bernanke, Ben S and Boivin, Jean and Eliasz, Piotr},
doi = {10.1162/0033553053327452},
isbn = {0033-5533},
issn = {00335533},
journal = {Quarterly Journal of Economics},
number = {February},
pages = {387--422},
pmid = {16473683},
title = {{Measuring the Effects of Monetary Policy: A Factor-Argumented Vector Autoregressive (FAVAR) Aproach}},
volume = {120},
year = {2005}
}
@article{Yadav2017,
abstract = {The continuously increasing cost of the US healthcare system has received significant attention. Central to the ideas aimed at curbing this trend is the use of technology, in the form of the mandate to implement electronic health records (EHRs). EHRs consist of patient information such as demographics, medications, laboratory test results, diagnosis codes and procedures. Mining EHRs could lead to improvement in patient health management as EHRs contain detailed information related to disease prognosis for large patient populations. In this manuscript, we provide a structured and comprehensive overview of data mining techniques for modeling EHR data. We first provide a detailed understanding of the major application areas to which EHR mining has been applied and then discuss the nature of EHR data and its accompanying challenges. Next, we describe major approaches used for EHR mining, the metrics associated with EHRs, and the various study designs. With this foundation, we then provide a systematic and methodological organization of existing data mining techniques used to model EHRs and discuss ideas for future research. We conclude this survey with a comprehensive summary of clinical data mining applications of EHR data, as illustrated in the online supplement.},
archivePrefix = {arXiv},
arxivId = {1702.03222},
author = {Yadav, Pranjul and Steinbach, Michael and Kumar, Vipin and Simon, Gyorgy},
doi = {1539-9087/2016/04-ART1},
eprint = {1702.03222},
issn = {00314005},
number = {1},
pages = {1--41},
title = {{Mining Electronic Health Records: A Survey}},
url = {http://arxiv.org/abs/1702.03222},
volume = {1},
year = {2017}
}
@article{Yang2016,
abstract = {Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.},
archivePrefix = {arXiv},
arxivId = {1605.06391},
author = {Yang, Yongxin and Hospedales, Timothy},
eprint = {1605.06391},
file = {:Users/andrea/Documents/Papers/yang2017.pdf:pdf},
pages = {1--12},
title = {{Deep Multi-task Representation Learning: A Tensor Factorisation Approach}},
url = {http://arxiv.org/abs/1605.06391},
year = {2016}
}
@article{acar2017,
abstract = {Neuroimaging modalities such as functional magnetic resonance imaging (fMRI) and electroencephalography (EEG) provide information about neurological functions in complementary spatiotemporal resolutions; therefore, fusion of these modalities is expected to provide better understanding of brain activity. In this paper, we jointly analyze fMRI and multi-channel EEG signals collected during an auditory oddball task with the goal of capturing brain activity patterns that differ between patients with schizophrenia and healthy controls. Rather than selecting a single electrode or matricizing the third-order tensor that can be naturally used to represent multi-channel EEG signals, we preserve the multi-way structure of EEG data and use a coupled matrix and tensor factorization (CMTF) model to jointly analyze fMRI and EEG signals. Our analysis reveals that (i) joint analysis of EEG and fMRI using a CMTF model can capture meaningful temporal and spatial signatures of patterns that behave differently in patients and controls, and (ii) these differences and the interpretability of the associated components increase by including multiple electrodes from frontal, motor and parietal areas, but not necessarily by including all electrodes in the analysis.},
author = {Acar, Evrim and Levin-Schwartz, Yuri and Calhoun, Vince D. and Adali, Tulay},
doi = {10.1109/ISCAS.2017.8050303},
file = {:Users/andrea/Documents/Papers/acar2017.pdf:pdf},
isbn = {9781467368520},
issn = {02714310},
journal = {Proceedings - IEEE International Symposium on Circuits and Systems},
pages = {1--4},
title = {{Tensor-based fusion of EEG and FMRI to understand neurological changes in schizophrenia}},
year = {2017}
}
@article{Zhong2016,
abstract = {Since about 100 years ago, to learn the intrinsic structure of data, many representation learning approaches have been proposed, including both linear ones and nonlinear ones, supervised ones and unsupervised ones. Particularly, deep architectures are widely applied for representation learning in recent years, and have delivered top results in many tasks, such as image classification, object detection and speech recognition. In this paper, we review the development of data representation learning methods. Specifically, we investigate both traditional feature learning algorithms and state-of-the-art deep learning models. The history of data representation learning is introduced, while available resources (e.g. online course, tutorial and book information) and toolboxes are provided. Finally, we conclude this paper with remarks and some interesting research directions on data representation learning.},
archivePrefix = {arXiv},
arxivId = {1611.08331},
author = {Zhong, Guoqiang and Wang, Li-Na and Dong, Junyu},
doi = {10.1016/j.jfds.2017.05.001},
eprint = {1611.08331},
file = {:Users/andrea/Documents/Papers/zhong2016.pdf:pdf},
issn = {2405-9188},
journal = {The Journal of Finance and Data Science},
keywords = {deep learning,feature learning,representation learning},
number = {4},
pages = {265--278},
publisher = {Elsevier Ltd},
title = {{An Overview on Data Representation Learning: From Traditional Feature Learning to Recent Deep Learning}},
url = {http://arxiv.org/abs/1611.08331},
volume = {2},
year = {2016}
}
@article{Hsu2015a,
abstract = {BACKGROUND: Indwelling arterial catheters (IACs) are used extensively in the ICU for hemo- dynamic monitoring and for blood gas analysis. IAC use also poses potentially serious risks, including bloodstream infections and vascular complications. Th e purpose of this study was to assess whether IAC use was associated with mortality in patients who are mechanically venti- lated and do not require vasopressor support. METHODS: Th is study used the Multiparameter Intelligent Monitoring in Intensive Care II database, consisting of . 24,000 patients admitted to the Beth Israel Deaconess Medical Center ICU between 2001 and 2008. Patients requiring mechanical ventilation who did not require vaso- pressors or have a diagnosis of sepsis were identifi ed, and the primary outcome was 28-day mortality. A model based on patient demographics, comorbidities, vital signs, and laboratory results was developed to estimate the propensity for IAC placement. Patients were then propensity matched, and McNemar test was used to evaluate the association of IAC with 28-day mortality. RESULTS: We identifi ed 1,776 patients who were mechanically ventilated who met inclusion criteria. Th ere were no diff erences in the covariates included in the fi nal propensity model between the IAC and non-IAC propensity-matched groups. For the matched cohort, there was no diff erence in 28-day mortality between the IAC group and the non-IAC group (14.7{\%} vs 15.2{\%}; OR, 0.96; 95{\%} CI, 0.62-1.47). CONCLUSIONS: In hemodynamically stable patients who are mechanically ventilated, the pres- ence of an IAC is not associated with a diff erence in 28-day mortality. Validation in other datasets, as well as further analyses in other subgroups, is warranted.},
author = {Hsu, Douglas J. and Feng, Mengling and Kothari, Rishi and Zhou, Hufeng and Chen, Kenneth P. and Celi, Leo A.},
doi = {10.1378/chest.15-0516},
file = {:Users/andrea/Documents/Papers/Hsu2015appendix.pdf:pdf},
isbn = {2164442911},
issn = {19313543},
journal = {Chest},
keywords = {Hsu2015appendix},
number = {6},
pages = {1470--1476},
pmid = {26270005},
title = {{The association between indwelling arterial catheters and mortality in hemodynamically stable patients with respiratory failure a propensity score analysis (online supplement)}},
volume = {148},
year = {2015}
}
@article{Jenatton2011,
abstract = {We consider the empirical risk minimization problem for linear supervised learning, with regularization by structured sparsity-inducing norms. These are defined as sums of Euclidean norms on certain subsets of variables, extending the usual {\$}\backslashell{\_}1{\$}-norm and the group {\$}\backslashell{\_}1{\$}-norm by allowing the subsets to overlap. This leads to a specific set of allowed nonzero patterns for the solutions of such problems. We first explore the relationship between the groups defining the norm and the resulting nonzero patterns, providing both forward and backward algorithms to go back and forth from groups to patterns. This allows the design of norms adapted to specific prior knowledge expressed in terms of nonzero patterns. We also present an efficient active set algorithm, and analyze the consistency of variable selection for least-squares linear regression in low and high-dimensional settings.},
archivePrefix = {arXiv},
arxivId = {0904.3523},
author = {Jenatton, Rodolphe and Audibert, Jean-Yves and Bach, Francis},
doi = {arXiv:0904.3523},
eprint = {0904.3523},
isbn = {1532-4435},
issn = {1532-4435},
journal = {J. Mach. Learn. Res. (JMLR)},
keywords = {active set algorithm,consistency,convex optimization,sparsity,variable selection},
pages = {2777--2824},
title = {{Structured Variable Selection with Sparsity-Inducing Norms}},
volume = {12},
year = {2011}
}
@article{yin2019,
author = {Yin, Kejing and Qian, Dong and Cheung, William K and Fung, Benjamin C M and Poon, Jonathan},
file = {:Users/andrea/Documents/Papers/yin2019.pdf:pdf},
title = {{Learning Phenotypes and Dynamic Patient Representations via RNN Regularized Collective Non-negative Tensor Factorization}},
year = {2018}
}
@article{ramachandram2017,
abstract = {A survey on recent advances and trends T he success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal represen-tations in deep-learning architectures. We highlight two areas of research—regularization strategies and methods that learn or optimize multimodal fusion structures—as exciting areas for future work.},
author = {Ramachandram, Dhanesh and Taylor, Graham W},
doi = {10.1109/MSP.2017.2738401},
file = {:Users/andrea/Documents/Papers/Ramachandram2017.pdf:pdf},
isbn = {9789462997295},
issn = {1053-5888},
journal = {IEEE SIgnal ProcESSIng MagazInE},
number = {November},
pmid = {8103116},
title = {{Deep Multimodal learning: a survey on recent advances and trends}},
year = {2017}
}
@article{Yin2018,
abstract = {Non-negative tensor factorization has been shown effective for discovering phenotypes from the EHR data with minimal human supervision. In most cases, an interaction tensor of the elements in the EHR (e.g., diagnoses and medications) has to be first established before the factorization can be applied. Such correspondence information however is often missing. While different heuristics can be used to estimate the missing correspondence, any errors introduced will in turn cause inaccuracy for the subsequent phenotype discovery task. This is especially true for patients with multiple diseases diagnosed (e.g., under critical care). To alleviate this limitation, we propose the hidden interaction tensor factorization (HITF) where the diagnosis-medication correspondence and the underlying phenotypes are inferred simultaneously. We formulate it under a Poisson non-negative tensor factorization framework and learn the HITF model via maximum likelihood estimation. For performance evaluation, we applied HITF to the MIMIC III dataset. Our empirical results show that both the phenotypes and the correspondence inferred are clinically meaningful. In addition, the inferred HITF model outperforms a number of state-of-the-art methods for mortality prediction.},
author = {Yin, Kejing and Cheung, William K. and Liu, Yang and Fung, Benjamin C.M. and Poon, Jonathan},
file = {:Users/andrea/Documents/Papers/yin2018.pdf:pdf},
isbn = {9780999241127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning Applications: Bio,Machine Learning: Dimensionality Reduction and Man,Machine Learning: Unsupervised Learning,Medicine},
pages = {3627--3633},
title = {{Joint learning of phenotypes and diagnosis-medication correspondence via hidden interaction tensor factorization}},
volume = {2018-July},
year = {2018}
}
@article{Saeed2006,
author = {Saeed, Mohammed and Mark, Roger},
file = {:Users/andrea/Documents/Papers/Saeed2006.pdf:pdf},
keywords = {MIMIC},
mendeley-tags = {MIMIC},
pages = {679--683},
title = {{A Novel Method for the Efficient Retrieval of Similar Multiparameter Physiologic Time Series Using Wavelet-Based Symbolic Representations}},
year = {2006}
}
@article{Hsu2008,
abstract = {A subset selection method is proposed for vector autoregressive (VAR) processes using the Lasso [Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B 58, 267–288] technique. Simply speaking, Lasso is a shrinkage method in a regression setup which selects the model and estimates the parameters simultaneously. Compared to the conventional information-based methods such as AIC and BIC, the Lasso approach avoids computationally intensive and exhaustive search. On the other hand, compared to the existing subset selection methods with parameter constraints such as the top-down and bottom-up strategies, the Lasso method is computationally efficient and its result is robust to the order of series included in the autoregressive model. We derive the asymptotic theorem for the Lasso estimator under VAR processes. Simulation results demonstrate that the Lasso method performs better than several conventional subset selection methods for small samples in terms of prediction mean squared errors and estimation errors under various settings. The methodology is applied to modeling U.S. macroeconomic data for illustration.},
annote = {Proposed VAR-lasso framework},
author = {Hsu, Nan-Jung and Hung, Hung-Lin and Chang, Ya-Mei},
doi = {10.1016/j.csda.2007.12.004},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
number = {7},
pages = {3645--3657},
title = {{Subset selection for vector autoregressive processes using Lasso}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947307004549},
volume = {52},
year = {2008}
}
@inproceedings{Jacob2009,
abstract = {We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of co-variates defined a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theoretical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1110.0413v1},
author = {Jacob, Laurent and Obozinski, Guillaume and Vert, Jean-Philippe},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553431},
eprint = {arXiv:1110.0413v1},
isbn = {9781605585161},
pages = {1--8},
title = {{Group lasso with overlap and graph lasso}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553431},
year = {2009}
}
@article{Ghassemi2018,
abstract = {Healthcare is a natural arena for the application of machine learning, especially as modern electronic health records (EHRs) provide increasingly large amounts of data to answer clinically meaningful questions. However, clinical data and practice present unique challenges that complicate the use of common methodologies. This article serves as a primer on addressing these challenges and highlights opportunities for members of the machine learning and data science communities to contribute to this growing domain.},
archivePrefix = {arXiv},
arxivId = {1806.00388},
author = {Ghassemi, Marzyeh and Naumann, Tristan and Schulam, Peter and Beam, Andrew L. and Ranganath, Rajesh},
eprint = {1806.00388},
file = {:Users/andrea/Documents/Papers/Ghassemi2018.pdf:pdf},
title = {{Opportunities in Machine Learning for Healthcare}},
url = {http://arxiv.org/abs/1806.00388},
year = {2018}
}
@article{Ocampo2012,
abstract = {This document presents how to estimate and implement a structural VAR-X model under long run and impact identification restrictions. Esti-mation by Bayesian and classical methods are presented. Applications of the structural VAR-X for impulse response functions to structural shocks, multiplier analysis of the exogenous variables, forecast error variance decom-position and historical decomposition of the endogenous variables are also described, as well as a method for computing higher posterior density re-gions in a Bayesian context. Some of the concepts are exemplified with an application to US data. Resumen Este documento cubre la estimaci{\'{o}}n e implementaci{\'{o}}n del modelo VAR-X estructural bajo restricciones de identificaci{\'{o}}n de corto y largo plazo. Se pre-senta la estimaci{\'{o}}n tanto por m{\'{e}}todos cl{\'{a}}sicos como Bayesianos. Tambi{\'{e}}n se describen aplicaciones del modelo como impulsos respuesta ante choques estructurales, an{\'{a}}lisis de multiplicadores de las variables ex{\'{o}}genas, descom-posici{\'{o}}n de varianza del error de pron{\'{o}}stico y descomposici{\'{o}}n hist{\'{o}}rica de las variables end{\'{o}}genas. As{\'{i}} mismo se presenta un m{\'{e}}todo para calcular regiones de alta densidad posterior en el contexto Bayesiano. Algunos de los conceptos son ejemplificados con una aplicaci{\'{o}}n a datos de los Estados Unidos. Palabras clave: econometr{\'{i}}a, modelo estructural, series de tiempo Baye-sianas, vector autoregresivo.},
author = {Ocampo, Sergio and Rodr{\'{i}}guez, Norberto},
journal = {Revista Colombiana de Estad{\'{i}}stica Diciembre},
keywords = {Bayesian time series,Econometrics,Structural model,Vector autoregression},
number = {3},
pages = {479--508},
title = {{An Introductory Review of a Structural VAR-X Estimation and Applications}},
volume = {35},
year = {2012}
}
@article{Suo2016,
abstract = {We consider regression scenarios where it is natural to impose an order constraint on the coefficients. We propose an order-constrained version of L1-regularized regression for this problem, and show how to solve it efficiently using the well-known Pool Adjacent Violators Algorithm as its proximal operator. The main application of this idea is time-lagged regression, where we predict an outcome at time t from features at the previous K time points. In this setting it is natural to assume that the coefficients decay as we move farther away from t, and hence the order constraint is reasonable. Potential applications include financial time series and prediction of dynamic patient out- comes based on clinical measurements. We illustrate this idea on real and simulated data.},
archivePrefix = {arXiv},
arxivId = {1405.6447},
author = {Tibshirani, Robert and Suo, Xiaotong},
doi = {10.1080/00401706.2015.1079245},
eprint = {1405.6447},
file = {:Users/andrea/Library/Application Support/Mendeley Desktop/Downloaded/Tibshirani, Suo - 2016 - An Ordered Lasso and Sparse Time-Lagged Regression.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Feature selection,Monotone coefficients,Penalized regression},
title = {{An Ordered Lasso and Sparse Time-Lagged Regression}},
year = {2016}
}
@article{Imani2018,
abstract = {Perhaps the most basic query made by a data analyst confronting a new data source is "Show me some representative/typical data." Answering this question is trivial in many domains, but surprisingly, it is very difficult in large time series datasets. The major difficulty is not time or space complexity, but defining what it means to be representative data in this domain. In this work, we show that the obvious candidate definitions: motifs, shapelets, cluster centers, random samples etc., are all poor choices. Thus motivated, we introduce time series snippets, a novel representation of typical time series subsequences. Beyond their utility for visualizing and summarizing massive time series collections, we show that time series snippets have utility for high-level comparison of large time series collections.},
author = {Imani, Shima and Madrid, Frank and Ding, Wei and Crouter, Scott and Keogh, Eamonn},
doi = {10.1109/ICBK.2018.00058},
file = {:Users/andrea/Documents/Papers/imani2018.pdf:pdf},
isbn = {9781538691243},
journal = {Proceedings - 9th IEEE International Conference on Big Knowledge, ICBK 2018},
keywords = {Diversification,Motifs,Sampling,Time series},
pages = {382--389},
title = {{Matrix profile XIII: Time series snippets: A new primitive for time series data mining}},
year = {2018}
}
@article{Davis2012,
abstract = {The vector autoregressive (VAR) model has been widely used for modeling temporal de- pendence in a multivariate time series. For large (and even moderate) dimensions, the number of AR coefficients can be prohibitively large, resulting in noisy estimates, unstable predictions and difficult-to-interpret temporal dependence. To overcome such drawbacks, we propose a 2-stage approach for fitting sparse VAR (sVAR) models in which many of the AR coefficients are zero. The first stage selects non-zero AR coefficients based on an estimate of the partial spectral coherence (PSC) together with the use of BIC. The PSC is useful for quantifying the conditional relationship between marginal series in a multivariate process. A refinement second stage is then applied to further reduce the number of parameters. The performance of this 2-stage approach is illustrated with simulation results. The 2-stage approach is also applied to two real data examples: the first is the Google Flu Trends data and the second is a time series of concentration levels of air pollutants.},
annote = {development of VAR+lasso},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.0520v1},
author = {Davis, Richard a. and Zang, Pengfei and Zheng, Tian},
doi = {10.1080/10618600.2015.1092978},
eprint = {arXiv:1207.0520v1},
issn = {1061-8600},
journal = {arXiv preprint arXiv:1207.0520},
keywords = {model,partial spectral coherence,psc,sparsity,var,vector autoregressive},
number = {June},
pages = {1--39},
title = {{Sparse Vector Autoregressive Modeling}},
url = {http://arxiv.org/abs/1207.0520},
volume = {8600},
year = {2012}
}
@article{beaulieu-jones2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.01484v1},
author = {Beaulieu-jones, Brett K and Finlayson, Samuel G and Yuan, William and Wu, Zhiwei Steven},
eprint = {arXiv:1812.01484v1},
file = {:Users/andrea/Documents/Papers/beaulieu-jones2018.pdf:pdf},
title = {{Privacy-Preserving Distributed Deep Learning for Clinical Data}},
year = {2018}
}
@article{Carmack2012,
abstract = {Since its introduction by Stone (1974) and Geisser (1975), cross-validation has been studied and improved by several authors including Burman et al. (1994), Hart {\&} Yi (1998), Racine (2000), Hart {\&} Lee (2005), and Carmack et al. (2009). Perhaps the most widely used and best known is generalized cross-validation (GCV) (Craven {\&} Wahba, 1979), which establishes a single-pass method that penalizes the t by the trace of the smoother matrix assuming independent errors. We propose an extension to GCV in the context of correlated errors, which is motivated by a natural de nition for residual degrees of freedom. The ecacy of the new method is investigated with a simulation ex- periment on a kernel smoother with bandwidth selection in local linear regression. Next, the winning methodology is illustrated by application to spatial modeling of fMRI data using a nonparametric semivariogram. We conclude with remarks about the heteroscedas- tic case and a potential maximum likelihood framework for Gaussian random processes},
annote = {Nothing particular
},
author = {Carmack, Patrick S. and Spence, Jeffrey S. and Schucany, William R.},
doi = {10.1080/10485252.2012.655733},
issn = {10485252},
journal = {Journal of Nonparametric Statistics},
keywords = {effective degrees of freedom,fMRI,model selection,nonparametric,spatial semivariogram,supervised learning,tuning parameter},
number = {2},
pages = {269--282},
title = {{Generalised correlated cross-validation}},
volume = {24},
year = {2012}
}
@article{Molitor2018,
abstract = {In classification problems, especially those that categorize data into a large number of classes, the classes often naturally follow a hierarchical structure. That is, some classes are likely to share similar structures and features. Those characteristics can be captured by considering a hierarchical relationship among the class labels. Here, we extend a recent simple classification approach on binary data in order to efficiently classify hierarchical data. In certain settings, specifically, when some classes are significantly easier to identify than others, we showcase computational and accuracy advantages.},
archivePrefix = {arXiv},
arxivId = {1807.08825},
author = {Molitor, Denali and Needell, Deanna},
eprint = {1807.08825},
file = {:Users/andrea/Documents/Papers/needell2018.pdf:pdf},
keywords = {binary measurements,classification,one-bit representations},
pages = {1--30},
title = {{Hierarchical Classification using Binary Data}},
url = {http://arxiv.org/abs/1807.08825},
volume = {19},
year = {2018}
}
@article{Futoma2017a,
abstract = {Sepsis is a poorly understood and potentially life-threatening complication that can occur as a result of infection. Early detection and treatment improves patient outcomes, and as such it poses an important challenge in medicine. In this work, we develop a flexible classifier that leverages streaming lab results, vitals, and medications to predict sepsis before it occurs. We model patient clinical time series with multi-output Gaussian processes, maintaining uncertainty about the physiological state of a patient while also imputing missing values. The mean function takes into account the effects of medications administered on the trajectories of the physiological variables. Latent function values from the Gaussian process are then fed into a deep recurrent neural network to classify patient encounters as septic or not, and the overall model is trained end-to-end using back-propagation. We train and validate our model on a large dataset of 18 months of heterogeneous inpatient stays from the Duke University Health System, and develop a new "real-time" validation scheme for simulating the performance of our model as it will actually be used. Our proposed method substantially outperforms clinical baselines, and improves on a previous related model for detecting sepsis. Our model's predictions will be displayed in a real-time analytics dashboard to be used by a sepsis rapid response team to help detect and improve treatment of sepsis.},
archivePrefix = {arXiv},
arxivId = {1708.05894},
author = {Futoma, Joseph and Hariharan, Sanjay and Sendak, Mark and Brajer, Nathan and Clement, Meredith and Bedoya, Armando and O'Brien, Cara and Heller, Katherine},
eprint = {1708.05894},
file = {:Users/andrea/Documents/Papers/Futoma2017.pdf:pdf},
isbn = {0005-2086 (Print)$\backslash$r0005-2086 (Linking)},
number = {2006},
pmid = {7271663},
title = {{An Improved Multi-Output Gaussian Process RNN with Real-Time Validation for Early Sepsis Detection}},
url = {http://arxiv.org/abs/1708.05894},
volume = {68},
year = {2017}
}
@article{Lei2019,
abstract = {Electronic Health Records (EHRs) provide possibilities to improve patient care and facilitate clinical research. However, there are many challenges faced by the applications of EHRs, such as temporality, high dimensionality, sparseness, noise, random error, and systematic bias. In particular, temporal patient information is difficult to effectively use by traditional machine learning methods while the sequential information of EHRs is very useful. In this paper, we propose a general-purpose patient representation learning approach to summarize sequential EHRs. Specifically, a recurrent neural network based denoising autoencoder is employed to encode in hospital records of each patient into a low dimensional dense vector. Based on EHR data collected from Shanghai Shuguang Hospital, we experimentally evaluate our proposed method on both mortality prediction and comorbidity prediction tasks. Experimental studies show that our proposed method outperforms other reference methods based on raw EHRs data. We also apply the “Deep Feature” represented by our method to track similar patients with t-SNE, which also achieves interesting results.},
author = {Lei, Liqi and Zhou, Yangming and Zhai, Jie and Zhang, Le and Fang, Zhijia and He, Ping and Gao, Ju},
doi = {10.1109/BIBM.2018.8621542},
file = {:Users/andrea/Documents/Papers/zhou2019.pdf:pdf},
isbn = {9781538654880},
journal = {Proceedings - 2018 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018},
keywords = {Deep learning,electronic health records,recurrent neural network,representation learning},
number = {February},
pages = {885--892},
title = {{An Effective Patient Representation Learning for Time-series Prediction Tasks Based on EHRs}},
year = {2019}
}
@article{Bose2018,
abstract = {Objective:Patients often overstay in intensive care units (ICU) after they are deemed discharge ready. The objective of this study was to examine the impact of such discharge delays (DD) on subsequent in-hospital morbidity and mortality.Design:Retrospective cohort study.Setting:Single tertiary academic medical center.Patients:Adult patients admitted to the medical ICU between 2005 and 2011.Interventions:For all patients, DD (ie, time between request for a ward bed and time of ICU discharge) was calculated. Discharge delays was dichotomized as long (≥24 hours) or short ({\textless}24 hours). Multivariable linear and logistic regressions were used to assess the association between dichotomized DD and post-ICU clinical outcomes.Results:Overall, 9673 discharges were included of which 10.4{\%} patients had long DDs. In the fully adjusted model, a long delay was not associated with increased odds of death (adjusted odds ratio [aOR]: 0.99, 95{\%} confidence interval [CI]: 0.74-1.31, P = .95) but was associated with a shorter lo...},
author = {Bose, Somnath and Johnson, Alistair E. W. and Moskowitz, Ari and Celi, Leo Anthony and Raffa, Jesse D.},
doi = {10.1177/0885066618800276},
isbn = {0885066618},
issn = {0885-0666},
journal = {Journal of Intensive Care Medicine},
keywords = {ICU administration,critical care utilization,delay,discharge delay,intensive care unit discharge,workflow},
pages = {088506661880027},
title = {{Impact of Intensive Care Unit Discharge Delays on Patient Outcomes: A Retrospective Cohort Study}},
url = {http://journals.sagepub.com/doi/10.1177/0885066618800276},
year = {2018}
}
@article{Johnson2018,
abstract = {OBJECTIVES To evaluate the relative validity of criteria for the identification of sepsis in an ICU database. DESIGN Retrospective cohort study of adult ICU admissions from 2008 to 2012. SETTING Tertiary teaching hospital in Boston, MA. PATIENTS Initial admission of all adult patients to noncardiac surgical ICUs. INTERVENTIONS Comparison of five different algorithms for retrospectively identifying sepsis, including the Sepsis-3 criteria. MEASUREMENTS AND MAIN RESULTS 11,791 of 23,620 ICU admissions (49.9{\%}) met criteria for the study. Within this subgroup, 59.9{\%} were suspected of infection on ICU admission, 75.2{\%} of admissions had Sequential Organ Failure Assessment greater than or equal to 2, and 49.1{\%} had both suspicion of infection and Sequential Organ Failure Assessment greater than or equal to 2 thereby meeting the Sepsis-3 criteria. The area under the receiver operator characteristic of Sequential Organ Failure Assessment (0.74) for hospital mortality was consistent with previous studies of the Sepsis-3 criteria. The Centers for Disease Control and Prevention, Angus, Martin, Centers for Medicare {\&} Medicaid Services, and explicit coding methods for identifying sepsis revealed respective sepsis incidences of 31.9{\%}, 28.6{\%}, 14.7{\%}, 11.0{\%}, and 9.0{\%}. In-hospital mortality increased with decreasing cohort size, ranging from 30.1{\%} (explicit codes) to 14.5{\%} (Sepsis-3 criteria). Agreement among the criteria was acceptable (Cronbach's alpha, 0.40-0.62). CONCLUSIONS The new organ dysfunction-based Sepsis-3 criteria have been proposed as a clinical method for identifying sepsis. These criteria identified a larger, less severely ill cohort than that identified by previously used administrative definitions. The Sepsis-3 criteria have several advantages over prior methods, including less susceptibility to coding practices changes, provision of temporal context, and possession of high construct validity. However, the Sepsis-3 criteria also present new challenges, especially when calculated retrospectively. Future studies on sepsis should recognize the differences in outcome incidence among identification methods and contextualize their findings according to the different cohorts identified.},
author = {Johnson, Alistair E. W. and Aboab, Jerome and Raffa, Jesse D. and Pollard, Tom J. and Deliberato, Rodrigo O. and Celi, Leo A. and Stone, David J.},
doi = {10.1097/CCM.0000000000002965},
file = {:Users/andrea/Library/Application Support/Mendeley Desktop/Downloaded/Johnson et al. - 2018 - A Comparative Analysis of Sepsis Identification Methods in an Electronic Database.pdf:pdf},
isbn = {0000000000},
issn = {0090-3493},
journal = {Critical Care Medicine},
pmid = {29303796},
title = {{A Comparative Analysis of Sepsis Identification Methods in an Electronic Database}},
year = {2018}
}
@article{Pearl2018,
abstract = {Current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling.},
archivePrefix = {arXiv},
arxivId = {1801.04016},
author = {Pearl, Judea},
doi = {10.1145/3159652.3176182},
eprint = {1801.04016},
file = {:Users/andrea/Documents/Papers/Pearl2018.pdf:pdf},
isbn = {9781450355810},
pages = {1--8},
title = {{Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution}},
url = {http://arxiv.org/abs/1801.04016},
year = {2018}
}
@article{Folgado2018,
abstract = {When a comparison between time series is required, measurement functions provide meaningful scores to characterize similarity between sequences. Quite often, time series appear warped in time, i.e, although they may exhibit amplitude and shape similarity, they appear dephased in time. The most common algorithm to overcome this challenge is the Dynamic Time Warping, which aligns each sequence prior establishing distance measurements. However, Dynamic Time Warping takes only into account amplitude similarity. A distance which characterizes the degree of time warping between two sequences can deliver new insights for applications where the timing factor is essential, such well-defined movements during sports or rehabilitation exercises. We propose a novel measurement called Time Alignment Measurement, which delivers similarity information on the temporal domain. We demonstrate the potential of our approach in measuring performance of time series alignment methodologies and in the characterization of synthetic and real time series data acquired during human movement.},
author = {Folgado, Duarte and Barandas, Mar{\'{i}}lia and Matias, Ricardo and Martins, Rodrigo and Carvalho, Miguel and Gamboa, Hugo},
doi = {10.1016/j.patcog.2018.04.003},
file = {:Users/andrea/Documents/Papers/folgado2018.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Distance,Signal alignment,Similarity,Time series,Time warping},
pages = {268--279},
publisher = {Elsevier Ltd},
title = {{Time Alignment Measurement for Time Series}},
url = {https://doi.org/10.1016/j.patcog.2018.04.003},
volume = {81},
year = {2018}
}
@article{Hoffmann2017,
abstract = {Longitudinal electronic health records on 99,785 Genetic Epidemiology Research on Adult Health and Aging (GERA) cohort individuals provided 1,342,814 systolic and diastolic blood pressure measurements for a genome-wide association study on long-term average systolic, diastolic, and pulse pressure. We identified 39 new loci among 75 genome-wide significant loci (P ≤ 5 × 10-8), with most replicating in the combined International Consortium for Blood Pressure (ICBP; n = 69,396) and UK Biobank (UKB; n = 152,081) studies. Combining GERA with ICBP yielded 36 additional new loci, with most replicating in UKB. Combining all three studies (n = 321,262) yielded 241 additional genome-wide significant loci, although no replication sample was available for these. All associated loci explained 2.9{\%}, 2.5{\%}, and 3.1{\%} of variation in systolic, diastolic, and pulse pressure, respectively, in GERA non-Hispanic whites. Using multiple blood pressure measurements in GERA doubled the variance explained. A normalized risk score was associated with time to onset of hypertension (hazards ratio = 1.18, P = 8.2 × 10-45). Expression quantitative trait locus analysis of blood pressure loci showed enrichment in aorta and tibial artery.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Hoffmann, Thomas J. and Ehret, Georg B. and Nandakumar, Priyanka and Ranatunga, Dilrini and Schaefer, Catherine and Kwok, Pui Yan and Iribarren, Carlos and Chakravarti, Aravinda and Risch, Neil},
doi = {10.1038/ng.3715},
eprint = {15334406},
file = {:Users/andrea/Documents/Papers/Hoffmann2017.pdf:pdf},
isbn = {0000287431},
issn = {15461718},
journal = {Nature Genetics},
number = {1},
pages = {54--64},
pmid = {27841878},
title = {{Genome-wide association analyses using electronic health records identify new loci influencing blood pressure variation}},
url = {https://www.nature.com/articles/ng.3715.pdf},
volume = {49},
year = {2017}
}
@article{Wu2012,
abstract = {In this paper, we extend the modified lasso of Wang et al. (2007) to the linear regression model with autoregressive moving average (ARMA) errors. Such an extension is far from trivial because new devices need to be called for to establish the asymptotics due to the existence of the moving average component. A shrinkage procedure is proposed to simultaneously estimate the parameters and select the informative variables in the regression, autoregressive, and moving average components. We show that the resulting estimator is consistent in both parameter estimation and variable selection, and enjoys the oracle properties. To overcome the complexity in numerical computation caused by the existence of the moving average component, we propose a procedure based on a least squares approximation to implement estimation. The ordinary least squares formulation with the use of the modified lasso makes the computation very efficient. Simulation studies are conducted to evaluate the finite sample performance of the procedure. An empirical example of ground-level ozone is also provided. {\textcopyright} 2012 Elsevier B.V.},
author = {Wu, Rongning and Wang, Qin},
doi = {10.1016/j.jspi.2012.02.047},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Modified lasso,Oracle estimator,Regression model with ARMA errors,Shrinkage estimation,Variable selection},
number = {7},
pages = {2136--2148},
title = {{Shrinkage estimation for linear regression with ARMA errors}},
volume = {142},
year = {2012}
}
@article{lokhandwala2018,
abstract = {Rationale Factors associated with one-year mortality after recovery from critical illness are not well understood. Clinicians generally lack information regarding post-hospital discharge outcomes of patients from the intensive care unit, which may be important when counseling patients and families. Objective We sought to determine which factors among patients who survived for at least 30 days post-ICU admission are associated with one-year mortality. Methods Single-center, longitudinal retrospective cohort study of all ICU patients admitted to a tertiary-care academic medical center from 2001-2012 who survived 30 days from ICU admission. Cox's proportional hazards model was used to identify the variables that are associated with one-year mortality. The primary outcome was one-year mortality. Results 32,420 patients met the inclusion criteria and were included in the study. Among patients who survived to 30 days, 28,583 (88.2{\%}) survived for greater than one year, whereas 3,837 (11.8{\%}) did not. Variables associated with decreased one-year survival include: increased age, malignancy, number of hospital admissions within the prior year, duration of mechanical ventilation and vasoactive agent use, sepsis, history of congestive heart failure, end-stage renal disease, cirrhosis, chronic obstructive pulmonary disease, and the need for renal replacement therapy. Numerous effect modifications between these factors were found. Conclusion Among survivors of critical illness, a significant number survive less than one year. More research is needed to help clinicians accurately identify those patients who, despite surviving their acute illness, are likely to suffer one-year mortality, and thereby to improve the quality of the decisions and care that impact this outcome.Copyright {\textcopyright} 2018 Lokhandwala et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
author = {Lokhandwala, Sharukh and McCague, Ned and Chahin, Abdullah and Escobar, Braiam and Feng, Mengling and Ghassemi, Mohammad M. and Stone, David J and Celi, Leo Anthony},
doi = {http://dx.doi.org/10.1371/journal.pone.0197226},
file = {:Users/andrea/Documents/Papers/lokhandwala2018.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLoS ONE},
keywords = {*critical illness,*mortality,*prognosis,adult,aged,aging,article,artificial ventilation,chronic obstructive lung disease,cohort analysis,congestive heart failure,end stage renal disease,female,hospital admission,human,liver cirrhosis,longitudinal study,major clinical study,male,malignant neoplasm,renal replacement therapy,retrospective study,risk factor,sepsis,survival,vasoactive agent},
number = {5},
pages = {e0197226},
title = {{One-year mortality after recovery from critical illness: A retrospective cohort study}},
url = {http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0197226{\&}type=printable{\%}0Ahttp://ovidsp.ovid.com/ovidweb.cgi?T=JS{\&}PAGE=reference{\&}D=emexb{\&}NEWS=N{\&}AN=622125433},
volume = {13},
year = {2018}
}
@article{Esteva2019,
abstract = {Here we present deep-learning techniques for healthcare, centering our discussion on deep learning in computer vision, natural language processing, reinforcement learning, and generalized methods. We describe how these computational techniques can impact a few key areas of medicine and explore how to build end-to-end systems. Our discussion of computer vision focuses largely on medical imaging, and we describe the application of natural language processing to domains such as electronic health record data. Similarly, reinforcement learning is discussed in the context of robotic-assisted surgery, and generalized deep-learning methods for genomics are reviewed.},
author = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
doi = {10.1038/s41591-018-0316-z},
file = {:Users/andrea/Documents/Papers/esteva2019.pdf:pdf},
issn = {1546-170X},
journal = {Nature medicine},
number = {1},
pages = {24--29},
pmid = {30617335},
publisher = {Springer US},
title = {{A guide to deep learning in healthcare.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30617335},
volume = {25},
year = {2019}
}
@article{henderson2017,
author = {Henderson, Jette and Ho, Joyce C and Kho, Abel N and Denny, Joshua C and Malin, Bradley A and Sun, Jimeng and Ghosh, Joydeep},
doi = {10.1109/ICHI.2017.61},
file = {:Users/andrea/Documents/Papers/henderson2017.pdf:pdf},
isbn = {9781509048816},
keywords = {computational phenotyping,data mining,electronic health records,feature extraction,health informa-,high-throughput phenotyping,one way to perform,tensor factoriza-,tensors are,tion,tion management},
title = {{Granite : Diversified , Sparse Tensor Factorization for Electronic Health Record-Based Phenotyping}},
year = {2017}
}
@article{Doshi-Velez2017,
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
archivePrefix = {arXiv},
arxivId = {1702.08608},
author = {Doshi-Velez, Finale and Kim, Been},
doi = {10.1016/j.intell.2013.05.008},
eprint = {1702.08608},
file = {:Users/andrea/Documents/Papers/Doshi-Velez2017.pdf:pdf},
isbn = {9781450360128},
issn = {0160-2896},
number = {Ml},
pages = {1--13},
title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
url = {http://arxiv.org/abs/1702.08608},
year = {2017}
}
@article{Soleimani2018,
abstract = {Missing data and noisy observations pose significant challenges for reliably predicting events from irregularly sampled multivariate time series (longitudinal) data. Imputation methods, which are typically used for completing the data prior to event prediction, lack a principled mechanism to account for the uncertainty due to missingness. Alternatively, state-of-the-art joint modeling techniques can be used for jointly modeling the longitudinal and event data and compute event probabilities conditioned on the longitudinal observations. These approaches, however, make strong parametric assumptions and do not easily scale to multivariate signals with many observations. Our proposed approach consists of several key innovations. First, we develop a flexible and scalable joint model based upon sparse multiple-output Gaussian processes. Unlike state-of-the-art joint models, the proposed model can explain highly challenging structure including non-Gaussian noise while scaling to large data. Second, we derive an optimal policy for predicting events using the distribution of the event occurrence estimated by the joint model. The derived policy trades-off the cost of a delayed detection versus incorrect assessments and abstains from making decisions when the estimated event probability does not satisfy the derived confidence criteria. Experiments on a large dataset show that the proposed framework significantly outperforms state-of-the-art techniques in event prediction.},
archivePrefix = {arXiv},
arxivId = {1708.04757},
author = {Soleimani, Hossein and Hensman, James and Saria, Suchi},
doi = {10.1109/TPAMI.2017.2742504},
eprint = {1708.04757},
file = {:Users/andrea/Documents/Papers/soleimani2018.pdf:pdf},
isbn = {1939-3539 (Electronic)0098-5589 (Linking)},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Uncertainty-aware prediction,joint modeling,missing data,scalable Gaussian processes,survival analysis,time series},
number = {8},
pages = {1948--1963},
pmid = {28841550},
publisher = {IEEE},
title = {{Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction}},
volume = {40},
year = {2018}
}
@article{Dai2018,
abstract = {Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions.},
author = {Dai, Bin and Wang, Yu and Aston, John and Hua, Gang and Wipf, David},
file = {:Users/andrea/Documents/Papers/dai2018.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep Generative Model,Robust PCA,Variational Autoencoder},
number = {41},
pages = {1--42},
title = {{Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models}},
url = {http://jmlr.org/papers/v19/17-704.html},
volume = {19},
year = {2018}
}
@article{Candes2018a,
abstract = {Many contemporary large-scale applications involve building interpretable models linking a large set of potential covariates to a response in a nonlinear fashion, such as when the response is binary. Although this modeling problem has been extensively studied, it remains unclear how to effectively control the fraction of false discoveries even in high-dimensional logistic regression, not to mention general high-dimensional nonlinear models. To address such a practical problem, we propose a new framework of {\$}model{\$}-{\$}X{\$} knockoffs, which reads from a different perspective the knockoff procedure (Barber and Cand$\backslash$`es, 2015) originally designed for controlling the false discovery rate in linear models. Whereas the knockoffs procedure is constrained to homoscedastic linear models with {\$}n\backslashge p{\$}, the key innovation here is that model-X knockoffs provide valid inference from finite samples in settings in which the conditional distribution of the response is arbitrary and completely unknown. Furthermore, this holds no matter the number of covariates. Correct inference in such a broad setting is achieved by constructing knockoff variables probabilistically instead of geometrically. To do this, our approach requires the covariates be random (independent and identically distributed rows) with a distribution that is known, although we provide preliminary experimental evidence that our procedure is robust to unknown/estimated distributions. To our knowledge, no other procedure solves the {\$}controlled{\$} variable selection problem in such generality, but in the restricted settings where competitors exist, we demonstrate the superior power of knockoffs through simulations. Finally, we apply our procedure to data from a case-control study of Crohn's disease in the United Kingdom, making twice as many discoveries as the original analysis of the same data.},
archivePrefix = {arXiv},
arxivId = {1610.02351},
author = {Cand{\`{e}}s, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
doi = {10.1111/rssb.12265},
eprint = {1610.02351},
file = {:Users/andrea/Documents/Papers/Candes2018.pdf:pdf},
isbn = {0011-7471},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {False discovery rate,Generalized linear models,Genomewide association study,Knockoff filter,Logistic regression,Markov blanket,Testing for conditional independence in non-linear},
number = {3},
pages = {551--577},
title = {{Panning for gold: ‘model-X' knockoffs for high dimensional controlled variable selection}},
volume = {80},
year = {2018}
}
@article{boone2016,
abstract = {Purpose: Prior studies report that weekend admission to an intensive care unit is associated with increased mortality, potentially attributed to the organizational structure of the unit. This study aims to determine whether treatment of hypotension, a risk factor for mortality, differs according to level of staffing. Methods: Using the Multiparameter Intelligent Monitoring in Intensive Care database, we conducted a retrospective study of patients admitted to an intensive care unit at Beth Israel Deaconess Medical Center who experienced one or more episodes of hypotension. Episodes were categorized according to the staffing level, defined as high during weekday daytime (7 am-7 pm) and low during weekends or nighttime (7 pm-7 am). Results: Patients with a hypotensive event on a weekend were less likely to be treated compared with those that occurred during the weekday daytime (P = .02). No association between weekday daytime vs weekday nighttime staffing levels and treatment of hypotension was found (risk ratio, 1.02; 95{\%} confidence interval, 0.98-1.07). Conclusion: Patients with a hypotensive event on a weekend were less likely to be treated than patients with an event during high-staffing periods. No association between weekday nighttime staffing and hypotension treatment was observed. We conclude that treatment of a hypotensive episode relies on more than solely staffing levels.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Boone, M. Dustin and Massa, Jennifer and Mueller, Ariel and Jinadasa, Sayuri P. and Lee, Joon and Kothari, Rishi and Scott, Daniel J. and Callahan, Julie and Celi, Leo Anthony and Hacker, Michele R.},
doi = {10.1016/j.jcrc.2016.02.009},
eprint = {15334406},
file = {:Users/andrea/Documents/Papers/boone2016.pdf:pdf},
isbn = {0000287431},
issn = {15578615},
journal = {Journal of Critical Care},
keywords = {Critical care,Hospital medical staff,Hypotension,Intensive care unit,Resuscitation},
pages = {14--18},
pmid = {26975737},
publisher = {Elsevier Inc.},
title = {{The organizational structure of an intensive care unit influences treatment of hypotension among critically ill patients: A retrospective cohort study}},
url = {http://dx.doi.org/10.1016/j.jcrc.2016.02.009},
volume = {33},
year = {2016}
}
@article{Biecek2018,
abstract = {Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). Such methods are usually described by a large number of parameters or hyper parameters - a price that one needs to pay for elasticity. The very number of parameters makes models hard to understand. This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes. Each explainer is a technique for exploration of a black box model. Presented approaches are model-agnostic, what means that they extract useful information from any predictive method despite its internal structure. Each explainer is linked with a specific aspect of a model. Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable. Every explainer presented in this paper works for a single model or for a collection of models. In the latter case, models can be compared against each other. Such comparison helps to find strengths and weaknesses of different approaches and gives additional possibilities for model validation. Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended. The current implementation supports the most popular frameworks for classification and regression.},
archivePrefix = {arXiv},
arxivId = {1806.08915},
author = {Biecek, Przemyslaw},
eprint = {1806.08915},
file = {:Users/andrea/Documents/Papers/biecek2018.pdf:pdf},
keywords = {explainable artificial intelligence,interpretable machine learning,model visualization,modelling,predictive},
pages = {1--5},
title = {{DALEX: explainers for complex predictive models}},
url = {http://arxiv.org/abs/1806.08915},
volume = {19},
year = {2018}
}
@article{Beaulieu-jones2017,
abstract = {Electronic health records (EHRs) have become a vital source of patient outcome data but the widespread prevalence of missing data presents a major challenge. Different causes of missing data in the EHR data may introduce unintentional bias. Here, we compare the effectiveness of popular multiple imputation strategies with a deeply learned autoencoder using the Pooled Resource Open-Access ALS Clinical Trials Database (PRO-ACT). To evaluate performance, we examined imputation accuracy for known values simulated to be either missing completely at random or missing not at random. We also compared ALS disease progression prediction across different imputation models. Autoencoders showed strong performance for imputation accuracy and contributed to the strongest disease progression predictor. Finally, we show that despite clinical heterogeneity, ALS disease progression appears homogenous with time from onset being the most important predictor.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Beaulieu-jones, Brett K and Moore, Jason H.},
doi = {10.1142/9789813207813_0021},
eprint = {15334406},
file = {:Users/andrea/Documents/Papers/beaulieu-jones2016.pdf:pdf},
isbn = {978-981-320-780-6},
issn = {2335-6936},
number = {c},
pages = {2--5},
pmid = {27896976},
title = {{Missing data imputation in the electronic health records using deeply learned autoencoders}},
volume = {0000},
year = {2017}
}
@article{Jensen2012,
abstract = {Clinical data describing the phenotypes and treatment of patients represents an underused data source that has much greater research potential than is currently realized. Mining of electronic health records (EHRs) has the potential for establishing new patient-stratification principles and for revealing unknown disease correlations. Integrating EHR data with genetic data will also give a finer understanding of genotype-phenotype relationships. However, a broad range of ethical, legal and technical reasons currently hinder the systematic deposition of these data in EHRs and their mining. Here, we consider the potential for furthering medical research and clinical care using EHR data and the challenges that must be overcome before this is a reality.},
author = {Jensen, Peter B. and Jensen, Lars J. and Brunak, So{\o}ren},
doi = {10.1038/nrg3208},
isbn = {1471-0064 (Electronic)$\backslash$r1471-0056 (Linking)},
issn = {14710056},
journal = {Nature Reviews Genetics},
number = {6},
pages = {395--405},
pmid = {22549152},
publisher = {Nature Publishing Group},
title = {{Mining electronic health records: Towards better research applications and clinical care}},
url = {http://dx.doi.org/10.1038/nrg3208},
volume = {13},
year = {2012}
}
@article{johnson2018,
abstract = {A large volume of research has considered the creation of predictive models for clinical data; however, much existing literature reports results using only a single source of data. In this work, we evaluate the performance of models trained on the publicly-available eICU Collaborative Research Database. We show that cross-validation using many distinct centers provides a reasonable estimate of model performance in new centers. We further show that a single model trained across centers transfers well to distinct hospitals, even compared to a model retrained using hospital-specific data. Our results motivate the use of multi-center datasets for model development and highlight the need for data sharing among hospitals to maximize model performance.},
archivePrefix = {arXiv},
arxivId = {1812.02275v1},
author = {Johnson, Alistair E W and Pollard, Tom J and Naumann, Tristan},
doi = {arXiv:1812.02275v1},
eprint = {1812.02275v1},
file = {:Users/andrea/Documents/Papers/johnson2018.pdf:pdf},
isbn = {1812.02275v1},
title = {{Generalizability of predictive models for intensive care unit patients}},
url = {https://github.com/alistairewj/icu-model-transfer},
year = {2018}
}
@article{Bach2012,
abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. While naturally cast as a combinatorial optimization problem, variable or feature selection admits a convex relaxation through the regularization by the {\$}\backslashell{\_}1{\$}-norm. In this paper, we consider situations where we are not only interested in sparsity, but where some structural prior knowledge is available as well. We show that the {\$}\backslashell{\_}1{\$}-norm can then be extended to structured norms built on either disjoint or overlapping groups of variables, leading to a flexible framework that can deal with various structures. We present applications to unsupervised learning, for structured sparse principal component analysis and hierarchical dictionary learning, and to supervised learning in the context of non-linear variable selection.},
annote = {Very important!},
archivePrefix = {arXiv},
arxivId = {1109.2397},
author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
doi = {10.1214/12-STS394},
eprint = {1109.2397},
file = {::},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Sparsity, Convex optimization,,and phrases,convex optimization,sparsity},
number = {4},
pages = {450--468},
title = {{Structured Sparsity through Convex Optimization}},
url = {http://projecteuclid.org/euclid.ss/1356098550},
volume = {27},
year = {2012}
}
@article{Stretch2017,
abstract = {OBJECTIVES: Hospitals use a variety of strategies to maximize the availability of limited ICU beds. Boarding, which involves assigning patients to an open bed in a different subspecialty ICU, is one such practice employed when ICU occupancy levels are high, and beds in a particular unit are unavailable. Boarding disrupts the normal geographic colocation of patients and care teams, exposing patients to nursing staff with different training and expertise to those caring for nonboarders. We analyzed whether medical ICU patients boarding in alternative specialty ICUs are at increased risk of mortality. DESIGN: Retrospective cohort study using an instrumental variable analysis to control for unmeasured confounding. A semiparametric bivariate probit estimation strategy was employed for the instrumental model. Propensity score matching and standard logistic regression (generalized linear modeling) were used as robustness checks. SETTING: The medical ICU of a tertiary care nonprofit hospital in the United States between 2002 and 2012. PATIENTS: All medical ICU admissions during the specified time period. INTERVENTIONS: None. MEASUREMENTS AND MAIN RESULTS: The study population consisted of 8,429 patients of whom 1,871 were boarders. The instrumental variable model demonstrated a relative risk of 1.18 (95{\%} CI, 1.01-1.38) for ICU stay mortality for boarders. The relative risk of in-hospital mortality among boarders was 1.22 (95{\%} CI, 1.00-1.49). GLM and propensity score matching without use of the instrument yielded similar estimates. Instrumental variable estimates are for marginal patients, whereas generalized linear modeling and propensity score matching yield population average effects. CONCLUSIONS: Mortality increased with boarding of critically ill patients. Further research is needed to identify safer practices for managing patients during periods of high ICU occupancy.},
author = {Stretch, Robert and {Della Penna}, Nicol{\'{a}}s and Celi, Leo Anthony and Landon, Bruce E.},
doi = {10.1097/CCM.0000000000002905},
file = {::},
isbn = {0000000000},
issn = {0090-3493},
journal = {Critical Care Medicine},
title = {{Effect of Boarding on Mortality in ICUs}},
year = {2018}
}
@article{Rani2016,
abstract = {Missing attribute values are quite common in the datasets available in the literature. Missing values are also possible because all attributes values may not be recorded and hence unavailable due to several practical reasons. For all these one must fix missing attribute vales if the analysis has to be done. Imputation is the first step in analyzing medical datasets. Hence this has achieved significant contribution from several medical domain researchers. Several data mining researchers have proposed various methods and approaches to impute missing values. However very few of them concentrate on dimensionality reduction. In this paper, we discuss a novel imputation framework for missing values imputation. Our approach of filling missing values is rooted on class based clustering approach and essentially aims at medical records dimensionality reduction. We use these dimensionality records for carrying prediction and classification analysis. A case study is discussed which shows how imputation is performed using proposed method.},
archivePrefix = {arXiv},
arxivId = {1605.01010},
author = {Rani, Y. Usha and Sammulal, P.},
eprint = {1605.01010},
file = {:Users/andrea/Documents/Papers/usharani2016b.pdf:pdf},
issn = {02540770},
journal = {Revista Tecnica de la Facultad de Ingenieria Universidad del Zulia},
keywords = {Attribute domain,Classification,Clustering,Impute,Medical record,Prediction},
number = {2},
pages = {184--195},
title = {{A novel approach for imputation of missing attribute values for efficient mining of medical datasets class based cluster approach}},
volume = {39},
year = {2016}
}
@article{Ahrens2015,
abstract = {The vast majority of spatial econometric research relies on the assumption that the spatial network structure is known a priori. This study considers a two-step estimation strategy for estimating the n(n-1) interaction effects in a spatial autoregressive panel model where the spatial dimension is potentially large. The identifying assumption is approximate sparsity of the spatial weights matrix. The proposed estimation methodology exploits the Lasso estimator and mimics two-stage least squares (2SLS) to account for endogeneity of the spatial lag. The developed two-step estimator is of more general interest. It may be used in applications where the number of endogenous regressors and the number of instrumental variables is larger than the number of observations. We derive convergence rates for the two-step Lasso estimator. Our Monte Carlo simulation results show that the two-step estimator is consistent and successfully recovers the spatial network structure for reasonable sample size, T.},
author = {Ahrens, Achim and Bhattacharjee, Arnab},
doi = {10.3390/econometrics3010128},
issn = {2225-1146},
journal = {Econometrics},
number = {1},
pages = {128--155},
title = {{Two-Step Lasso Estimation of the Spatial Weights Matrix}},
url = {http://www.mdpi.com/2225-1146/3/1/128/},
volume = {3},
year = {2015}
}
@article{Fanaee-T2016,
abstract = {Traditional spectral-based methods such as PCA are popular for anomaly detection in a variety of problems and domains. However, if data includes tensor (multiway) structure (e.g. space-time-measurements), some meaningful anomalies may remain invisible with these methods. Although tensor-based anomaly detection (TAD) has been applied within a variety of disciplines over the last twenty years, it is not yet recognized as a formal category in anomaly detection. This survey aims to highlight the potential of tensor-based techniques as a novel approach for detection and identification of abnormalities and failures. We survey the interdisciplinary works in which TAD is reported and characterize the learning strategies, methods and applications; extract the important open issues in TAD and provide the corresponding existing solutions according to the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fanaee-T, Hadi and Gama, Jo{\~{a}}o},
doi = {10.1016/j.knosys.2016.01.027},
eprint = {arXiv:1011.1669v3},
file = {:Users/andrea/Documents/Papers/Fanaee2016.pdf:pdf},
isbn = {978-3-540-75176-2},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Anomaly detection,Multiway data,Tensor analysis,Tensor decomposition,Tensorial learning},
pages = {130--147},
pmid = {25246403},
publisher = {Elsevier B.V.},
title = {{Tensor-based anomaly detection: An interdisciplinary survey}},
url = {http://dx.doi.org/10.1016/j.knosys.2016.01.027},
volume = {98},
year = {2016}
}
@article{Schulam2015,
author = {Schulam, Peter and Wigley, Fredrick and Saria, Suchi},
file = {:Users/andrea/Documents/Papers/schulam2015.pdf:pdf},
journal = {AI Access Foundation},
keywords = {Novel Machine Learning Algorithms Track},
pages = {2956--2964},
title = {{Clustering longitudinal clinical marker trajectories from electronic health data}},
url = {https://jhu.pure.elsevier.com/en/publications/clustering-longitudinal-clinical-marker-trajectories-from-electro},
year = {2015}
}
@article{Gong2018,
author = {Gong, Jen J and Guttag, John V},
file = {:Users/andrea/Documents/Papers/Gong2018.pdf:pdf},
pages = {1--19},
title = {{Learning to Summarize Electronic Health Records Using Cross-Modality Correspondences}},
url = {https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/5b737288575d1f627e5a09bd/1534292618033/8.pdf},
year = {2018}
}
@article{Zhang2017,
abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difficult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works. Finally, we present theoretical analyses and discuss several future directions for MTL.},
archivePrefix = {arXiv},
arxivId = {1707.08114},
author = {Zhang, Yu and Yang, Qiang},
doi = {10.3171/2015.6.SPINE141197.},
eprint = {1707.08114},
file = {:Users/andrea/Documents/Papers/Zhang2018.pdf:pdf},
isbn = {9781538618806},
pages = {1--20},
pmid = {149241},
title = {{A Survey on Multi-Task Learning}},
url = {http://arxiv.org/abs/1707.08114},
year = {2017}
}
@article{Liang2019,
abstract = {Artificial intelligence (AI)-based methods have emerged as powerful tools to transform medical care. Although machine learning classifiers (MLCs) have already demonstrated strong performance in image-based diagnoses, analysis of diverse and massive electronic health record (EHR) data remains challenging. Here, we show that MLCs can query EHRs in a manner similar to the hypothetico-deductive reasoning used by physicians and unearth associations that previous statistical methods have not found. Our model applies an automated natural language processing system using deep learning techniques to extract clinically relevant information from EHRs. In total, 101.6 million data points from 1,362,559 pediatric patient visits presenting to a major referral center were analyzed to train and validate the framework. Our model demonstrates high diagnostic accuracy across multiple organ systems and is comparable to experienced pediatricians in diagnosing common childhood diseases. Our study provides a proof of concept for implementing an AI-based system as a means to aid physicians in tackling large amounts of data, augmenting diagnostic evaluations, and to provide clinical decision support in cases of diagnostic uncertainty or complexity. Although this impact may be most evident in areas where healthcare providers are in relative shortage, the benefits of such an AI system are likely to be universal.},
author = {Liang, Huiying and Tsui, Brian Y. and Ni, Hao and Valentim, Carolina C. S. and Baxter, Sally L. and Liu, Guangjian and Cai, Wenjia and Kermany, Daniel S. and Sun, Xin and Chen, Jiancong and He, Liya and Zhu, Jie and Tian, Pin and Shao, Hua and Zheng, Lianghong and Hou, Rui and Hewett, Sierra and Li, Gen and Liang, Ping and Zang, Xuan and Zhang, Zhiqi and Pan, Liyan and Cai, Huimin and Ling, Rujuan and Li, Shuhua and Cui, Yongwang and Tang, Shusheng and Ye, Hong and Huang, Xiaoyan and He, Waner and Liang, Wenqing and Zhang, Qing and Jiang, Jianmin and Yu, Wei and Gao, Jianqun and Ou, Wanxing and Deng, Yingmin and Hou, Qiaozhen and Wang, Bei and Yao, Cuichan and Liang, Yan and Zhang, Shu and Duan, Yaou and Zhang, Runze and Gibson, Sarah and Zhang, Charlotte L. and Li, Oulan and Zhang, Edward D. and Karin, Gabriel and Nguyen, Nathan and Wu, Xiaokang and Wen, Cindy and Xu, Jie and Xu, Wenqin and Wang, Bochu and Wang, Winston and Li, Jing and Pizzato, Bianca and Bao, Caroline and Xiang, Daoman and He, Wanting and He, Suiqin and Zhou, Yugui and Haw, Weldon and Goldbaum, Michael and Tremoulet, Adriana and Hsu, Chun-Nan and Carter, Hannah and Zhu, Long and Zhang, Kang and Xia, Huimin},
doi = {10.1038/s41591-018-0335-9},
file = {:Users/andrea/Documents/Papers/liang2019.pdf:pdf},
issn = {1078-8956},
journal = {Nature Medicine},
keywords = {Computational biology and bioinformatics,Data integration,Health care,Paediatrics},
pages = {1},
pmid = {30742121},
publisher = {Springer US},
title = {{Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence}},
url = {http://www.nature.com/articles/s41591-018-0335-9},
year = {2019}
}
@article{Bouabana2018,
abstract = {The massive influx of data in healthcare encouraged the building of data-driven
machine learning models in order to improve medical diagnosis and assess patients
health status. Thus, gaining knowledge and actionable insights from this complex,
heterogeneous, poorly annotated and generally unstructured data remains a key
challenge in transforming healthcare.
In this thesis, we focus on consecutive events in patients Electronic Health Records
(EHR) data to predict organ failure in the ICU. We first investigate the ability of
both logistic regression and random forest classifiers to achieve this task. Then,
we compare these methods along with convolutional neural networks (CNN) for
prediction of multiple organ failures: cardiovascular and pulmonary. Our predictions
are done using a restricted set of parameters to enable “real-time” usage in the
ICU. Predictions are made three hours in the future to support clinically actionable
planning while having a 90 minutes window of parameters history. Our results
suggest that the CNN has substantial additional predictive value for cardiovascular
and pulmonary organ failures among critically ill patients.},
author = {Bouabana, Hatem},
file = {:Users/andrea/Documents/Papers/bouabana2018.pdf:pdf},
pages = {64},
title = {{Predicting Deterioration of Patients in the Intensive Care Unit}},
url = {https://tampub.uta.fi/bitstream/handle/10024/103811/1530013271.pdf?sequence=1{\&}isAllowed=y},
year = {2018}
}
@article{Vogt2019,
abstract = {Background
Balance training is an important aspect in prevention and rehabilitation of musculoskeletal lower limb injuries. Virtual reality (VR) is a promising addition or alternative to traditional training. This review aims to provide a comprehensive overview of VR technology and games employed, balance outcome measures, and effects for both balance prevention and balance rehabilitation following musculoskeletal lower limb impairments.
Methods
A systematic literature search was conducted in electronic databases to identify all related articles with a longitudinal study design on VR, balance, and prevention or musculoskeletal rehabilitation of the lower limbs in adult subjects between 19 and 65 years.
Results
Eleven articles concerning balance prevention and five articles regarding balance rehabilitation were included. All studies used screen-based VR and off-the-shelf gaming consoles with accompanying games. The Star Excursion Balance Test (SEBT) was the most frequently used outcome measure. Two studies found positive effects of VR balance training in healthy adults, while none reported negative effects. None of the included studies showed a significant difference in balance performance after a VR balance rehabilitation intervention compared to traditional balance training.
Conclusion
Few studies have been published concerning musculoskeletal balance rehabilitation and balance prevention in healthy adult subjects. However, the studies published have shown that VR exercises are equally effective compared to traditional balance training for both domains of application. As there is large variability between studies, recommendations for future research are given to prospectively investigate the use of VR technology for balance training.},
author = {Vogt, Sarah and Skj{\ae}ret-Maroni, Nina and Neuhaus, Dorothee and Baumeister, Jochen},
doi = {10.1016/j.ijmedinf.2019.03.009},
file = {:Users/andrea/Documents/Papers/vogt2019.pdf:pdf},
issn = {13865056},
journal = {International Journal of Medical Informatics},
keywords = {Balance training,Exergames,Lower limb,Musculoskeletal disorders,Virtual reality},
number = {April 2018},
pages = {46--58},
publisher = {Elsevier},
title = {{Virtual Reality Interventions for Balance Prevention and Rehabilitation after Musculoskeletal Lower Limb Impairments in Young up to Middle-Aged Adults: A Comprehensive Review on Used Technology, Balance Outcome Measures and Observed Effects}},
url = {https://doi.org/10.1016/j.ijmedinf.2019.03.009},
volume = {126},
year = {2019}
}
@article{Lehman2016b,
author = {Lehman, Li-wei H and Mark, Roger G and Nemati, Shamim},
doi = {10.1109/JBHI.2016.2636808},
file = {:Users/andrea/Documents/Papers/Lehman2016b.pdf:pdf},
number = {c},
pages = {1--11},
title = {{A Model-Based Machine Learning Approach to Probing Autonomic Regulation from Nonstationary Vital-Sign Time Series}},
volume = {2194},
year = {2016}
}
@article{Medeiros,
annote = {Elastic-net works well even when errors are non-gaussian and conditionally heteroskedastic},
author = {Medeiros, Marcelo C and Mendes, Eduardo F and Moreira, Marcelo J and Guerre, Emmanuel and Jiang, Wenxin and Tanner, Martin and Hillebrand, Eric},
file = {::},
keywords = {acknowledgements,adalasso,and thiago ferreira for,asger lunde,emmanuel guerre,eric hillebrand,fernandes,forecasting,francesco audrino,garch,insightful discussions,lasso,laurent callot,marcelo,marcelo j,martin tanner,moreira,shrinkage,simon knaus,sparse models,the authors would like,time series,to thank anders kock,wenxin jiang},
pages = {1--49},
title = {{l1 Estimation of high dimensional time series models with flexible innovations}}
}
@article{Kolda2009,
abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition:CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
archivePrefix = {arXiv},
arxivId = {1404.3905},
author = {Kolda, Tamara G. and Bader, Brett W.},
doi = {10.1137/07070111X},
eprint = {1404.3905},
file = {:Users/andrea/Documents/Papers/Kolda2009.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {15a69,65f99,ams subject classifications,candecomp,canonical decomposition,higher-order principal components analysis,higher-order singular value decomposition,hosvd,multilinear algebra,multiway arrays,parafac,parallel factors,tensor decompositions,tucker},
number = {3},
pages = {455--500},
title = {{Tensor Decompositions and Applications}},
url = {http://epubs.siam.org/doi/10.1137/07070111X},
volume = {51},
year = {2009}
}
@article{Cao2003,
abstract = {A novel type of learning machine called support vector machine (SVM) has been receiving increasing interest in areas ranging from its original application in pattern recognition to other applications such as regression estimation due to its remarkable generalization performance. This paper deals with the application of SVM in financial time series forecasting. The feasibility of applying SVM in financial forecasting is first examined by comparing it with the multilayer back-propagation (BP) neural network and the regularized radial basis function (RBF) neural network. The variability in performance of SVM with respect to the free parameters is investigated experimentally. Adaptive parameters are then proposed by incorporating the nonstationarity of financial time series into SVM. Five real futures contracts collated from the Chicago Mercantile Market are used as the data sets. The simulation shows that among the three methods, SVM outperforms the BP neural network in financial forecasting, and there are comparable generalization performance between SVM and the regularized RBF neural network. Furthermore, the free parameters of SVM have a great effect on the generalization performance. SVM with adaptive parameters can both achieve higher generalization performance and use fewer support vectors than the standard SVM in financial forecasting.},
author = {Cao, Lijuan and Tay, Francis E. H.},
doi = {10.1109/TNN.2003.820556},
isbn = {1045-9227},
issn = {1045-9227},
journal = {IEEE Transaction on Neural Networks},
number = {6},
pages = {1506--18},
pmid = {18244595},
title = {{Support vector machine with adaptive parameters in financial time series forecasting.}},
volume = {14},
year = {2003}
}
@article{Che2017,
abstract = {Parkinson's disease (PD) is a chronic disease that de-velops over years and varies dramatically in its clini-cal manifestations. A preferred strategy to resolve this heterogeneity and thus enable better prognosis and tar-geted therapies is to segment out more homogeneous patient sub-populations. However, it is challenging to evaluate the clinical similarities among patients because of the longitudinality and temporality of their records. To address this issue, we propose a deep model that directly learns patient similarity from longitudinal and multi-modal patient records with an Recurrent Neural Network (RNN) architecture, which learns the similar-ity between two longitudinal patient record sequences through dynamically matching temporal patterns in patient sequences. Evaluations on real world patient records demonstrate the promising utility and efficacy of the proposed architecture in personalized predictions.},
author = {Che, Chao and Xiao, Cao and Liang, Jian and Jin, Bo and Zho, Jiayu and Wang, Fei},
doi = {10.1137/1.9781611974973.23},
file = {:Users/andrea/Documents/Papers/che2017.pdf:pdf},
journal = {Proceedings of the 2017 SIAM International Conference on Data Mining},
pages = {198--206},
title = {{An RNN Architecture with Dynamic Temporal Matching for Personalized Predictions of Parkinson's Disease}},
url = {https://epubs.siam.org/doi/10.1137/1.9781611974973.23},
year = {2017}
}
@article{Zhang2019,
author = {Zhang, Chunkai and Chen, Yingyang and Yin, Ao and Wang, Xuan},
doi = {10.3934/mbe.2019105},
file = {:Users/andrea/Documents/Papers/zhang2019.pdf:pdf},
issn = {1547-1063},
journal = {Mathematical Biosciences and Engineering},
keywords = {anomaly detection,binary,electrocardiography,similarity measurement,trend distance},
number = {4},
pages = {2154--2167},
title = {{Anomaly detection in ECG based on trend symbolic aggregate approximation}},
volume = {16},
year = {2019}
}
@article{Ren2013,
abstract = {Model selection (lag order selection and coefficient matrices substructures determination) is an integral part of statistical analysis of vector autoregression (VAR) models. This paper proposes a two-step shrinkage method for VAR model selection. The proposed method can be implemented through a simple algorithm. The resulting estimator is unbiased and subset-selection consistent, and the estimator of the nonzero components of the true parameter vector has asymptotically normal distribution. Limited finite sample Monte Carlo studies suggest that the proposed method outperforms existing alternatives in terms of accuracy in lag order estimation, forecasting and impulse response analysis. We also apply the proposed method to a multivariate macroeconomic time series for illustration. ?? 2013 Elsevier Inc.},
author = {Ren, Yunwen and Xiao, Zhiguo and Zhang, Xinsheng},
doi = {10.1016/j.jmva.2013.01.004},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Adaptive jump selection,Impulse response analysis,Lag order selection,VAR models},
pages = {349--364},
title = {{Two-step adaptive model selection for vector autoregressive processes}},
volume = {116},
year = {2013}
}
@article{Choi2015,
abstract = {Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79{\%} recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.},
archivePrefix = {arXiv},
arxivId = {1511.05942},
author = {Choi, Edward and Bahadori, Mohammad Taha and Schuetz, Andy and Stewart, Walter F. and Sun, Jimeng},
doi = {10.1227/01.NEU.0000065136.70455.6F},
eprint = {1511.05942},
file = {:Users/andrea/Documents/Papers/Choi2016.pdf:pdf},
isbn = {0148-396X (Print)$\backslash$n0148-396X (Linking)},
issn = {0148396X},
pmid = {12762891},
title = {{Doctor AI: Predicting Clinical Events via Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.05942},
volume = {56},
year = {2015}
}
@article{harutyunyan2018,
abstract = {Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.},
archivePrefix = {arXiv},
arxivId = {1703.07771},
author = {Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Steeg, Greg Ver and Galstyan, Aram},
doi = {10.1145/nnnnnnn.nnnnnnn},
eprint = {1703.07771},
file = {:Users/andrea/Documents/Papers/harutyunyan2018.pdf:pdf},
isbn = {9781450335492},
issn = {16130073},
pages = {1--19},
pmid = {397311},
title = {{Multitask Learning and Benchmarking with Clinical Time Series Data}},
url = {http://arxiv.org/abs/1703.07771},
year = {2018}
}
@article{rush2018,
author = {Rush, Barret and Wiskar, Katie and Celi, Leo Anthony and Walley, Keith and Russell, James and McDermid, Robert and Boyd, John},
doi = {10.1586/14737175.2015.1028369.Focused},
file = {:Users/andrea/Documents/Papers/rush2018.pdf:pdf},
isbn = {4164805765},
issn = {1744-8360},
keywords = {2,and other substances,barrier,bbb,between the plasma,blood-brain barrier,drug delivery,focused ultrasound,introduction to the blood-brain,ions,neurodegenerative disease,nutrients,the exchange of water,waste},
number = {5},
pages = {477--491},
pmid = {25936845},
title = {{Association of household income level and in-hospital mortality in patients with sepsis: a natio nwide retrospective cohort analysis}},
volume = {15},
year = {2016}
}
@article{Michailidis2013,
abstract = {Reconstructing gene regulatory networks from high-throughput measurements represents a key problem in functional genomics. It also represents a canonical learning problem and thus has attracted a lot of attention in both the informatics and the statistical learning literature. Numerous approaches have been proposed, ranging from simple clustering to rather involved dynamic Bayesian network modeling, as well as hybrid ones that combine a number of modeling steps, such as employing ordinary differential equations coupled with genome annotation. These approaches are tailored to the type of data being employed. Available data sources include static steady state data and time course data obtained either for wild type phenotypes or from perturbation experiments.This review focuses on the class of autoregressive models using time course data for inferring gene regulatory networks. The central themes of sparsity, stability and causality are discussed as well as the ability to integrate prior knowledge for successful use of these models for the learning task at hand. ?? 2013 Elsevier Inc.},
annote = {time-course data, inferring gene regulatory networks GRN

linear autoregressive models used for network inference

interactions amongst variables are defiend if past observations of one variable result in improved prediction of other variables

these seem to be over-time. a network overtime???DAG},
author = {Michailidis, George and D'Alch??-Buc, Florence},
doi = {10.1016/j.mbs.2013.10.003},
isbn = {0025-5564},
issn = {00255564},
journal = {Mathematical Biosciences},
keywords = {Autoregressive models,Causality,Dynamic Bayesian networks,Gene regulatory network inference,Sparsity,Stability},
number = {2},
pages = {326--334},
pmid = {24176667},
title = {{Autoregressive models for gene regulatory network inference: Sparsity, stability and causality issues}},
volume = {246},
year = {2013}
}
@article{Nijs2007,
abstract = {What are the drivers of retailer pricing tactics over time? Based on multivariate time-series analysis of two rich data sets, we quantify the relative importance of competitive retailer prices, pricing history, brand demand, wholesale prices, and retailer category-management considerations as drivers of retail prices. Interestingly, competitive retailer prices account for less than 10{\%} of the over-time variation in retail prices. Instead, pricing history, wholesale price, and brand demand are the main drivers of retail-price variation over time. Moreover, the influence of these price drivers on retailer pricing tactics is linked to retailer category margin. We find that demand-based pricing and category-management considerations are associated with higher retailer margins. In contrast, dependence on pricing history and pricing based on store traffic considerations imply lower retailer margins.},
annote = {application
VAR in marketing},
author = {Nijs, Vincent R. and Srinivasan, Shuba and Pauwels, Koen},
doi = {10.1287/mksc.1060.0205},
isbn = {0732-2399},
issn = {0732-2399},
journal = {Marketing Science},
number = {4},
pages = {473--487},
pmid = {26915327},
title = {{Retail-Price Drivers and Retailer Profits}},
url = {http://pubsonline.informs.org/doi/10.1287/mksc.1060.0205},
volume = {26},
year = {2007}
}
@article{Liao2019,
abstract = {Objectives: To develop and implement an integrated cloud technology with the aim of ensuring medication reconciliation during transitions of care and improve medication safety in aged societies. Methods: PharmaCloud is a new technical platform adopted by the National Health Insurance Administration of Taiwan to collect patients' medication information via cloud technology. Using this platform, healthcare providers can access patients' medication-related information with patient consent. Our hospital applied this technology and developed several approaches to collect and detect medication-related information and alert physicians for the purpose of enhancing patients' medication safety. In addition, pharmacists were involved in the admission process to access medication data and provide optimal suggestions to physicians. Several indicators, including a reduction in the number of drug items in each prescription and medication expenditure, were employed to evaluate the overall effects of the cloud inquiry. Results: After the application of PharmaCloud, the average number of prescribed drug items significantly decreased (change of 0.04 to −0.35 per prescription, p {\textless} 0.05), and the median medication expenditure significantly decreased by an average of 3.55 USD, (p {\textless} 0.05) per prescription. Intra-hospital medication duplication rates also showed a downward trend. Conclusions: The use of the cloud technology and value-added applications significantly improved patient medication safety. Further long-term beneficial effects in terms of medication safety and medical cost savings are expected.},
author = {Liao, Chieh Yu and Wu, Ming Fen and Poon, Sek Kwong and Liu, Ying Mei and Chen, Hsiu Chu and Wu, Chieh Liang and Sheu, Wayne H.H. and Liou, Wen Shyong},
doi = {10.1016/j.ijmedinf.2019.03.012},
file = {:Users/andrea/Documents/Papers/liao2019.pdf:pdf},
issn = {18728243},
journal = {International Journal of Medical Informatics},
keywords = {Cloud technology,Health Insurance Medical Information Cloud Inquiry,Patient safety,PharmaCloud,Pharmacist},
number = {March},
pages = {65--71},
publisher = {Elsevier},
title = {{Improving medication safety by cloud technology: Progression and value-added applications in Taiwan}},
url = {https://doi.org/10.1016/j.ijmedinf.2019.03.012},
volume = {126},
year = {2019}
}
@article{yeung2019,
author = {Yeung, Serena and Rinaldo, Francesca and Jopling, Jeffrey and Liu, Bingbin and Mehra, Rishab and Downing, N. Lance and Guo, Michelle and Bianconi, Gabriel M. and Alahi, Alexandre and Lee, Julia and Campbell, Brandi and Deru, Kayla and Beninati, William and Fei-Fei, Li and Milstein, Arnold},
doi = {10.1038/s41746-019-0087-z},
file = {:Users/andrea/Documents/Papers/yeung2019.pdf:pdf},
issn = {2398-6352},
journal = {npj Digital Medicine},
number = {1},
pages = {11},
publisher = {Springer US},
title = {{A computer vision system for deep learning-based detection of patient mobilization activities in the ICU}},
url = {http://www.nature.com/articles/s41746-019-0087-z},
volume = {2},
year = {2019}
}
@book{Anantasech2019,
abstract = {The proceedings contain 67 papers. The special focus in this conference is on Information and Communication Technology. The topics include: A novel methodology to detect bone cancer stage using mean intensity of MRI imagery and region growing algorithm; BCI for comparing eyes activities measured from temporal and occipital lobes; an adaptive edge-preserving image denoising using block-based singular value decomposition in wavelet domain; performance analysis of voltage controlled ring oscillators; a novel symmetric key cryptography using dynamic matrix approach; experimenting large prime numbers generation in MPI cluster; design and analysis of high performance CMOS temperature sensor using VCO; efficient data dissemination in wireless sensor network using adaptive and dynamic mobile sink based on particle swarm optimization; a new approach to intuitionistic fuzzy soft sets and its application in decision-making; categorical data clustering based on cluster ensemble process; region-based clustering approach for energy efficient wireless sensor networks; analyzing complexity using a proposed approximation algorithm MDA in permutation flow shop scheduling environment; empirical evaluation of threshold and time constraint algorithm for non-replicated dynamic data allocation in distributed database systems; automated usability evaluation of web applications; an efficient approach for frequent pattern mining method using fuzzy set theory; a green computing approach in cloud data centers; detecting myocardial infarction by multivariate multiscale covariance analysis of multilead electrocardiograms; comparing various classifier techniques for efficient mining of data and fingerprint recognition system by termination points using cascade-forward backpropagation neural network.},
author = {Anantasech, Pichamon and Ratanamahatana, Chotirat Ann},
doi = {10.1007/978-981-13-1165-9},
file = {:Users/andrea/Documents/Papers/anantasech2019.pdf:pdf},
isbn = {978-981-13-1164-2},
keywords = {Adaptive weight,Dynamic time warping,Time series classification,Weighted dynamic time warping,adaptive weight,dynamic time warping,time series classification,weighted dynamic time warping},
pages = {655--664},
publisher = {Springer Singapore},
title = {{Enhanced weighted dynamic time warping for time series classification}},
url = {http://link.springer.com/10.1007/978-981-13-1165-9},
volume = {797},
year = {2019}
}
@article{Shivade2014,
abstract = {Objective To summarize literature describing approaches aimed at automatically identifying patients with a common phenotype. Materials and methods We performed a review of studies describing systems or reporting techniques developed for identifying cohorts of patients with specific phenotypes. Every full text article published in (1) Journal of American Medical Informatics Association, (2) Journal of Biomedical Informatics, (3) Proceedings of the Annual American Medical Informatics Association Symposium, and (4) Proceedings of Clinical Research Informatics Conference within the past 3years was assessed for inclusion in the review. Only articles using automated techniques were included. Results Ninety-seven articles met our inclusion criteria. Forty-six used natural language processing (NLP)-based techniques, 24 described rule-based systems, 41 used statistical analyses, data mining, or machine learning techniques, while 22 described hybrid systems. Nine articles described the architecture of large-scale systems developed for determining cohort eligibility of patients. Discussion We observe that there is a rise in the number of studies associated with cohort identification using electronic medical records. Statistical analyses or machine learning, followed by NLP techniques, are gaining popularity over the years in comparison with rule-based systems. Conclusions There are a variety of approaches for classifying patients into a particular phenotype. Different techniques and data sources are used, and good performance is reported on datasets at respective institutions. However, no system makes comprehensive use of electronic medical records addressing all of their known weaknesses.},
author = {Shivade, Chaitanya and Raghavan, Preethi and Fosler-Lussier, Eric and Embi, Peter J. and Elhadad, Noemie and Johnson, Stephen B. and Lai, Albert M.},
doi = {10.1136/amiajnl-2013-001935},
file = {:Users/andrea/Documents/Papers/shivade2013.pdf:pdf},
isbn = {1067-5027},
issn = {10675027},
journal = {Journal of the American Medical Informatics Association},
number = {2},
pages = {221--230},
pmid = {24201027},
title = {{A review of approaches to identifying patient phenotype cohorts using electronic health records}},
volume = {21},
year = {2014}
}
@article{Racine2000,
abstract = {{{This paper considers the impact of Shao's (1993) recent results regarding the asymp- totic inconsistency of model selection via leave-one-out cross-validation on h-block cross-validation, a cross-validatory method for dependent data proposed by Burman, Chow and Nolan (1994, Journal of Time Series Analysis 13, 189{\}}207). It is shown that h-block cross-validation is inconsistent in the sense of Shao (1993, Journal of American Statistical Association 88(422), 486{\}}495) and therefore is not asymptotically optimal. A modi"cation of the h-block method, dubbed {\&}hv-block' cross-validation, is proposed which is asymptotically optimal. The proposed approach is consistent for general stationary observations in the sense that the probability of selecting the model with the best predictive ability converges to 1 as the total number of observations approaches in"nity. This extends existing results and yields a new approach which contains leave- one-out cross-validation, leave-n?-out cross-validation, and h-block cross-validation as special cases. Applications are considered. ? 2000 Elsevier Science S.A. All rights reserved.},
annote = {hv-block CV, asymptotically optimal

consistent for stationary observations, probability of selecting the model with the best predictive ability converges to 1 as n grows large},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Racine, Jeff},
doi = {10.1016/S0304-4076(00)00030-0},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Cross-validation,Dependence,Model-selection,Prediction},
pages = {39--61},
pmid = {25246403},
title = {{Consistent cross-validatory model-selection for dependent data: hv-block cross-validation}},
volume = {99},
year = {2000}
}
@article{Zou2006,
abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection. The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.},
archivePrefix = {arXiv},
arxivId = {NIHMS201118},
author = {Zou, Hui},
doi = {10.1198/016214506000000735},
eprint = {NIHMS201118},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {476},
pages = {1418--1429},
pmid = {20122298},
title = {{The Adaptive Lasso and Its Oracle Properties}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000000735},
volume = {101},
year = {2006}
}
@article{Brisimi2018,
abstract = {Background: In an era of “big data,” computationally efficient and privacy-aware solutions for large-scale machine learning problems become crucial, especially in the healthcare domain, where large amounts of data are stored in different locations and owned by different entities. Past research has been focused on centralized algorithms, which assume the existence of a central data repository (database) which stores and can process the data from all participants. Such an architecture, however, can be impractical when data are not centrally located, it does not scale well to very large datasets, and introduces single-point of failure risks which could compromise the integrity and privacy of the data. Given scores of data widely spread across hospitals/individuals, a decentralized computationally scalable methodology is very much in need. Objective: We aim at solving a binary supervised classification problem to predict hospitalizations for cardiac events using a distributed algorithm. We seek to develop a general decentralized optimization framework enabling multiple data holders to collaborate and converge to a common predictive model, without explicitly exchanging raw data. Methods: We focus on the soft-margin l 1 -regularized sparse Support Vector Machine (sSVM) classifier. We develop an iterative cluster Primal Dual Splitting (cPDS) algorithm for solving the large-scale sSVM problem in a decentralized fashion. Such a distributed learning scheme is relevant for multi-institutional collaborations or peer-to-peer applications, allowing the data holders to collaborate, while keeping every participant's data private. Results: We test cPDS on the problem of predicting hospitalizations due to heart diseases within a calendar year based on information in the patients Electronic Health Records prior to that year. cPDS converges faster than centralized methods at the cost of some communication between agents. It also converges faster and with less communication overhead compared to an alternative distributed algorithm. In both cases, it achieves similar prediction accuracy measured by the Area Under the Receiver Operating Characteristic Curve (AUC) of the classifier. We extract important features discovered by the algorithm that are predictive of future hospitalizations, thus providing a way to interpret the classification results and inform prevention efforts.},
author = {Brisimi, Theodora S. and Chen, Ruidi and Mela, Theofanie and Olshevsky, Alex and Paschalidis, Ioannis Ch and Shi, Wei},
doi = {10.1016/j.ijmedinf.2018.01.007},
file = {:Users/andrea/Documents/Papers/brisimi2018.pdf:pdf},
issn = {18728243},
journal = {International Journal of Medical Informatics},
keywords = {Distributed learning,Electronic Health Records (EHRs),Federated databases,Heart diseases,Hospitalization,Predictive models},
number = {November 2017},
pages = {59--67},
publisher = {Elsevier},
title = {{Federated learning of predictive models from federated Electronic Health Records}},
url = {https://doi.org/10.1016/j.ijmedinf.2018.01.007},
volume = {112},
year = {2018}
}
@article{Fiterau2017,
abstract = {In healthcare applications, temporal variables that encode movement, health status and longitudinal patient evolution are often accompanied by rich structured information such as demographics, diagnostics and medical exam data. However, current methods do not jointly optimize over structured covariates and time series in the feature extraction process. We present ShortFuse, a method that boosts the accuracy of deep learning models for time series by explicitly modeling temporal interactions and dependencies with structured covariates. ShortFuse introduces hybrid convolutional and LSTM cells that incorporate the covariates via weights that are shared across the temporal domain. ShortFuse outperforms competing models by 3{\%} on two biomedical applications, forecasting osteoarthritis-related cartilage degeneration and predicting surgical outcomes for cerebral palsy patients, matching or exceeding the accuracy of models that use features engineered by domain experts.},
archivePrefix = {arXiv},
arxivId = {1705.04790},
author = {Fiterau, Madalina and Bhooshan, Suvrat and Fries, Jason and Bournhonesque, Charles and Hicks, Jennifer and Halilaj, Eni and R{\'{e}}, Christopher and Delp, Scott},
eprint = {1705.04790},
file = {:Users/andrea/Documents/Papers/Fiterau2017.pdf:pdf},
pages = {1--15},
title = {{ShortFuse: Biomedical Time Series Representations in the Presence of Structured Information}},
url = {http://arxiv.org/abs/1705.04790},
year = {2017}
}
@article{Giorgino2009,
abstract = {Dynamic time warping is a popular technique for comparing time series, providing both a distance measure that is insensitive to local compression and stretches and the warping which optimally deforms one of the two input series onto the other. A variety of algorithms and constraints have been discussed in the literature. The dtw package provides an unification of them; it allows R users to compute time series alignments mixing freely a variety of continuity constraints, restriction windows, endpoints, local distance definitions, and so on. The package also provides functions for visualizing alignments and constraints using several classic diagram types.},
author = {Giorgino, Toni},
doi = {10.18637/jss.v031.i07},
file = {:Users/andrea/Documents/Papers/giorgino2009.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {alignment,dynamic programming,dynamic time warping,timeseries},
number = {7},
title = {{Computing and Visualizing Dynamic Time Warping Alignments in R : The dtw Package}},
volume = {31},
year = {2009}
}
@article{Yuan2006,
abstract = {Summary. We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
author = {Yuan, Ming and Lin, Yi},
doi = {10.1111/j.1467-9868.2005.00532.x},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Analysis of variance,Lasso,Least angle regression,Non-negative garrotte,Piecewise linear solution path},
number = {1},
pages = {49--67},
pmid = {11161800},
title = {{Model selection and estimation in regression with grouped variables}},
volume = {68},
year = {2006}
}
@article{achille2018,
abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
archivePrefix = {arXiv},
arxivId = {1706.01350},
author = {Achille, Alessandro and Soatto, Stefano},
doi = {10.1109/ITA.2018.8503149},
eprint = {1706.01350},
file = {:Users/andrea/Documents/Papers/achille2018.pdf:pdf},
isbn = {9781728101248},
issn = {1590-8577},
journal = {2018 Information Theory and Applications Workshop, ITA 2018},
keywords = {Deep learning,Flat minima,Generalization,Information bottleneck,Information complexity,Minimality,Neural network,Overfitting,PAC-Bayes,Regularization,Representation,Sensitivity,Stochastic gradient descent,Sufficiency,Total correlation},
pages = {1--34},
pmid = {17625293},
title = {{Emergence of invariance and disentanglement in deep representations}},
volume = {19},
year = {2018}
}
@article{Zheng2017a,
abstract = {Objective To discover diverse genotype-phenotype associations affiliated with Type 2 Diabetes Mellitus (T2DM) via genome-wide association study (GWAS) and phenome-wide association study (PheWAS), more cases (T2DM subjects) and controls (subjects without T2DM) are required to be identified (e.g., via Electronic Health Records (EHR)). However, existing expert based identification algorithms often suffer in a low recall rate and could miss a large number of valuable samples under conservative filtering standards. The goal of this work is to develop a semi-automated framework based on machine learning as a pilot study to liberalize filtering criteria to improve recall rate with a keeping of low false positive rate. Materials and methods We propose a data informed framework for identifying subjects with and without T2DM from EHR via feature engineering and machine learning. We evaluate and contrast the identification performance of widely-used machine learning models within our framework, including k-Nearest-Neighbors, Na{\"{i}}ve Bayes, Decision Tree, Random Forest, Support Vector Machine and Logistic Regression. Our framework was conducted on 300 patient samples (161 cases, 60 controls and 79 unconfirmed subjects), randomly selected from 23,281 diabetes related cohort retrieved from a regional distributed EHR repository ranging from 2012 to 2014. Results We apply top-performing machine learning algorithms on the engineered features. We benchmark and contrast the accuracy, precision, AUC, sensitivity and specificity of classification models against the state-of-the-art expert algorithm for identification of T2DM subjects. Our results indicate that the framework achieved high identification performances (∼0.98 in average AUC), which are much higher than the state-of-the-art algorithm (0.71 in AUC). Discussion Expert algorithm-based identification of T2DM subjects from EHR is often hampered by the high missing rates due to their conservative selection criteria. Our framework leverages machine learning and feature engineering to loosen such selection criteria to achieve a high identification rate of cases and controls. Conclusions Our proposed framework demonstrates a more accurate and efficient approach for identifying subjects with and without T2DM from EHR.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Zheng, Tao and Xie, Wei and Xu, Liling and He, Xiaoying and Zhang, Ya and You, Mingrong and Yang, Gong and Chen, You},
doi = {10.1016/j.ijmedinf.2016.09.014},
eprint = {15334406},
file = {:Users/andrea/Documents/Papers/Zheng2016.pdf:pdf},
isbn = {9783319626970},
issn = {18728243},
journal = {International Journal of Medical Informatics},
keywords = {Data mining,Electronic health records,Feature engineering,Machine learning,Type 2 diabetes},
pages = {120--127},
pmid = {27919371},
publisher = {Elsevier Ireland Ltd},
title = {{A machine learning-based framework to identify type 2 diabetes through electronic health records}},
url = {http://dx.doi.org/10.1016/j.ijmedinf.2016.09.014},
volume = {97},
year = {2017}
}
@article{Bao2017,
abstract = {The application of deep learning approaches to finance has received a great deal of attention from both investors and researchers. This study presents a novel deep learning framework where wavelet transforms (WT), stacked autoencoders (SAEs) and long-short term memory (LSTM) are combined for stock price forecasting. The SAEs for hierarchically extracted deep features is introduced into stock price forecasting for the first time. The deep learning framework comprises three stages. First, the stock price time series is decomposed by WT to eliminate noise. Second, SAEs is applied to generate deep high-level features for predicting the stock price. Third, high-level denoising features are fed into LSTM to forecast the next day's closing price. Six market indices and their corresponding index futures are chosen to examine the performance of the proposed model. Results show that the proposed model outperforms other similar models in both predictive accuracy and profitability performance.},
author = {Bao, Wei and Yue, Jun and Rao, Yulei},
doi = {10.1371/journal.pone.0180944},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pmid = {28708865},
title = {{A deep learning framework for financial time series using stacked autoencoders and long-short term memory}},
volume = {12},
year = {2017}
}
@article{baltrusaitis2017,
abstract = {Objectives Our aim was to assess midterm safety and clinical outcomes of intracoronary infusion of cardiosphere-derived cells (CDCs) after staged palliation in patients with hypoplastic left heart syndrome (HLHS). Methods In this prospective, controlled study, 14 consecutive patients with HLHS who were undergoing 2- or 3-stage surgical palliations were assigned to receive intracoronary CDC infusion 1 month after cardiac surgery (n = 7), followed by 7 patients allocated to a control group with standard care alone. The primary end point was to assess procedural feasibility and safety; the secondary end point was to evaluate cardiac function and heart failure status through 36-month follow-up. Results No complications, including tumor formation, were reported within 36 months after CDC infusion. Echocardiography showed significantly greater improvement in right ventricular ejection fraction (RVEF) in infants receiving CDCs than in controls at 36 months (+8.0{\%} ± 4.7{\%} vs +2.2{\%} ± 4.3{\%}; P =.03). These cardiac function improvements resulted in reduced brain natriuretic peptide levels (P =.04), lower incidence of unplanned catheter interventions (P =.04), and higher weight-for-age z score (P =.02) at 36 months relative to controls. As independent predictors of treatment responsiveness, absolute changes in RVEF at 36 months were negatively correlated with age, weight-for-age z score, and RVEF at CDC infusion. Conclusions Intracoronary CDC infusion after staged procedure in patients with HLHS is safe and improves RVEF, which persists during 36-month follow-up. This therapeutic strategy may enhance somatic growth and reduce incidence of heart failure.},
archivePrefix = {arXiv},
arxivId = {1705.09406v2},
author = {Baltrusaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
doi = {10.1016/j.jtcvs.2015.06.076},
eprint = {1705.09406v2},
file = {:Users/andrea/Documents/Papers/Baltrusaitis2017.pdf:pdf},
isbn = {0022-5223},
issn = {1097685X},
journal = {Journal of Thoracic and Cardiovascular Surgery},
keywords = {cardiosphere,congenital heart disease,heart failure,hypoplastic left heart syndrome,stem cell therapy},
number = {5},
pages = {1198--1207},
pmid = {26232942},
title = {{Multimodal Machine Learning: a survey and taxonomy}},
volume = {150},
year = {2015}
}
@article{Arlot2010,
abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplic-ity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
annote = {(very general, small part of time dependency. good if intersted in CV in general context)},
archivePrefix = {arXiv},
arxivId = {0907.4728},
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
eprint = {0907.4728},
isbn = {1935-7516},
issn = {1935-7516},
journal = {Statistics Surveys},
number = {0},
pages = {40--79},
pmid = {1000183221},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Caruana2015,
author = {Caruana, Rich and Koch, Paul and Lou, Yin and Gehrke, Johannes and Sturm, Marc},
file = {:Users/andrea/Documents/Papers/Caruana2015.pdf:pdf},
isbn = {9781450336642},
keywords = {additive,classification,healthcare,intelligibility,interaction detection,logistic regression,models,risk prediction},
pages = {1721--1730},
title = {{Intelligible Models for HealthCare : Predicting Pneumonia Risk and Hospital 30-day Readmission}},
url = {http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf}
}
@article{Sitaram2015,
author = {Sitaram, Dinkar and Dalwani, Aditya and Narang, Anish and Das, Madhura and Auradkar, Prafullata},
doi = {10.1109/ICACCE.2015.14},
file = {:Users/andrea/Documents/Papers/sitaram2015.pdf:pdf},
isbn = {9781479917341},
journal = {Proceedings - 2015 2nd IEEE International Conference on Advances in Computing and Communication Engineering, ICACCE 2015},
keywords = {Mahalanobis Distance,Missing Values,Similarity Measure,Time Series},
pages = {622--627},
publisher = {IEEE},
title = {{A Measure of Similarity of Time Series Containing Missing Data Using the Mahalanobis Distance}},
year = {2015}
}
@article{Lehman2015,
author = {Lehman, Li-wei H and Adams, Ryan P and Mayaud, Louis and Moody, George B and Malhotra, Atul and Mark, Roger G},
doi = {10.1109/JBHI.2014.2330827},
file = {:Users/andrea/Documents/Papers/Lehman2015.pdf:pdf},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {MIMIC},
mendeley-tags = {MIMIC},
number = {3},
pages = {1068--1076},
publisher = {IEEE},
title = {{A Physiological Time Series Dynamics-Based Approach to Patient Monitoring and Outcome Prediction}},
volume = {19},
year = {2015}
}
@article{Chen2012,
author = {Chen, Gong and Guo, Li-Mei and Ren, Chang-Zhong and Guo, Lai-Chun and Zhao, Guo-Jun and Hu, Yue-Gao and Zeng, Zhao-Hai},
doi = {10.3724/SP.J.1006.2011.02066},
file = {::},
issn = {0496-3490},
journal = {Acta Agronomica Sinica},
keywords = {common vetch,intercropping,oat,row space,yield},
month = {jan},
number = {11},
pages = {2066--2074},
title = {{Effects of Two Row Spaces and Intercropping on Forage and Crude Protein Yields of Oat ({\textless}I{\textgreater}Avena sativa{\textless}/I{\textgreater} L.) and Common Vetch ({\textless}I{\textgreater}Vicia sativa{\textless}/I{\textgreater} L.)}},
url = {http://pub.chinasciencejournal.com/article/getArticleRedirect.action?doiCode=10.3724/SP.J.1006.2011.02066},
volume = {37},
year = {2012}
}
@article{Gurden2001c,
abstract = {Recent years have seen an increase in the number of regression problems for which the predictor and/or response arrays have orders higher than two, i.e. multiway data. Examples are found in, e.g. industrial batch process analysis, chemical calibration using second-order instrumentation and quantitative structure–activity relationships (QSAR). As these types of problems increase in complexity in terms of both the dimensions and the underlying structures of the data sets, the number of options with respect to different types of scaling and regression models also increases. In this article, three methods for multiway regression are compared: unfold partial least squares (PLS), multilinear PLS and multiway covariates regression (MCovR). All three methods differ either in the structural model imposed on the data or the way the model components are calculated. Three methods of scaling multiway arrays are also compared, along with the option of applying no scaling. Three data sets drawn from industrial processes are used in the comparison. The general conclusion is that the type of data and scaling used is more important than the type of regression model used in terms of predictive ability. The models do differ, however, in terms of interpretability.},
author = {Gurden, Stephen P. and Westerhuis, Johan A. and Bro, Rasmus and Smilde, Age K.},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {chemometrics,multilinear,multiway,n-pls,predictive{\_}ability},
mendeley-tags = {chemometrics,multilinear,multiway,n-pls,predictive{\_}ability},
month = {nov},
number = {1-2},
pages = {121--136},
title = {{A comparison of multiway regression and scaling methods}},
type = {Journal article},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0169-7439(01)00168-X},
volume = {59},
year = {2001}
}
@article{Warton2011a,
author = {Warton, D.I. and Hui, F.K.C.},
file = {::},
journal = {Ecology},
keywords = {arcsine transformation,binomial,generalized linear mixed models,logistic regression,logit,overdispersion,power,transformation,type i error},
number = {1},
pages = {3--10},
publisher = {Eco Soc America},
title = {{The arcsine is asinine: the analysis of proportions in ecology}},
url = {http://www.esajournals.org/doi/abs/10.1890/10-0340.1},
volume = {92},
year = {2011}
}
@article{Stiegler2009a,
author = {Stigler, S},
issn = {0933-2480},
journal = {Chance},
month = {feb},
number = {4},
pages = {12--12},
title = {{Fisher and the 5{\%} level}},
url = {http://www.springerlink.com/index/10.1007/s00144-008-0033-3 http://www.springerlink.com/index/P546581236KW3G67.pdf},
volume = {21},
year = {2008}
}
@article{Wild2017,
abstract = {BACKGROUND Understanding the genetic architecture of cardiac structure and function may help to prevent and treat heart disease. This investigation sought to identify common genetic variations associated with inter-individual variability in cardiac structure and function. METHODS A GWAS meta-analysis of echocardiographic traits was performed, including 46,533 individuals from 30 studies (EchoGen consortium). The analysis included 16 traits of left ventricular (LV) structure, and systolic and diastolic function. RESULTS The discovery analysis included 21 cohorts for structural and systolic function traits (n = 32,212) and 17 cohorts for diastolic function traits (n = 21,852). Replication was performed in 5 cohorts (n = 14,321) and 6 cohorts (n = 16,308), respectively. Besides 5 previously reported loci, the combined meta-analysis identified 10 additional genome-wide significant SNPs: rs12541595 near MTSS1 and rs10774625 in ATXN2 for LV end-diastolic internal dimension; rs806322 near KCNRG, rs4765663 in CACNA1C, rs6702619 near PALMD, rs7127129 in TMEM16A, rs11207426 near FGGY, rs17608766 in GOSR2, and rs17696696 in CFDP1 for aortic root diameter; and rs12440869 in IQCH for Doppler transmitral A-wave peak velocity. Findings were in part validated in other cohorts and in GWAS of related disease traits. The genetic loci showed associations with putative signaling pathways, and with gene expression in whole blood, monocytes, and myocardial tissue. CONCLUSION The additional genetic loci identified in this large meta-analysis of cardiac structure and function provide insights into the underlying genetic architecture of cardiac structure and warrant follow-up in future functional studies. FUNDING For detailed information per study, see Acknowledgments.},
author = {Wild, Philipp S. and Felix, Janine F. and Schillert, Arne and Teumer, Alexander and Chen, Ming Huei and Leening, Maarten J.G. and V{\"{o}}lker, Uwe and Gro{\ss}mann, Vera and Brody, Jennifer A. and Irvin, Marguerite R. and Shah, Sanjiv J. and Pramana, Setia and Lieb, Wolfgang and Schmidt, Reinhold and Stanton, Alice V. and Malzahn, D{\"{o}}rthe and Smith, Albert Vernon and Sundstr{\"{o}}m, Johan and Minelli, Cosetta and Ruggiero, Daniela and Lyytik{\"{a}}inen, Leo Pekka and Tiller, Daniel and Smith, J. Gustav and Monnereau, Claire and {Di Tullio}, Marco R. and Musani, Solomon K. and Morrison, Alanna C. and Pers, Tune H. and Morley, Michael and Kleber, Marcus E. and Aragam, Jayashri and Benjamin, Emelia J. and Bis, Joshua C. and Bisping, Egbert and Broeckel, Ulrich and Cheng, Susan and Deckers, Jaap W. and {Del Greco M}, Fabiola and Edelmann, Frank and Fornage, Myriam and Franke, Lude and Friedrich, Nele and Harris, Tamara B. and Hofer, Edith and Hofman, Albert and Huang, Jie and Hughes, Alun D. and K{\"{a}}h{\"{o}}nen, Mika and Kruppa, Jochen and Lackner, Karl J. and Lannfelt, Lars and Laskowski, Rafael and Launer, Lenore J. and Leosdottir, Margr{\'{e}}t and Lin, Honghuang and Lindgren, Cecilia M. and Loley, Christina and MacRae, Calum A. and Mascalzoni, Deborah and Mayet, Jamil and Medenwald, Daniel and Morris, Andrew P. and M{\"{u}}ller, Christian and M{\"{u}}ller-Nurasyid, Martina and Nappo, Stefania and Nilsson, Peter M. and Nuding, Sebastian and Nutile, Teresa and Peters, Annette and Pfeufer, Arne and Pietzner, Diana and Pramstaller, Peter P. and Raitakari, Olli T. and Rice, Kenneth M. and Rivadeneira, Fernando and Rotter, Jerome I. and Ruohonen, Saku T. and Sacco, Ralph L. and Samdarshi, Tandaw E. and Schmidt, Helena and Sharp, Andrew S.P. and Shields, Denis C. and Sorice, Rossella and Sotoodehnia, Nona and Stricker, Bruno H. and Surendran, Praveen and Thom, Simon and T{\"{o}}glhofer, Anna M. and Uitterlinden, Andr{\'{e}} G. and Wachter, Rolf and V{\"{o}}lzke, Henry and Ziegler, Andreas and M{\"{u}}nzel, Thomas and M{\"{a}}rz, Winfried and Cappola, Thomas P. and Hirschhorn, Joel N. and Mitchell, Gary F. and Smith, Nicholas L. and Fox, Ervin R. and Dueker, Nicole D. and Jaddoe, Vincent W.V. and Melander, Olle and Russ, Martin and Lehtim{\"{a}}ki, Terho and Ciullo, Marina and Hicks, Andrew A. and Lind, Lars and Gudnason, Vilmundur and Pieske, Burkert and Barron, Anthony J. and Zweiker, Robert and Schunkert, Heribert and Ingelsson, Erik and Liu, Kiang and Arnett, Donna K. and Psaty, Bruce M. and Blankenberg, Stefan and Larson, Martin G. and Felix, Stephan B. and Franco, Oscar H. and Zeller, Tanja and Vasan, Ramachandran S. and D{\"{o}}rr, Marcus},
doi = {10.1172/JCI84840},
isbn = {0021-9738},
issn = {15588238},
journal = {Journal of Clinical Investigation},
pmid = {28394258},
title = {{Large-scale genome-wide analysis identifies genetic variants associated with cardiac structure and function}},
year = {2017}
}
@techreport{CenterforWorkforceStudies2018a,
author = {{Center for Workforce Studies}},
number = {April},
pages = {1--252},
title = {{Physician Supply and Demand Through 2030: Key Findings}},
year = {2018}
}
@article{Harapan2017,
abstract = {Vaccination strategies are being considered as a part of dengue prevention programs in endemic countries. To accelerate the introduction of dengue vaccine into the public sector program and private markets, understanding the private economic benefits of a dengue vaccine is therefore essential. The aim of this study was to assess the willingness to pay (WTP) for a dengue vaccine among community members in Indonesia and its associated explanatory variables. A community-based, cross-sectional survey was conducted in nine regencies of Aceh province, Indonesia, from November 2014 to March 2015. A pre-tested validated questionnaire was used to facilitate the interviews. To assess the explanatory variables influencing participants' WTP for a dengue vaccine, a linear regression analysis was employed. We interviewed 677 healthy community members; 476 participants (87.5{\%} of the total) were included in the final analysis. An average individual was willing to pay around US-{\$} 4 (mean: US-{\$} 4.04; median: US-{\$} 3.97) for a dengue vaccine. Our final multivariate model revealed that working as a civil servant, living in the city, and having good knowledge on dengue viruses, a good attitude towards dengue, and good preventive practice against dengue virus infection were associated with a higher WTP (P {\textless} 0.05). Our model suggests that marketing efforts should be directed to community members who are working in the suburbs especially as farmers. In addition, the results of our study underscore the need for low-cost quality vaccines, public sector subsidies for vaccinations, and intensifying efforts to further educate and encourage households regarding other dengue preventive measures, using trusted individuals as facilitators.},
author = {Harapan, Harapan and Anwar, Samsul and Bustamam, Aslam and Radiansyah, Arsil and Angraini, Pradiba and Fasli, Riny and Salwiyadi, Salwiyadi and Bastian, Reza Akbar and Oktiviyari, Ade and Akmal, Imaduddin and Iqbalamin, Muhammad and Adil, Jamalul and Henrizal, Fenni and Darmayanti, Darmayanti and Mahmuda, Mahmuda and Mudatsir, Mudatsir and Imrie, Allison and Sasmono, R. Tedjo and Kuch, Ulrich and Shkedy, Ziv and Pramana, Setia},
doi = {10.1016/j.actatropica.2016.11.035},
isbn = {0001-706X},
issn = {18736254},
journal = {Acta Tropica},
keywords = {Dengue fever,Dengue vaccine,Indonesia,Vaccine demand,Vaccine introduction,Willingness to pay},
pmid = {27908746},
title = {{Willingness to pay for a dengue vaccine and its associated determinants in Indonesia: A community-based, cross-sectional survey in Aceh}},
year = {2017}
}
@article{ThomasR.Knapp1990,
annote = {{\textless}m:note{\textgreater}undefined{\textless}/m:note{\textgreater}},
author = {{Thomas R. Knapp}},
journal = {Nursing Research},
keywords = {Measurement{\_}level,admissible{\_}statistics,measurement-statistics{\_}debate},
mendeley-tags = {Measurement{\_}level,admissible{\_}statistics,measurement-statistics{\_}debate},
number = {2},
pages = {121--123},
title = {{Treating Ordinal Scales as Interval Scales: An Attempt To Resolve the Controversy}},
url = {http://www.mat.ufrgs.br/{~}viali/estatistica/mat2282/material/textos/treating{\_}ordinal{\_}scales[1].pdf},
volume = {39},
year = {1990}
}
@article{Liu2007,
abstract = {The main oligomeric procyanidins from litchi pericarp (Litchi chinensis) were isolated and identified. Complete assignment of epicatechin-(4?????8, 2?????O???7)-epicatechin-(4?????8)-epicatechin structure was obtained using CD, NMR and ESI-MS techniques. The known (-)-epicatechin and procyanidin A2 were also isolated from litchi pericarp. The effects of oligomeric procyanidins, A2 and epicatechin-(4?????8, 2?????O???7)-epicatechin-(4?????8)-epicatechin from litchi pericarp on free radical-scavenging were determined by using a chemiluminescent method. The results showed that all of them had a strong scavenging effect on {\{}radical dot{\}}OH, and the IC50 values were 2.60 ??g/ml, 1.75 ??g/ml and 1.65 ??g/ml for oligomeric procyanidins, A2 and the trimeric procyanidins, respectively. The antioxidant activities of A-type dimeric and trimeric procyanidins seemed to be related to the number of hydroxyls in their molecular structures. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Liu, Liang and Xie, Bijun and Cao, Shaoqian and Yang, Erning and Xu, Xiaoyun and Guo, Shanshan},
doi = {10.1016/j.foodchem.2007.05.022},
issn = {03088146},
journal = {Food Chemistry},
keywords = {Antioxidant activity,Identification,Litchi,Procyanidins},
number = {4},
pages = {1446--1451},
title = {{A-type procyanidins from Litchi chinensis pericarp with antioxidant activity}},
volume = {105},
year = {2007}
}
@article{Magnusson2016,
abstract = {{\textless}p{\textgreater} Monozygotic (MZ) twins stem from the same single fertilized egg and therefore share all their {\textless}italic{\textgreater}inherited{\textless}/italic{\textgreater} genetic variation. This is one of the unequivocal facts on which genetic epidemiology and twin studies are based. To what extent this also implies that MZ twins share genotypes in adult tissues is not precisely established, but a common pragmatic assumption is that MZ twins are 100{\%} genetically identical also in adult tissues. During the past decade, this view has been challenged by several reports, with observations of differences in post-zygotic copy number variations (CNVs) between members of the same MZ pair. In this study, we performed a systematic search for differences of CNVs within 38 adult MZ pairs who had been misclassified as dizygotic (DZ) twins by questionnaire-based assessment. Initial scoring by PennCNV suggested a total of 967 CNV discordances. The within-pair correlation in number of CNVs detected was strongly dependent on confidence score filtering and reached a plateau of {\textless}italic{\textgreater}r{\textless}/italic{\textgreater} = 0.8 when restricting to CNVs detected with confidence score larger than 50. The top-ranked discordances were subsequently selected for validation by quantitative polymerase chain reaction (qPCR), from which one single {\~{}}120kb deletion in {\textless}italic{\textgreater}NRXN1{\textless}/italic{\textgreater} on chromosome 2 (bp 51017111–51136802) was validated. Despite involving an exon, no sign of cognitive/mental consequences was apparent in the affected twin pair, potentially reflecting limited or lack of expression of the transcripts containing this exon in nerve/brain. {\textless}/p{\textgreater}},
author = {Magnusson, Patrik K.E. and Lee, Donghwan and Chen, Xu and Szatkiewicz, Jin and Pramana, Setia and Teo, Shumei and Sullivan, Patrick F. and Feuk, Lars and Pawitan, Yudi},
doi = {10.1017/thg.2016.5},
issn = {18392628},
journal = {Twin Research and Human Genetics},
keywords = {NRXN1,PennCNV,copy number variation,monozygotic twins,quantitative polymerase chain reaction},
title = {{One CNV Discordance in NRXN1 Observed Upon Genome-wide Screening in 38 Pairs of Adult Healthy Monozygotic Twins}},
year = {2016}
}
@article{Tauler2002,
author = {Tauler, Roma},
issn = {08869383},
journal = {Journal of Chemometrics},
number = {2},
pages = {117--118},
title = {{Multivariate Data Analysis-In Pactice. An Introduction to multivariate data analysis and experimental design (4th edn), Kim H. Esbensen, CAMO, OSLO, 2000, ISBN 82-9933302-4, xviii + 600pp, US230.00.}},
url = {http://doi.wiley.com/10.1002/cem.692},
volume = {16},
year = {2002}
}
@article{Chrisman1998a,
abstract = {Stevens' measurement levels (nominal, ordinal, interval and ratio) have become a familiar part of cartography and GIS. These levels have been accepted unquestioned from publications in social sciences dating from the 1940s and 1950s. The Stevens taxonomy has been used to prescribe appropriate symbolism (or analytical treatment) to each scale of measurement. This paper reviews the process by which these levels became a part of cartography, as well as subsequent literature that cartographers have all but ignored over the intervening four decades. The paper concludes that the four levels of measurement are not adequate to cover the circumstances of cartography, and that attribute issues alone do not provide a sufficient guide to symbolism or analytical treatment. A broader framework for measurement must be considered, including the relationships of control that constrain variation in one component to permit measurement of another. An informed use of tools does not depend on numbers alone, but on the whole “measurement framework,” the system of objects, relationships and axioms implied by a given system of representation.},
author = {Chrisman, Nicholas R.},
issn = {15230406},
journal = {Cartography and Geographic Information Science},
month = {oct},
number = {4},
pages = {231--242},
publisher = {Cartography and Geographic Information Society},
title = {{Rethinking Levels of Measurement for Cartography}},
url = {http://www.ingentaconnect.com/content/acsm/cagis/1998/00000025/00000004/art00005},
volume = {25},
year = {1998}
}
@article{Hawkins2004,
author = {Hawkins, Douglas M},
file = {::},
issn = {00952338},
journal = {Journal of Chemical Information and Computer Sciences},
number = {1},
pages = {1--12},
publisher = {American Chemical Society},
title = {{The Problem of Overfitting}},
url = {http://dx.doi.org/10.1021/ci0342472},
volume = {44},
year = {2004}
}
@article{Rakeih2010,
author = {Rakeih, Nazeih and Kayyal, Hamed and Larbi, Asamoah and Habib, Nabil},
file = {::},
keywords = {aggressivity,barley,land equivalent ratio,mixture,relative crowding coefficient,triticale},
number = {2},
pages = {194--207},
title = {{Forage Yield and Competition Indices of Triticale and Barley Mixed Intercropping with Common Vetch and Grasspea in the Mediterranean Region}},
volume = {6},
year = {2010}
}
@article{Grzeskowiak2014,
abstract = {Natural selection imposed by pathogens is a strong and pervasive evolutionary force structuring genetic diversity within their hosts' genomes and populations. As a model system for understanding the genomic impact of host-parasite coevolution, we have been studying the evolutionary dynamics of disease resistance genes in wild relatives of the cultivated tomato species. In this study, we investigated the sequence variation and evolutionary history of three linked genes involved in pathogen resistance in populations of Solanum peruvianum (Pto, Fen, and Prf). These genes encode proteins, which form a multimeric complex and together activate defense responses. We used standard linkage disequilibrium, as well as partitioning of linkage disequilibrium components across populations and correlated substitution analysis to identify amino acid positions that are candidates for coevolving sites between Pto/Fen and Prf. These candidates were mapped onto known and predicted structures of Pto, Fen and Prf to visualize putative coevolving regions between proteins. We discuss the functional significance of these coevolving pairs in the context of what is known from previous structure-function studies of Pto, Fen and Prf.},
annote = {NULL},
author = {Grzeskowiak, Lukasz and Stephan, Wolfgang and Rose, Laura E},
doi = {10.1016/j.meegid.2014.06.019},
file = {::},
issn = {1567-7257},
journal = {Infection, genetics and evolution : journal of molecular epidemiology and evolutionary genetics in infectious diseases},
keywords = {Coadaptation,Crop improvement,Disease resistance,Host–parasite interactions,Pto,Solanum peruvianum},
month = {jul},
pmid = {24997333},
title = {{Epistatic selection and coadaptation in the Prf resistance complex of wild tomato.}},
url = {http://www.sciencedirect.com/science/article/pii/S1567134814002226},
year = {2014}
}
@article{Peng2014a,
abstract = {BACKGROUND Predicting the prognosis of prostate cancer disease through gene expression analysis is receiving increasing interest. In many cases, such analyses are based on formalin-fixed, paraffin embedded (FFPE) core needle biopsy material on which Gleason grading for diagnosis has been conducted. Since each patient typically has multiple biopsy samples, and since Gleason grading is an operator dependent procedure known to be difficult, the impact of the operator's choice of biopsy was evaluated. METHODS Multiple biopsy samples from 43 patients were evaluated using a previously reported gene signature of IGFBP3, F3 and VGLL3 with potential prognostic value in estimating overall survival at diagnosis of prostate cancer. A four multiplex one-step qRT-PCR test kit, designed and optimized for measuring the signature in FFPE core needle biopsy samples was used. Concordance of gene expression levels between primary and secondary Gleason tumor patterns, as well as benign tissue specimens, was analyzed. RESULTS The gene expression levels of IGFBP3 and F3 in prostate cancer epithelial cell-containing tissue representing the primary and secondary Gleason patterns were high and consistent, while the low expressed VGLL3 showed more variation in its expression levels. CONCLUSION The assessment of IGFBP3 and F3 gene expression levels in prostate cancer tissue is independent of Gleason patterns, meaning that the impact of operator's choice of biopsy is low.},
author = {Peng, Zhuochun and Andersson, Karl and Lindholm, Johan and Bodin, Inger and Pramana, Setia and Pawitan, Yudi and Nist{\'{e}}r, Monica and Nilsson, Sten and Li, Chunde},
doi = {10.1371/journal.pone.0109610},
issn = {19326203},
journal = {PLoS ONE},
pmid = {25296164},
title = {{Operator dependent choice of prostate cancer biopsy has limited impact on a gene signature analysis for the highly expressed genes IGFBP3 and F3 in prostate cancer epithelial cells}},
year = {2014}
}
@article{Peng2016,
abstract = {BACKGROUND A previously reported expression signature of three genes (IGFBP3, F3 and VGLL3) was shown to have potential prognostic value in estimating overall and cancer-specific survivals at diagnosis of prostate cancer in a pilot cohort study using freshly frozen Fine Needle Aspiration (FNA) samples. METHODS We carried out a new cohort study with 241 prostate cancer patients diagnosed from 2004-2007 with a follow-up exceeding 6 years in order to verify the prognostic value of gene expression signature in formalin fixed paraffin embedded (FFPE) prostate core needle biopsy tissue samples. The cohort consisted of four patient groups with different survival times and death causes. A four multiplex one-step RT-qPCR test kit, designed and optimized for measuring the expression signature in FFPE core needle biopsy samples, was used. In archive FFPE biopsy samples the expression differences of two genes (IGFBP3 and F3) were measured. The survival time predictions using the current clinical parameters only, such as age at diagnosis, Gleason score, PSA value and tumor stage, and clinical parameters supplemented with the expression levels of IGFBP3 and F3, were compared. RESULTS When combined with currently used clinical parameters, the gene expression levels of IGFBP3 and F3 are improving the prediction of survival time as compared to using clinical parameters alone. CONCLUSION The assessment of IGFBP3 and F3 gene expression levels in FFPE prostate cancer tissue would provide an improved survival prediction for prostate cancer patients at the time of diagnosis.},
author = {Peng, Zhuochun and Andersson, Karl and Lindholm, Johan and Dethlefsen, Olga and Pramana, Setia and Pawitan, Yudi and Nist{\'{e}}r, Monica and Nilsson, Sten and Li, Chunde},
doi = {10.1371/journal.pone.0145545},
issn = {19326203},
journal = {PLoS ONE},
pmid = {26731648},
title = {{Improving the prediction of prostate cancer overall survival by supplementing readily available clinical data with gene expression levels of IGFBP3 and F3 in formalin-fixed paraffin embedded core needle biopsy material}},
year = {2016}
}
@article{Grzeskowiak2013,
abstract = {KEY MESSAGE: In this study, we identified several genes, which potentially contribute to phenological variation in the grapevine. This may help to maintain consistent yield and suitability of particular varieties in future climatic conditions. The timing of major developmental events in fruit crops differs with cultivar, weather conditions and ecological site. This plasticity results also in diverse levels of fruitfulness. Identifying the genetic factors responsible for phenology and fertility variation may help to improve these traits to better match future climates. Two Vitis vinifera populations, an F1 progeny of Syrah × Pinot Noir and a phenological core collection composed of 163 cultivars, were evaluated for phenology and fertility subtraits during three to six growing seasons in the same geographical location. The phenotypic variability in the core collection mostly overlapped with that observed in the F1 progeny and several accessions had exceeding values of phenological response. The progeny population was used together with SSR and SNP markers to map quantitative trait loci (QTLs). This allowed us to detect nine QTLs related to budburst, flowering beginning, the onset of ripening (v{\'{e}}raison) and total fertility, explaining from 8 to 44 {\%} of phenotypic variation. A genomic region on chromosome 15 was associated with budburst and v{\'{e}}raison and two QTLs for fruitfulness were located on chromosomes 3 and 18. Several genes potentially affecting fertility and the timing of fruit development were proposed, based on their position and putative function. Allelic variation at these candidate loci may be explored by sampling accessions from the core collection.},
annote = {NULL},
author = {Grzeskowiak, Lukasz and Costantini, Laura and Lorenzi, Silvia and Grando, M Stella},
doi = {10.1007/s00122-013-2170-1},
issn = {1432-2242},
journal = {TAG. Theoretical and applied genetics. Theoretische und angewandte Genetik},
number = {11},
pages = {2763--76},
pmid = {23918063},
title = {{Candidate loci for phenology and fruitfulness contributing to the phenotypic variability observed in grapevine.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3825586{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {126},
year = {2013}
}
@article{Reshef2011,
abstract = {Identifying interesting relationships between pairs of variables in large data sets is increasingly important. Here, we present a measure of dependence for two-variable relationships: the maximal information coefficient (MIC). MIC captures a wide range of associations both functional and not, and for functional relationships provides a score that roughly equals the coefficient of determination (R2) of the data relative to the regression function. MIC belongs to a larger class of maximal information-based nonparametric exploration (MINE) statistics for identifying and classifying relationships. We apply MIC and MINE to data sets in global health, gene expression, major-league baseball, and the human gut microbiota and identify known and novel relationships.},
author = {Reshef, D. N. and Reshef, Y. A. and Finucane, H. K. and Grossman, S. R. and McVean, G. and Turnbaugh, P. J. and Lander, E. S. and Mitzenmacher, M. and Sabeti, P. C.},
file = {::},
issn = {0036-8075},
journal = {Science},
keywords = {Large datasets,exploratory analysis},
mendeley-tags = {Large datasets,exploratory analysis},
month = {dec},
number = {6062},
pages = {1518--1524},
title = {{Detecting Novel Associations in Large Data Sets}},
url = {http://www.sciencemag.org/content/334/6062/1518.abstract},
volume = {334},
year = {2011}
}
@article{Road1992,
author = {Road, Oriole},
file = {::},
journal = {differences},
keywords = {181-185,1992,72,annual ryegrass,barley,base d,britannique,can,common vetch,d,dans le sud de,de cultures annuelles b,ensilage n,et moore,forage mixtures,forage yield and quality,g,i,intdrieur de la colombie-,j,l,orge cultivde seule pour,orge en r6gime irrigud,plant sci,production fourragbre de quatre,stout,successions,t,thompson,utilise pas toute},
title = {{Forage production emphasizing barley under irrigation in southern interior British Columbia}},
volume = {185},
year = {1992}
}
@article{Ihaka1996,
abstract = {In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.},
author = {Ihaka, Ross and Gentleman, Robert},
journal = {Journal of Computational and Graphical Statistics},
keywords = {R},
mendeley-tags = {R},
number = {3},
pages = {299 -- 314},
title = {{R: A Language for Data Analysis and Graphics}},
url = {http://www.jstor.org/stable/1390807},
volume = {5},
year = {1996}
}
@article{Kassie2011,
author = {Kassie, Menale},
file = {::},
journal = {Scientist},
number = {November},
title = {{nvironmental Economic and Environmental Benefits of Forage Legume Intercropping in the Mixed ixed Farming System : A Case Study in West}},
year = {2011}
}
@article{Ahmad,
author = {Ahmad, Munir and Khan, Shahjahan},
title = {{Islamic Countries Society of Statistical Sciences}},
url = {www.isoss.net}
}
@article{DIXON2009a,
abstract = {Five methods for discrimination are described, namely Euclidean Distance to centroids (EDC), Linear Discriminant Analysis (LDA) (based on the Mahalanobis distance and pooled variance covariance matrix), Quadratic Discriminant Analysis (QDA) (based on the Mahalanobis distance and individual class variance covariance matrix — non-Bayesian form), Learning Vector Quantization (LVQ) and Support Vector Machines (SVMs) (using soft boundaries and Radial Basis Functions), and illustrated graphically as boundary methods. The performance of each method was determined using four synthetic datasets each consisting of 200 samples half belonging to one of two classes, and a further two synthetic datasets containing 400 samples, again equally split between the two classes. In datasets 1 to 3, five variables were distributed multinormally, in dataset 1 the classes are distributed roughly circularly but with a significant degree of overlap, in dataset 2, the distribution is in elongated hyperellipsoids with small overlap, and in dataset 3 there is a region of complete overlap between classes. In dataset 4 two variables are distributed in a crescent shape. In datasets 5 and 6, 100 variables were generated from multinormal populations, some of which were potential discriminators, however a large proportion of the variables were designed to be uninformative. The methods were optimised using a training set and their performance evaluated using a test set: this was repeated 100 times for different test and training set splits. The average {\%} correctly classified was computed for each class and model, as well as the model stability for each sample (the proportion of times the sample is classified into the same group over all 100 iterations). The conclusions are that the performance of the classifiers depends very much on the distribution of data. Approaches such as LVQ and SVMs that try to determine complex boundaries perform best when the data is not normally distributed such as in dataset 4, but can be prone to overfitting otherwise. QDA tends to perform best on multinormal data although it can be influenced by non-discriminative variables which show a difference in variance. It is recommended to look at the data structure prior to model building to determine the optimal type of model.},
author = {DIXON, S and BRERETON, R},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {boundary methods,classification,euclidean distance,learning vector quantization,linear discriminant analysis,mahalanobis distance,quadratic discriminant analysis,support vector machines},
month = {jan},
number = {1},
pages = {1--17},
title = {{Comparison of performance of five common classifiers represented as boundary methods: Euclidean Distance to Centroids, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Learning Vector Quantization and Support Vector Machines, as dependent on}},
url = {http://dx.doi.org/10.1016/j.chemolab.2008.07.010},
volume = {95},
year = {2009}
}
@article{Zuur2009,
abstract = {1. While teaching statistics to ecologists, the lead authors of this paper have noticed common statistical problems. If a random sample of their work (including scientific papers) produced before doing these courses were selected, half would probably contain violations of the underlying assumptions of the statistical techniques employed. 2. Some violations have little impact on the results or ecological conclusions; yet others increase type I or type II errors, potentially resulting in wrong ecological conclusions. Most of these violations can be avoided by applying better data exploration. These problems are especially troublesome in applied ecology, where management and policy decisions are often at stake. 3. Here, we provide a protocol for data exploration; discuss current tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, double zeros in multivariate analysis, zero inflation in generalized linear modelling, and the correct type of relationships between dependent and independent variables; and provide advice on how to address these problems when they arise. We also address misconceptions about normality, and provide advice on data transformations. 4. Data exploration avoids type I and type II errors, among other problems, thereby reducing the chance of making wrong ecological conclusions and poor recommendations. It is therefore essential for good quality management and policy based on statistical analyses.},
author = {Zuur, Alain F. and Ieno, Elena N. and Elphick, Chris S.},
file = {::},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {data-exploration},
mendeley-tags = {data-exploration},
month = {nov},
number = {9999},
title = {{A protocol for data exploration to avoid common statistical problems}},
type = {Journal article},
url = {http://www3.interscience.wiley.com/cgi-bin/abstract/122683826/ABSTRACT},
volume = {9999},
year = {2009}
}
@article{Gerard1998,
author = {Gerard, Patrick D. and Smith, David R. and Weerakkody, Govinda},
doi = {10.2307/3802357},
file = {::},
issn = {0022541X},
journal = {The Journal of Wildlife Management},
month = {apr},
number = {2},
pages = {801},
title = {{Limits of Retrospective Power Analysis}},
url = {http://www.jstor.org/stable/3802357?origin=crossref},
volume = {62},
year = {1998}
}
@article{Roy2008,
abstract = {While building a predictive quantitative structure-activity relationship (QSAR), validation of the developed model is a very important task. However, a truly new set of data being often unavailable for checking predictability and robustness of the developed model, a typical external validation in QSAR studies is commonly performed by splitting the available data into training and test sets. In the present work we have attempted to explore the impact of training set size on the quality of prediction using different topological descriptors and three different statistical techniques. Three different data sets of moderate size have been used for the present study: cytoprotection data of anti-HIV thiocarbamates (n = 62), HIV reverse transcriptase inhibition data of 1-(2-hydroxyethoxy)methyl-6-(phenylthio)thymine (HEPT) derivatives (n = 107) and bioconcentration factor data of diverse functional compounds (n = 122). In each case, the data set was divided into different combinations of training and test sets maintaining different size ratios in several iterations. In cases of the first two data sets, significant impact of reduction of training set size was found on the predictive ability of the models while the first data set showing higher dependence on the size than the second one. However, in case of modeling of bioconcentration factor, no significant impact of training set size on the quality of prediction could be found. Hence, no general rule can be formulated regarding the impact of training set size on the quality of prediction. Optimum size of the training set should be set based on a particular data set and types of descriptors and statistical analysis being used. 2007 Elsevier B.V. All rights reserved.},
author = {Roy, P P and Leonard, J T and Roy, K},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {fa mlr,k means clusters,pls,qsar,stepwise regression,training set size,validation},
number = {1},
pages = {31--42},
publisher = {Elsevier},
title = {{Exploring the impact of size of training sets for the development of predictive QSAR models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0169743907001529},
volume = {90},
year = {2008}
}
@book{Gentleman2011,
abstract = {Instructional Book for using R for programming},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gentleman, Robert and Hornik, Kurt and Parmigiani, Giovanni G},
booktitle = {Applied Spatial Data Analysis with R},
doi = {10.1007/978-1-4419-7976-6},
eprint = {arXiv:1011.1669v3},
isbn = {9781441979759},
issn = {9780387938363},
pages = {154--197},
pmid = {22057480},
title = {{Use R !}},
year = {2011}
}
@article{Chen2011,
abstract = {Through simulation, Whitlock showed that when all the alternatives have the same effect size, the weighted z-test is superior to both unweighted z-test and Fisher's method when combining P-values from independent studies. In this paper, we show that under the same situation, the generalized Fisher method due to Lancaster outperforms the weighted z-test.},
author = {Chen, Z},
doi = {10.1111/j.1420-9101.2010.02226.x},
file = {::},
issn = {1420-9101},
journal = {Journal of evolutionary biology},
keywords = {Computer Simulation,Data Interpretation, Statistical,Meta-Analysis as Topic},
month = {apr},
number = {4},
pages = {926--30},
pmid = {21401770},
title = {{Is the weighted z-test the best method for combining probabilities from independent tests?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21401770},
volume = {24},
year = {2011}
}
@article{Tarik2014,
author = {Tarik, M and Reda, K M},
doi = {10.1093/annonc/mdm449},
isbn = {0923-7534},
journal = {Bevacizumab Can Be Infused Safely Over 10 Minutes in Metastatic Colorectal Cancer},
pages = {1},
title = {{Annals of Oncology}},
volume = {25},
year = {2014}
}
@article{Chen2001,
abstract = {Suppose that subjects in a population follow the model f(y*∣x*; $\theta$) where y* denotes a response, x* denotes a vector of covariates and $\theta$ is the parameter to be estimated. We consider response-biased sampling, in which a subject is observed with a probability which is a function of its response. Such response-biased sampling frequently occurs in econometrics, epidemiology and survey sampling. The semiparametric maximum likelihood estimate of $\theta$ is derived, along with its asymptotic normality, efficiency and variance estimates. The estimate proposed can be used as a maximum partial likelihood estimate in stratified response- selective sampling. Some computation algorithms are also provided.},
author = {Chen, Kani},
journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
number = {4},
pages = {775 -- 789},
title = {{Parametric Models for Response-Biased Sampling}},
url = {http://www.jstor.org/stable/2680666},
volume = {63},
year = {2001}
}
@article{Kapoor2009a,
abstract = {Epidemiologic data suggest an association between obesity and depression, however findings vary considerably across different studies. Both depression and obesity are disabling disorders associated with loss over appetite control, influenced by genetic and environmental factors and are risk factors for diseases like hypertension, cardiovascular disorders, etc. This study attempts to establish a link between the symptoms of depression, metabolic disorders, and obesity, to unravel the underlying association/s.},
author = {Kapoor, Manav and Kapur, Suman and Mehra, Shipra and Dube, Urvashi and Sharad, Shashwat and Sidhu, Sharda},
file = {::},
issn = {1520-6394},
journal = {Depression and anxiety},
keywords = {Adult,Alleles,Blood Pressure,Blood Pressure: genetics,Body Mass Index,Case-Control Studies,Depressive Disorder,Depressive Disorder: diagnosis,Depressive Disorder: genetics,Depressive Disorder: psychology,Female,Gene Frequency,Gene Frequency: genetics,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Genotype,Heterozygote Detection,Homozygote,Humans,Hypertension,Hypertension: diagnosis,Hypertension: genetics,Hypertension: psychology,India,Leptin,Leptin: genetics,Male,Middle Aged,Obesity,Obesity: genetics,Obesity: psychology,Polymerase Chain Reaction,Polymorphism, Genetic,Polymorphism, Genetic: genetics,Promoter Regions, Genetic,Promoter Regions, Genetic: genetics},
month = {jan},
number = {9},
pages = {791--5},
title = {{Genetic variation in D7S1875 repeat polymorphism of leptin gene is associated with increased risk for depression: a case-control study from India.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19382181},
volume = {26},
year = {2009}
}
@article{Yolcu2011,
author = {Yolcu, Halil},
file = {::},
journal = {Quality},
keywords = {chemical fertilizer,common vetch,leonardite,liquid manure,solid manure,zeolite},
number = {2},
pages = {197--202},
title = {{THE EFFECTS OF SOME ORGANIC AND CHEMICAL FERTILIZER APPLICATIONS ON YIELD , MORPHOLOGY , QUALITY AND MINERAL CONTENT OF COMMON VETCH ( Vicia sativa L .)}},
volume = {16},
year = {2011}
}
@article{Kapoor2009,
abstract = {Epidemiologic data suggest an association between obesity and depression, however findings vary considerably across different studies. Both depression and obesity are disabling disorders associated with loss over appetite control, influenced by genetic and environmental factors and are risk factors for diseases like hypertension, cardiovascular disorders, etc. This study attempts to establish a link between the symptoms of depression, metabolic disorders, and obesity, to unravel the underlying association/s.},
author = {Kapoor, Manav and Kapur, Suman and Mehra, Shipra and Dube, Urvashi and Sharad, Shashwat and Sidhu, Sharda},
file = {::},
issn = {1520-6394},
journal = {Depression and anxiety},
keywords = {Adult,Alleles,Blood Pressure,Blood Pressure: genetics,Body Mass Index,Case-Control Studies,Depressive Disorder,Depressive Disorder: diagnosis,Depressive Disorder: genetics,Depressive Disorder: psychology,Female,Gene Frequency,Gene Frequency: genetics,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Genotype,Heterozygote Detection,Homozygote,Humans,Hypertension,Hypertension: diagnosis,Hypertension: genetics,Hypertension: psychology,India,Leptin,Leptin: genetics,Male,Middle Aged,Obesity,Obesity: genetics,Obesity: psychology,Polymerase Chain Reaction,Polymorphism, Genetic,Polymorphism, Genetic: genetics,Promoter Regions, Genetic,Promoter Regions, Genetic: genetics},
month = {jan},
number = {9},
pages = {791--5},
title = {{Genetic variation in D7S1875 repeat polymorphism of leptin gene is associated with increased risk for depression: a case-control study from India.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19382181},
volume = {26},
year = {2009}
}
@article{Naser2017,
abstract = {Introduction There are a variety of challenges faced by pedestrians when they walk along and attempt to cross a road, as the most recorded accidents occur during this time. Pedestrians of all types, including both sexes with numerous aging groups, are always subjected to risk and are characterized as the most exposed road users. The increased demand for better traffic management strategies to reduce the risks at intersections, improve quality traffic management, traffic volume, and longer cycle time has further increased concerns over the past decade. Method This paper aims to develop a sustainable pedestrian gap crossing index model based on traffic flow density. It focusses on the gaps accepted by pedestrians and their decision for street crossing, where (Log-Gap) logarithm of accepted gaps was used to optimize the result of a model for gap crossing behavior. Through a review of extant literature, 15 influential variables were extracted for further empirical analysis. Subsequently, data from the observation at an uncontrolled mid-block in Jalan Ampang in Kuala Lumpur, Malaysia was gathered and Multiple Linear Regression (MLR) and Binary Logit Model (BLM) techniques were employed to analyze the results. Results and conclusions From the results, different pedestrian behavioral characteristics were considered for a minimum gap size model, out of which only a few (four) variables could explain the pedestrian road crossing behavior while the remaining variables have an insignificant effect. Among the different variables, age, rolling gap, vehicle type, and crossing were the most influential variables. The study concludes that pedestrians' decision to cross the street depends on the pedestrian age, rolling gap, vehicle type, and size of traffic gap before crossing. Practical applications The inferences from these models will be useful to increase pedestrian safety and performance evaluation of uncontrolled midblock road crossings in developing countries.},
author = {Naser, Mohamed M. and Zulkiple, Adnan and Al bargi, Walid A. and Khalifa, Nasradeen A. and Daniel, Basil David},
doi = {10.1016/j.jsr.2017.08.005},
issn = {00224375},
journal = {Journal of Safety Research},
keywords = {Acceptable gap,Mid-block,Modeling,Pedestrian,Road crossing},
pages = {91--98},
title = {{Modeling pedestrian gap crossing index under mixed traffic condition}},
volume = {63},
year = {2017}
}
@article{Lara2005,
abstract = {The influences of biomass burning emissions in the composition of aerosol have been studied during 1 year around the city of Piracicaba (Southeastern Brazil). Inhalable particles, separated in PM2.5 and coarse particulate mode (CPM, with size in the range (2.5{\textless}dp{\textless}10 [mu]m)), were sampled from April 1997 to March 1998 and analyzed for BC, Al, Si, P, S, Cl, K, Ca, Ti, V, Cr, Mn, Fe, Ni, Cu, Zn, Se, Br, Rb, Sr, Zr, Pb. The average concentrations of PM2.5, CPM, BC and chemical elements were statistically higher in the dry season than in the wet season. The results of absolute principal component analysis showed four and three different sources for PM2.5 and CPM, respectively. Sugar-cane burning is the main source of PM2.5 representing 60{\%} of PM2.5, soil dust accounted for 14{\%}, and industries and oil combustion contributed with 12{\%} each one. Resuspended soil is the main source of CPM followed by industrial emissions and sugar-cane burning. The sampling and analytical procedures applied in this study showed that sugar-cane burning and agricultural practices are the main sources of inhalable particles, possibly altering the aerosol composition around the city of Piracicaba.},
author = {LARA, L and ARTAXO, P and MARTINELLI, L and CAMARGO, P and VICTORIA, R and FERRAZ, E},
doi = {10.1016/j.atmosenv.2005.04.026},
file = {::},
isbn = {1352-2310},
issn = {13522310},
journal = {Atmospheric Environment},
keywords = {Aerosol composition,Aerosol source apportionment,Biomass burning,Black carbon,Brazil},
month = {aug},
number = {26},
pages = {4627--4637},
title = {{Properties of aerosols from sugar-cane burning emissions in Southeastern Brazil}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S135223100500395X},
volume = {39},
year = {2005}
}
@article{Lithourgidis2011,
author = {Lithourgidis, A S and Dordas, C A and Damalas, C A and Vlachostergios, D N},
file = {::},
journal = {Crops},
keywords = {agrobiodiversity,crop mixtures,intercropping,monocultures,sustainable agriculture},
number = {4},
pages = {396--410},
title = {{Review article Annual intercrops : an alternative pathway for sustainable agriculture}},
volume = {5},
year = {2011}
}
@article{Kapoor2010,
author = {Kapoor, Manav and Kapur, Suman and Dhaka, L C},
file = {::},
journal = {Journal of Mental Health {\&} Human Behavior},
keywords = {124bp allele,anthropome,d2s2944,depression,obesity},
number = {1},
pages = {24--30},
title = {{D2S2944 marker : A common marker for the obesity-depression associations}},
url = {http://ipsnz.org/journal 2010/Ed 2010{\_}5.PDF},
volume = {15},
year = {2010}
}
@article{Jahansooz2008,
author = {Jahansooz, M R and Chaichi, M R and Mashhadi, H Rahimian and Liaghat, A M},
file = {::},
keywords = {barely,intercropping,nitrogen,vetch,wue},
pages = {23--31},
title = {{Ar ch of of ch}},
volume = {10},
year = {2008}
}
@inproceedings{Pramana2017,
abstract = {Big Data is an umbrella term for explosion in the quantity and diversity of high frequency digital data and it is not usually coming from traditional sources. The speed and frequency by which data is produced and collected — by an increasing number of sources — is responsible for today's data deluge: the amount of available digital data is projected to increase by an annual 40{\%}. “Big Data for Development” is a concept that refers to the identification of sources of Big Data relevant to policy and planning of development programmes. It differs from both “traditional” development data and what the private sector and mainstream media call Big Data. Potential applicability of “Big Data for Development” at the most general level, when it is properly analysed, these new data can provide snapshots of the well-being of populations at high frequency, high degrees of granularity, and from a wide range of angles, narrowing both time and knowledge gaps. This research discussed several possible implementations of Big Data to the official statistics in Indonesia. Furthermore, three case studies would be discussed: (1) predicting inter-city commuting patterns using twitter, (2) developing a statistical model to nowcast food prices using crowdsourcing, and (3) Mobile Position Data for Tourism Statistics. The results show similar trend between crowdsourcing approach and BPS Survey for all commodities, between the twitter approach and the commuter survey 2014. For the MPD approach for tourist statistics, the number of visits based on the roaming and the visits based on the immigration are similar. The study reveals potential implementations of Bigdata in complementing official statistics for government policy in Indonesia.},
author = {Pramana, Setia},
doi = {10.1109/IWBIS.2017.8275097},
isbn = {9781538620380},
pages = {82--86},
publisher = {IEEE},
title = {{Big data for government policy: Potential implementations of bigdata for official statistics in Indonesia}},
url = {http://ieeexplore.ieee.org/document/8275097/},
volume = {5},
year = {2017}
}
@article{Wold2009a,
abstract = {Pell, Ramos and Manne (PRM) in a recent article in this journal claim that the lsquoconventionalrsquo PLS algorithm with orthogonal scores has an inherent inconsistency in that it uses different model spaces for calculating the prediction model coefficients and for calculating the X-space model and it's residuals 1. We disagree with PRM. All PLS model scores, residuals, coefficients, etc., obtained by the conventional PLS algorithm do come from the same underlying latent variable (LV) model, and not from different models or model spaces as PRM suggest. PRM have simply posed a different model with different assumptions and obtained slightly different results, as should have been expected. Copyright 2008 John Wiley {\&} Sons, Ltd.},
author = {Wold, Svante and H{\o}y, Martin and Martens, Harald and Trygg, Johan and Westad, Frank and MacGregor, John and Wise, Barry M},
issn = {08869383},
journal = {Journal of Chemometrics},
keywords = {1,latent variable model,latent variable regression,partial least squares},
number = {2},
pages = {67--68},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{The PLS model space revisited}},
url = {http://doi.wiley.com/10.1002/cem.1171},
volume = {23},
year = {2009}
}
@book{Logan2010,
address = {West Sussex, UK},
annote = {I find this book useful for those still learning what stat to use while teaching themselves R. If you know how to use a dichotomous key and think they are useful in finding a conclusing, then this book could be useful. There are very simple examples with potential to play around with real data that the author suplies. A course could easily be taught from this book. Logan suppies a cheat sheet if you have trouble remembering the code that you must often plug in. The examples are also so easy to use that you can simply copy them. There are explainations for several commonly and currently used statistics along with assumptions one must follow. Each chapter walks you through the analysis starting with testing for assumptions to further justify the use of the statistic.
This book shoud be on the shelf of many beginning scientisits that have only had a course or two in stats and are making an attempt to use R.},
author = {Logan, M},
isbn = {1405190086},
publisher = {Wiley-Blackwell},
title = {{Biostatistical design and analysis using R: a practical guide}},
year = {2010}
}
@article{Sciences,
author = {Sciences, IC Idiong - World Journal of Agricultural and undefined 2007},
file = {::},
journal = {researchgate.net},
title = {{Estimation of farm level technical efficiency in smallscale swamp rice production in cross river state of Nigeria: a stochastic frontier approach}},
url = {https://www.researchgate.net/profile/Idiong{\_}Idiong/publication/241482594{\_}Estimation{\_}of{\_}Farm{\_}Level{\_}Technical{\_}Efficiency{\_}in{\_}Smallscale{\_}Swamp{\_}Rice{\_}Production{\_}in{\_}Cross{\_}River{\_}State{\_}of{\_}Nigeria{\_}A{\_}Stochastic{\_}Frontier{\_}Approach/links/0deec534939c44c3f8000000.pdf}
}
@article{Shriner2011,
abstract = {Principal components analysis of genetic data is used to avoid inflation in type I error rates in association testing due to population stratification by covariate adjustment using the top eigenvectors and to estimate cluster or group membership independent of self-reported or ethnic identities. Eigendecomposition transforms correlated variables into an equal number of uncorrelated variables. Numerous stopping rules have been developed to identify which principal components should be retained. Recent developments in random matrix theory have led to a formal hypothesis test of the top eigenvalue, providing another way to achieve dimension reduction. In this study, I compare Velicer's minimum average partial test to a test on the basis of Tracy-Widom distribution as implemented in EIGENSOFT, the most widely used implementation of principal components analysis in genome-wide association analysis. By computer simulation of vicariance on the basis of coalescent theory, EIGENSOFT systematically overestimates the number of significant principal components. Furthermore, this overestimation is larger for samples of admixed individuals than for samples of unadmixed individuals. Overestimating the number of significant principal components can potentially lead to a loss of power in association testing by adjusting for unnecessary covariates and may lead to incorrect inferences about group differentiation. Velicer's minimum average partial test is shown to have both smaller bias and smaller variance, often with a mean squared error of 0, in estimating the number of principal components to retain. Velicer's minimum average partial test is implemented in R code and is suitable for genome-wide genotype data with or without population labels.Heredity advance online publication, 30 March 2011; doi:10.1038/hdy.2011.26.},
annote = {{\textless}m:note{\textgreater}How many principal components to retain?{\textless}/m:note{\textgreater}},
author = {Shriner, D},
issn = {1365-2540},
journal = {Heredity},
keywords = {PCA,Principal Components},
mendeley-tags = {PCA,Principal Components},
month = {mar},
title = {{Investigating population stratification and admixture using eigenanalysis of dense genotypes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21448230},
year = {2011}
}
@article{R.Kohavi1998,
author = {{R. Kohavi} and {F. Provost}},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Computer Science,Confusion matrix,glossary},
mendeley-tags = {Confusion matrix,glossary},
number = {2},
pages = {271--274--274},
publisher = {Springer Netherlands},
title = {{Glossary of Terms}},
url = {http://www.springerlink.com/content/x105525142v51t20/},
volume = {30},
year = {1998}
}
@article{Pramana2010,
abstract = {IsoGene is an R package for the analysis of dose-response microarray experiments to identify gene or subsets of genes with a monotone relationship between the gene expression and the doses. Several testing procedures (i.e., the likelihood ratio test, Williams, Marcus, the M, and Modified M), that take into account the order restriction of the means with respect to the increasing doses are implemented in the package. The inference is based on resampling methods, both permutations and the Significance Analysis of Microarrays (SAM).},
author = {Pramana, S. and Lin, D. and Haldermans, P. and Shkedy, Z. and Verbeke, T. and G{\"{o}}hlmann, H. and {De Bondt}, A. and Talloen, W. and Bijnens, L.},
issn = {20734859},
journal = {R Journal},
number = {1},
title = {{Isogene: An R package for analyzing dose-response studies in microarray experiments}},
volume = {2},
year = {2010}
}
@article{Sadras2009,
abstract = {This paper focuses on the interaction between genotype and environment, a critical aspect of plant breeding, from a physiological perspective. We present a theoretical framework largely based on Bradshaw's principles of phenotypic plasticity (Adv. Gen. 13: 115) updated to account for recent developments in physiology and genetics. Against this framework we discuss associations between plasticity of yield and plasticity of phenological development. Plasticity was quantified using linear models of phenotype vs environment for 169 wheat lines grown in 6 environments in Mexico, 32 sunflower hybrids grown in at least 15 environments in Argentina and 7 grapevine varieties grown in at least 14 environments in Australia. In wheat, yield ranged from 0.6 to 7.8 t ha-1 and the range of plasticity was 0.74-1.27 for yield and 0.85-1.17 for time to anthesis. The duration of the post-anthesis period as a fraction of the season was the trait with the largest range of plasticity, i.e. 0.47-1.80. High yield plasticity was an undesirable trait as it was associated with low yield in low-yielding environments. Low yield plasticity and high yield in low-yielding environments were associated with three phenological traits: early anthesis, long duration and low plasticity of post-anthesis development. In sunflower, yield ranged from 0.5 to 4.9 t ha-1 and the range of plasticity was 0.72-1.29 for yield and 0.72-1.22 for time to anthesis. High yield plasticity was a desirable trait as it was primarily associated with high yield in high-yielding environments. High yield plasticity and high yield in high-yielding environments were associated with two phenological traits: late anthesis and high plasticity of time to anthesis. In grapevine, yield ranged from 1.2 to 18.7 t ha-1 and the range of plasticity was 0.79-1.29 for yield, 0.86-1.30 for time of budburst, 0.84-1.18 for flowering, and 0.78-1.16 for veraison. High plasticity of yield was a desirable trait as it was primarily associated with high yield in high-yielding environments. High yield plasticity was associated with two phenological traits: plasticity of budburst and plasticity of anthesis. We report for the first time positive associations between plasticities of yield and phenology in crop species. It is concluded that in addition to phenology per se (i.e. mean time to a phenostage), plasticity of phenological development merits consideration as a distinct trait influencing crop adaptation and yield. ?? 2008 Elsevier B.V. All rights reserved.},
annote = {NULL},
author = {Sadras, V. O. and Reynolds, M. P. and de la Vega, A. J. and Petrie, P. R. and Robinson, R.},
journal = {Field Crops Research},
keywords = {Adaptation,Anthesis,Budburst,Environment,Evolution,Genotype,Post-anthesis,Veraison,Yield},
number = {3},
pages = {242--250},
title = {{Phenotypic plasticity of yield and phenology in wheat, sunflower and grapevine}},
volume = {110},
year = {2009}
}
@article{JamesCarifio2007,
annote = {{\textless}m:note{\textgreater}undefined{\textless}/m:note{\textgreater}},
author = {{James Carifio} and {Rocco J. Perla}},
journal = {Journal of Social Sciences},
keywords = {likert,ordinal},
mendeley-tags = {likert,ordinal},
number = {3},
pages = {106--116},
title = {{Ten Common Misunderstandings, Misconceptions, Persistent Myths and Urban Legends about Likert Scales and Likert Response Formats and their Antidotes}},
url = {http://www.scipub.org/fulltext/jss/jss33106-116.pdf},
volume = {3},
year = {2007}
}
@article{Manne2009,
author = {Manne, Rolf and Pell, Randy J and Ramos, L Scott},
issn = {08869383},
journal = {Journal of Chemometrics},
keywords = {bidiagonalization,model inconsistency,nipals,partial least squares},
number = {2},
pages = {76--77},
title = {{The PLS model space: the inconsistency persists}},
url = {http://doi.wiley.com/10.1002/cem.1181},
volume = {23},
year = {2009}
}
@article{Kiers2007,
abstract = {Abstract  Regression tends to give very unstable and unreliable regression weights when predictors are highly collinear. Several methods have been proposed to counter this problem. A subset of these do so by finding components that summarize the information in the predictors and the criterion variables. The present paper compares six such methods (two of which are almost completely new) to ordinary regression: Partial least Squares (PLS), Principal Component regression (PCR), Principle covariates regression, reduced rank regression, and two variants of what is called power regression. The comparison is mainly done by means of a series of simulation studies, in which data are constructed in various ways, with different degrees of collinearity and noise, and the methods are compared in terms of their capability of recovering the population regression weights, as well as their prediction quality for the complete population. It turns out that recovery of regression weights in situations with collinearity is often very poor by all methods, unless the regression weights lie in the subspace spanning the first few principal components of the predictor variables. In those cases, typically PLS and PCR give the best recoveries of regression weights. The picture is inconclusive, however, because, especially in the study with more real life like simulated data, PLS and PCR gave the poorest recoveries of regression weights in conditions with relatively low noise and collinearity. It seems that PLS and PCR are particularly indicated in cases with much collinearity, whereas in other cases it is better to use ordinary regression. As far as prediction is concerned: Prediction suffers far less from collinearity than recovery of the regression weights.},
author = {Kiers, Henk and Smilde, Age},
issn = {1618-2510},
journal = {Statistical Methods and Applications},
keywords = {chemometrics,multilinear,multiway},
mendeley-tags = {chemometrics,multilinear,multiway},
month = {aug},
number = {2},
pages = {193--228},
publisher = {Springer},
title = {{A comparison of various methods for multivariate regression with highly collinear variables}},
type = {Journal article},
url = {http://www.ingentaconnect.com/content/klu/10260/2007/00000016/00000002/00000025 http://www.springerlink.com/content/n4646n445726w229},
volume = {16},
year = {2007}
}
@misc{WarrenS.Sarle,
annote = {{\textless}m:note{\textgreater}undefined{\textless}/m:note{\textgreater}},
author = {{Warren S. Sarle}},
booktitle = {Disseminations of the International Statistical Applications Institute, volume 1, edition 4, 1995, Wichita: ACG Press, pp. 61-66.},
title = {{Measurement theory: Frequently Asked Questions}},
url = {http://www.infra.kth.se/{~}rl/Vetfil/Vetfil{\_}Varia/Sarle{\_}Measurement theory{\_}FAQ.pdf},
urldate = {2010-08-11}
}
@article{Pramana,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 273945780 THE - EFFECTS MODEL DECUBITUS . A . . . Conference CITATIONS 0 READS 28 2 , including : Some : Big Biclustering Setia Karolinska 44 SEE All . The . Abstract . A decubitus wound , is areas of injured skin and tissue . One of promising strategies for healing of decubitus wound is electrical stimulation . The effectiveness of two electrical stimulations (Microcurrent) in decubitus wounds closure was compared . General Linear Mixed - effects Model was used to deal with longitudinal data due to the wounds area were measured repeatedly over time . To get the most suitable model , a number of models with several possible mean effects , variance and serial correlation structures were fitted and compared . The wound closure evolution in both treatments is similar and decreases during the treatment . A wound closure depends on how long it has been treated , but a wound surface area after treatment depends also on the initial area . Therefore , a large wound need more time to heal than a small wound .},
author = {Pramana, Setia and Nurmalasari, Mieke},
keywords = {Linear Mixed Effects Models,Microcurrent,decubitus ulcers},
title = {{THE APPLICATION OF LINEAR MIXED - EFFECTS MODEL TO THE EFFECT OF MICROCURRENT ON DECUBITUS WOUNDS . A STUDY IN LIMBURG PROVINCE OF BELGIUM}}
}
@article{Frank93,
abstract = {Chemometrics is a field of chemistry that studies the application of statistical methods to chemical data analysis. In addition to borrowing many techniques from the statistics and engineering literatures, chemometrics itself has given rise to several new data-analytical methods. This article examines two methods commonly used in chemometrics for predictive modeling-partial least squares and principal components regression-from a statistical perspective. The goal is to try to understand their apparent successes and in what situations they can be expected to work well and to compare them with other statistical methods intended for those situations. These methods include ordinary least squares, variable subset selection, and ridge regression.},
annote = {{\textless}m:note{\textgreater}Methods such as subset regression with variable selection is indicated to give less predictive performance than methods such as PLS and ridge regression that keep all predictor variables (but calculate a compressed projection space).{\textless}/m:note{\textgreater}},
author = {Frank, Ildiko E. and Friedman, Jerome H.},
issn = {00401706},
journal = {Technometrics},
number = {2},
pages = {109--135},
publisher = {American Statistical Association and American Society for Quality},
title = {{A Statistical View of Some Chemometrics Regression Tools}},
type = {Journal article},
url = {http://www.jstor.org/stable/1269656},
volume = {35},
year = {1993}
}
@article{JoeH.Ward1963,
abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale ({\textless}latex{\textgreater}{\$}n {\textgreater} 100{\$}{\textless}/latex{\textgreater}) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n - 1 mutually exclusive sets by considering the union of all possible n(n - 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
author = {{Joe H. Ward}, Jr.},
journal = {Journal of the American Statistical Association},
language = {EN},
month = {nov},
pages = {236--244},
publisher = {American Statistical Association},
title = {{Hierarchical Grouping to Optimize an Objective Function}},
url = {http://www.jstor.org/pss/2282967?searchUrl=/action/doAdvancedSearch?q0=Ward{\&}f0=all{\&}c1=AND{\&}q1={\&}f1=all{\&}wc=on{\&}Search=Search{\&}sd=1963{\&}ed=1963{\&}la={\&}jo={\&}jc.Statistics{\_}JournaloftheAmericanStatisticalAssociation=j100549{\&}Search=yes},
volume = {48},
year = {1963}
}
@article{Stroup2002,
author = {Stroup, W. W.},
doi = {10.1198/108571102780},
file = {::},
issn = {1085-7117},
journal = {Journal of Agricultural, Biological, and Environmental Statistics},
month = {dec},
number = {4},
pages = {491--511},
title = {{Power analysis based on spatial effects mixed models: A tool for comparing design and analysis strategies in the presence of spatial variability}},
url = {http://www.springerlink.com/index/10.1198/108571102780},
volume = {7},
year = {2002}
}
@article{Austin2010,
abstract = {Objective: Statisticians have criticized the use of significance testing to compare the distribution of baseline covariates between treatment groups in randomized controlled trials (RCTs). Furthermore, some have advocated for the use of regression adjustment to estimate the effect of treatment after adjusting for potential imbalances in prognostically important baseline covariates between treatment groups. Study Design and Setting: We examined 114 RCTs published in the New England Journal of Medicine, the Journal of the American Medical Association, The Lancet, and the British Medical Journal between January 1, 2007 and June 30, 2007. Results: Significance testing was used to compare baseline characteristics between treatment arms in 38{\%} of the studies. The practice was very rare in British journals and more common in the U.S. journals. In 29{\%} of the studies, the primary outcome was continuous, whereas in 65{\%} of the studies, the primary outcome was either dichotomous or time-to-event in nature. Adjustment for baseline covariates was reported when estimating the treatment effect in 34{\%} of the studies. Conclusions: Our findings suggest the need for greater editorial consistency across journals in the reporting of RCTs. Furthermore, there is a need for greater debate about the relative merits of unadjusted vs. adjusted estimates of treatment effect. ?? 2010 Elsevier Inc. All rights reserved.},
author = {Austin, Peter C. and Manca, Andrea and Zwarenstein, Merrick and Juurlink, David N. and Stanbrook, Matthew B.},
doi = {10.1016/j.jclinepi.2009.06.002},
isbn = {1878-5921 (Electronic)$\backslash$n0895-4356 (Linking)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Analysis of covariance,Baseline covariates,CONSORT statement,Clinical trial,Randomized controlled trial,Regression adjustment,Significance testing},
number = {2},
pages = {142--153},
pmid = {19716262},
title = {{A substantial and confusing variation exists in handling of baseline covariates in randomized controlled trials: a review of trials published in leading medical journals}},
volume = {63},
year = {2010}
}
@article{Doering1979a,
abstract = {The ordinal-interval controversy contrasts the purists, who oppose application of parametric statistics to ordinal-level data, with the pragmatists, who advocate such application. Geographers have traditionally followed the purists' view, but the pragmatists' view has gained wide acceptance in other disciplines.},
author = {Doering, Thomas R. and Hubbard, Raymond},
journal = {Area},
keywords = {Measurement{\_}level,admissible{\_}statistics,measurement-statistics{\_}debate},
mendeley-tags = {Measurement{\_}level,admissible{\_}statistics,measurement-statistics{\_}debate},
number = {3},
pages = {237 -- 243},
title = {{Measurement and Statistics: The Ordinal-Interval Controversy and Geography}},
url = {http://www.jstor.org/stable/20001475},
volume = {11},
year = {1979}
}
@article{Hofmans2007a,
annote = {{\textless}m:note{\textgreater}This indicates that the number of response categories does not impact on the linearity of the scale. Moreover, there is no relationship between the number of response categories and the sensitivity of the scale as measured by the F ratio for the main effects.{\textless}/m:note{\textgreater}},
author = {Hofmans, Joeri and Theuns, Peter and Mairesse, Olivier},
issn = {1614-1881},
journal = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
keywords = {categories,ordinal},
mendeley-tags = {categories,ordinal},
month = {jan},
number = {4},
pages = {160--169},
title = {{Impact of the Number of Response Categories on Linearity and Sensitivity of Self-Anchoring Scales}},
url = {http://dx.doi.org/10.1027/1614-2241.3.4.160},
volume = {3},
year = {2007}
}
@article{Emanuelli2013,
abstract = {BACKGROUND: The economic importance of grapevine has driven significant efforts in genomics to accelerate the exploitation of Vitis resources for development of new cultivars. However, although a large number of clonally propagated accessions are maintained in grape germplasm collections worldwide, their use for crop improvement is limited by the scarcity of information on genetic diversity, population structure and proper phenotypic assessment. The identification of representative and manageable subset of accessions would facilitate access to the diversity available in large collections. A genome-wide germplasm characterization using molecular markers can offer reliable tools for adjusting the quality and representativeness of such core samples.$\backslash$n$\backslash$nRESULTS: We investigated patterns of molecular diversity at 22 common microsatellite loci and 384 single nucleotide polymorphisms (SNPs) in 2273 accessions of domesticated grapevine V. vinifera ssp. sativa, its wild relative V. vinifera ssp. sylvestris, interspecific hybrid cultivars and rootstocks. Despite the large number of putative duplicates and extensive clonal relationships among the accessions, we observed high level of genetic variation. In the total germplasm collection the average genetic diversity, as quantified by the expected heterozygosity, was higher for SSR loci (0.81) than for SNPs (0.34). The analysis of the genetic structure in the grape germplasm collection revealed several levels of stratification. The primary division was between accessions of V. vinifera and non-vinifera, followed by the distinction between wild and domesticated grapevine. Intra-specific subgroups were detected within cultivated grapevine representing different eco-geographic groups. The comparison of a phenological core collection and genetic core collections showed that the latter retained more genetic diversity, while maintaining a similar phenotypic variability.$\backslash$n$\backslash$nCONCLUSIONS: The comprehensive molecular characterization of our grape germplasm collection contributes to the knowledge about levels and distribution of genetic diversity in the existing resources of Vitis and provides insights into genetic subdivision within the European germplasm. Genotypic and phenotypic information compared in this study may efficiently guide further exploration of this diversity for facilitating its practical use.},
annote = {NULL},
author = {Emanuelli, Francesco and Lorenzi, Silvia and Grzeskowiak, Lukasz and Catalano, Valentina and Stefanini, Marco and Troggio, Michela and Myles, Sean and Martinez-Zapater, Jos{\'{e}} M and Zyprian, Eva and Moreira, Flavia M and Grando, M Stella},
doi = {10.1186/1471-2229-13-39},
file = {::},
isbn = {1471-2229},
issn = {1471-2229},
journal = {BMC plant biology},
keywords = {Genetic Variation,Genetic Variation: genetics,Genotype,Phylogeny,Polymorphism, Single Nucleotide,Polymorphism, Single Nucleotide: genetics,Vitis,Vitis: classification,Vitis: genetics},
pages = {39},
pmid = {23497049},
title = {{Genetic diversity and population structure assessed by SSR and SNP markers in a large germplasm collection of grape.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3610244{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {13},
year = {2013}
}
@article{Leonard2006,
abstract = {The development of predictive QSAR models depends not only on the statistical method but also on the algorithm used for the selection of training and test sets. Here, we describe the validation of QSAR models for three data sets with different sizes (n = 35, 56 and 87) based on random division, sorted biological activity data and K-meam clusters for the factor scores of the original variable matrix along with/without biological activity values. When the training and test sets were generated by random division or by the activity-range algorithm, predictive models were not obtained in most of the cases. In case of random division of the data sets into training and test sets, there is no correlation between internal and external validation statistics. However, good external validation statistics were obtained when training and test sets were selected based on K-means clusters of factor scores of the descriptor space along with/without the biological activity values. So, the selection of training and test sets should be based on the proximity of the representative points of the test set to representative points of the training set in the multidimensional descriptor space. The concept of closeness is based on the general assumption underlying all QSAR theories: similar compounds have similar activities. Thus, if one wishes to validate a QSAR model, the points of the test set must be close to the points of the training set in the multidimensional descriptor space. Based on the results of several methods for the division of the training and test sets, we propose that K-means-cluster- based division of training and prediction sets can be used as a reliable method of division of data set into training and test sets for developing predictive QSAR models. 2006 Wiley-VCH Verlag GmbH {\&} Co. KGaA.},
author = {Leonard, J  Thomas and Roy, Kunal},
issn = {1611020X},
journal = {QSAR Combinatorial Science},
keywords = {10,1002,2005,200510161,accepted,ccr5 antagonists,doi,hiv protease,july 7,k means clusters,mannitol,piperidinyl amides,propylamine,qsar,received,september 30,ureas,validation},
number = {3},
pages = {235--251},
title = {{On Selection of Training and Test Sets for the Development of Predictive QSAR models}},
url = {http://doi.wiley.com/10.1002/qsar.200510161},
volume = {25},
year = {2006}
}
@article{Analysis2017,
author = {Analysis, Trade and Agency, Development},
number = {September},
title = {{A New Paradigm in Trade Governance Strengthen Global Competitiveness}},
year = {2017}
}
@article{Emanuelli2013a,
abstract = {BACKGROUND:The economic importance of grapevine has driven significant efforts in genomics to accelerate the exploitation of Vitis resources for development of new cultivars. However, although a large number of clonally propagated accessions are maintained in grape germplasm collections worldwide, their use for crop improvement is limited by the scarcity of information on genetic diversity, population structure and proper phenotypic assessment. The identification of representative and manageable subset of accessions would facilitate access to the diversity available in large collections. A genome-wide germplasm characterization using molecular markers can offer reliable tools for adjusting the quality and representativeness of such core samples.RESULTS:We investigated patterns of molecular diversity at 22 common microsatellite loci and 384 single nucleotide polymorphisms (SNPs) in 2273 accessions of domesticated grapevine V. vinifera ssp. sativa, its wild relative V. vinifera ssp. sylvestris, interspecific hybrid cultivars and rootstocks. Despite the large number of putative duplicates and extensive clonal relationships among the accessions, we observed high level of genetic variation. In the total germplasm collection the average genetic diversity, as quantified by the expected heterozygosity, was higher for SSR loci (0.81) than for SNPs (0.34). The analysis of the genetic structure in the grape germplasm collection revealed several levels of stratification. The primary division was between accessions of V. vinifera and non-vinifera, followed by the distinction between wild and domesticated grapevine. Intra-specific subgroups were detected within cultivated grapevine representing different eco-geographic groups. The comparison of a phenological core collection and genetic core collections showed that the latter retained more genetic diversity, while maintaining a similar phenotypic variability.CONCLUSIONS:The comprehensive molecular characterization of our grape germplasm collection contributes to the knowledge about levels and distribution of genetic diversity in the existing resources of Vitis and provides insights into genetic subdivision within the European germplasm. Genotypic and phenotypic information compared in this study may efficiently guide further exploration of this diversity for facilitating its practical use.},
annote = {NULL},
author = {Emanuelli, Francesco and Lorenzi, Silvia and Grzeskowiak, Lukasz and Myles, Sean and Grando, M Stella},
doi = {10.1186/1471-2229-13-39},
file = {::;::},
issn = {1471-2229},
journal = {BMC Plant Biology},
number = {1},
pages = {39},
title = {{Genetic diversity and population structure assessed by SSR and SNP markers in a large germplasm collection of grape}},
url = {http://www.biomedcentral.com/1471-2229/13/39},
volume = {13},
year = {2013}
}
@article{Peng2014,
abstract = {BACKGROUND: This study aimed to identify biomarkers for estimating the overall and prostate cancer (PCa)-specific survival in PCa patients at diagnosis.$\backslash$n$\backslash$nMETHODS: To explore the importance of embryonic stem cell (ESC) gene signatures, we identified 641 ESC gene predictors (ESCGPs) using published microarray data sets. ESCGPs were selected in a stepwise manner, and were combined with reported genes. Selected genes were analyzed by multiplex quantitative polymerase chain reaction using prostate fine-needle aspiration samples taken at diagnosis from a Swedish cohort of 189 PCa patients diagnosed between 1986 and 2001. Of these patients, there was overall and PCa-specific survival data available for 97.9{\%}, and 77.9{\%} were primarily treated by hormone therapy only. Univariate and multivariate Cox proportional hazard ratios and Kaplan-Meier plots were used for the survival analysis, and a k-nearest neighbor (kNN) algorithm for estimating overall survival.$\backslash$n$\backslash$nRESULTS: An expression signature of VGLL3, IGFBP3 and F3 was shown sufficient to categorize the patients into high-, intermediate- and low-risk subtypes. The median overall survival times of the subtypes were 3.23, 4.00 and 9.85 years, respectively. The difference corresponded to hazard ratios of 5.86 (95{\%} confidence interval (CI): 2.91-11.78, P{\textless}0.001) for the high-risk subtype and 3.45 (95{\%} CI: 1.79-6.66, P{\textless}0.001) for the intermediate-risk compared with the low-risk subtype. The kNN models that included the gene expression signature outperformed the one designed on clinical parameters alone.$\backslash$n$\backslash$nCONCLUSIONS: The expression signature can potentially be used to estimate overall survival time. When validated in future studies, it could be integrated in the routine clinical diagnostic and prognostic procedure of PCa for an optimal treatment decision based on the estimated survival benefit.},
author = {Peng, Z. and Skoog, L. and Hellborg, H. and Jonstam, G. and Wingmo, I. L. and Hj{\"{a}}lm-Eriksson, M. and Harmenberg, U. and Cedermark, G. C. and Andersson, K. and {\"{A}}hrlund-Richter, L. and Pramana, S. and Pawitan, Y. and Nist{\'{e}}r, M. and Nilsson, S. and Li, C.},
doi = {10.1038/pcan.2013.57},
issn = {13657852},
journal = {Prostate Cancer and Prostatic Diseases},
keywords = {castration therapy,embryonic stem cells,gene expression signature,overall and cancer-specific survival},
pmid = {24394557},
title = {{An expression signature at diagnosis to estimate prostate cancer patients' overall survival}},
year = {2014}
}
@article{ZandScholten2009,
abstract = {Stevens' theory of admissible statistics [Stevens, S. S. (1946). On the theory of scales of measurement. Science, 103, 677680] states that measurement levels should guide the choice of statistical test, such that the truth value of statements based on a statistical analysis remains invariant under admissible transformations of the data. Lord [Lord, F. M. (1953). On the statistical treatment of football numbers. American Psychologist, 8, 750–751] challenged this theory. In a thought experiment, a parametric test is performed on football numbers (identifying players: a nominal representation) to decide whether a sample from the machine issuing these numbers should be considered non-random. This is an apparently illegal test, since its outcomes are not invariant under admissible transformations for the nominal measurement level. Nevertheless, it results in a sensible conclusion: the number-issuing machine was tampered with. In the ensuing measurement-statistics debate Lord's contribution has been influential, but has also led to much confusion. The present aim is to show that the thought experiment contains a serious flaw. First it is shown that the implicit assumption that the numbers are nominal is false. This disqualifies Lord's argument as a valid counterexample to Stevens' dictum. Second, it is argued that the football numbers do not represent just the nominal property of non-identity of the players; they also represent the amount of bias in the machine. It is a question about this property–not a property that relates to the identity of the football players–that the statistical test is concerned with. Therefore, only this property is relevant to Lord's argument. We argue that the level of bias in the machine, indicated by the population mean, conforms to a bisymmetric structure, which means that it lies on an interval scale. In this light, Lord's thought experiment–interpreted by many as a problematic counterexample to Stevens' theory of admissible statistics–conforms perfectly to Stevens' dictum.},
author = {{Zand Scholten}, Annemarie and Borsboom, Denny},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Measurement{\_}level,admissible statistics,admissible{\_}statistics,bisymmetry,measurement level,measurement-statistics debate,measurement-statistics{\_}debate},
mendeley-tags = {Measurement{\_}level,admissible{\_}statistics,measurement-statistics{\_}debate},
month = {apr},
number = {2},
pages = {69--75},
title = {{A reanalysis of Lord's statistical treatment of football numbers}},
url = {http://dx.doi.org/10.1016/j.jmp.2009.01.002},
volume = {53},
year = {2009}
}
@article{Lord1953a,
annote = {A critical commentary to the measurement levels proposed by Stevens, 1946 in Science.
http://www-stat.wharton.upenn.edu/{\~{}}hwainer/Readings/Frederick{\%}20Lord{\_}On{\%}20the{\%}20statistical{\%}20treatment{\%}20of{\%}20football{\%}20numbers.pdf},
author = {Lord, Frederic M.},
issn = {0003-066X},
journal = {American Psychologist},
keywords = {Measurement{\_}level,admissible{\_}statistics,measurement-statistics{\_}debate},
mendeley-tags = {Measurement{\_}level,admissible{\_}statistics,measurement-statistics{\_}debate},
number = {12},
pages = {750--751},
title = {{On the Statistical Treatment of Football Numbers.}},
url = {http://dx.doi.org/10.1037/h0063675},
volume = {8},
year = {1953}
}
@article{Chen2006a,
abstract = {Standard classification algorithms are generally designed to maximize the number of correct predictions (concordance). The criterion of maximizing the concordance may not be appropriate in certain applications. In practice, some applications may emphasize high sensitivity (e.g., clinical diagnostic tests) and others may emphasize high specificity (e.g., epidemiology screening studies). This paper considers effects of the decision threshold on sensitivity, specificity, and concordance for four classification methods: logistic regression, classification tree, Fisher's linear discriminant analysis, and a weighted k-nearest neighbor. We investigated the use of decision threshold adjustment to improve performance of either sensitivity or specificity of a classifier under specific conditions. We conducted a Monte Carlo simulation showing that as the decision threshold increases, the sensitivity decreases and the specificity increases; but, the concordance values in an interval around the maximum concordance are similar. For specified sensitivity and specificity levels, an optimal decision threshold might be determined in an interval around the maximum concordance that meets the specified requirement. Three example data sets were analyzed for illustrations.},
author = {Chen, J J and Tsai, C-A and Moon, H and Ahn, H and Young, J J and Chen, C-H},
issn = {1062-936X},
journal = {SAR and QSAR in environmental research},
keywords = {Algorithms,Animals,Artificial Intelligence,Classification,Colonic Neoplasms,Colonic Neoplasms: genetics,Computer Simulation,Databases,Decision Support Techniques,Decision Trees,Discriminant Analysis,Estrogen,Estrogen: metabolism,Factual,Gene Expression Profiling,Humans,Liver Neoplasms,Liver Neoplasms: chemically induced,Logistic Models,Monte Carlo Method,Receptors,Structure-Activity Relationship,classification,concordance,lda},
mendeley-tags = {classification,concordance,lda},
month = {jun},
number = {3},
pages = {337--52},
publisher = {Taylor {\&} Francis},
title = {{Decision threshold adjustment in class prediction.}},
url = {http://www.informaworld.com/10.1080/10659360600787700},
volume = {17},
year = {2006}
}
@article{Wold1984b,
abstract = {Theuse of partial least squares (PLS) for handling collinearities amongthe independent variables {\$}X{\$} in multiple regression is discussed. Consecutiveestimates {\$}({\{}\backslashtext{\{}rank {\}}{\}}1,2,\backslashcdots ){\$} are obtained using the residuals fromprevious rank as a new dependent variable {\$}y{\$}. The PLSmethod is equivalent to the conjugate gradient method used inNumerical Analysis for related problems.To estimate the "optimal" rank, crossvalidation is used. Jackknife estimates of the standard errors arethereby obtained with no extra computation.The PLS method is comparedwith ridge regression and principal components regression on a chemicalexample of modelling the relation between the measured biological activityand variables describing the chemical structure of a set ofsubstituted phenethylamines. {\textcopyright}1984 Society for Industrial and Applied Mathematics},
annote = {From Duplicate 1 ( 


The Collinearity Problem in Linear Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses


- Wold, S.; Ruhe, A.; Wold, H.; Dunn, W. J.; III )
And Duplicate 2 ( 


The Collinearity Problem in Linear Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses


- Wold, S.; Ruhe, A.; Wold, H.; Dunn, W. J.; III )



Col-linearity and PLS
Theuse of partial least squares (PLS) for handling collinearities amongthe independent variables in multiple regression is discussed.},
author = {Wold, S. and Ruhe, A. and Wold, H. and Dunn, W. J. and III},
journal = {SIAM Journal on Scientific and Statistical Computing},
keywords = {PLS,chemometrics,collinearity,conjugate gradients,cross validation,linear regression,principal components},
mendeley-tags = {PLS},
month = {sep},
number = {3},
pages = {735--743},
publisher = {SIAM},
shorttitle = {SIAM J. Sci. and Stat. Comput.},
title = {{The Collinearity Problem in Linear Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses}},
url = {http://link.aip.org/link/?SCE/5/735/1},
volume = {5},
year = {1984}
}
@article{Thomas1997a,
author = {Thomas, L. and Krebs, C.J.},
file = {::},
journal = {Bulletin of the Ecological Society of America},
number = {2},
pages = {126--138},
publisher = {Citeseer},
title = {{A review of statistical power analysis software}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.8662{\&}rep=rep1{\&}type=pdf},
volume = {78},
year = {1997}
}
@article{Pramana2011,
author = {Pramana, Setia},
pages = {1--30},
title = {{Dose-Response Modeling of Gene Expression Data in Microarray Experiments}},
year = {2011}
}
@book{VeerleVandenEyndenLouiseCorti2011,
abstract = {The information provided in this guide is designed to help researchers and data managers, across a wide range of research disciplines and research environments, produce highest quality research data with the greatest potential for long-term use. All these initiatives involve close liaising with numerous researchers spanning the natural and social sciences and humanities. Author-supplied keywords},
author = {{Veerle Van den Eynden, Louise Corti}, Matthew and Woollard, Libby Bishop and Laurence Horton},
booktitle = {UK Data Archive University of Essex},
isbn = {1904059783},
keywords = {2011 05 03,best practice,confidentiality,consent,copyright,managing data,sharing data,uk data archive},
number = {May},
pages = {1--40},
pmid = {21581},
title = {{Managing and Sharing Data - Best Practice For Researchers}},
url = {http://www.data-archive.ac.uk/media/2894/managingsharing.pdf},
year = {2011}
}
@article{Pairuz,
abstract = {See, stats, and : https : / / www . researchgate. net / publication / 272565760 Proc13thConf Data CITATIONS 0 READS 109 17 , including : Some : Fuzzy Biclustering project Robert Sekolah 16 SEE Setia Karolinska 41 SEE All . The . All - text and , letting . ABSTRACT Spatial analysis is gaining more popularity since Anselin in 1989 defined the newly born field as " The collection of techniques concerning the peculiarities caused by space in the statistical analysis of models on regional sciences " (Bill{\'{e}} and Arbia , 2013) . Many statistical methods and techniques have been developed for the analysis , however there are still few software available , such as GeoDa , GWR , ArcGIS , and several packages of R . Those software still have some limitation . GeoDa , GWR , and ArcGIS are limited to some spatial methods , while R covers more methods but still uses command line interface , which will be difficult for non - expert users . To overcome this limitation , WIRES , a GUI spatial analytical tool , is developed using C{\#} and R as back - end program . WIRES covers more features , spatial weight matrix generator , exploratory spatial data analysis , spatial regression , explanatory spatial data analysis , spatial logistic regression , ordinary and simple Kriging , and other descriptive tools . WIRES also covers methods which have not been covered by other spatial analytical tools , i . e . spatial cluster , spatial regional inequality (using Gini and Theil method) , and spatial shift - share .},
author = {Pairuz, Meidiana and Devianingrum, Hanik and Widityasari, Hergias and Ilmiyah, Zumrotul and Rahayu, Isna and Ria, Arinda and Daniaty, Diah and {Hardi Puspiaji}, Wahyu and {Purnatika Dewi}, Erma and Ismartini, Pudji and Kurniawan, Robert and Baidowi, Sodikin and Romzi, Muchammad and Asikin, Munawar and Pramana, Setia},
keywords = {C{\#},Graphical User Interface,R,Spatial Analysis},
pages = {201--212},
title = {{WIRES : A USER FRIENDLY SPATIAL ANALYSIS SOFTWARE}},
volume = {27}
}
@article{Hansen1990,
abstract = {Several means for improving the performance and training of neural networks for classification are proposed. Crossvalidation is used as a tool for optimizing network parameters and architecture. It is shown that the remaining residual generalization error can be reduced by invoking ensembles of similar networks},
annote = {An early paper on combining classifiers, so-called ensemble or committee classifiers},
author = {Hansen, L.K. and Salamon, P.},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {"ensemble classifier",ensemble},
mendeley-tags = {"ensemble classifier",ensemble},
number = {10},
pages = {993--1001},
title = {{Neural network ensembles}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=58871},
volume = {12},
year = {1990}
}
@article{Thomas1997,
author = {Thomas, L.},
file = {::},
journal = {Conservation Biology},
number = {1},
pages = {276--280},
publisher = {JSTOR},
title = {{Retrospective power analysis}},
url = {http://www.jstor.org/stable/2387304},
volume = {11},
year = {1997}
}
@book{Gama2005a,
abstract = {This paper argues that severe class imbalance is not just an interesting technical challenge that improved learning algorithms will address, it is much more serious. To be useful, a classifier must appreciably outperform a trivial solution, such as choosing the majority class. Any application that is inherently noisy limits the error rate, and cost, that is achievable. When data are normally distributed, even a Bayes optimal classifier has a vanishingly small reduction in the majority classifier's error rate, and cost, as imbalance increases. For fat tailed distributions, and when practical classifiers are used, often no reduction is achieved.},
address = {Berlin, Heidelberg},
author = {Gama, Jo{\~{a}}o and Camacho, Rui and Brazdil, Pavel and Jorge, Al{\'{i}}pio and Torgo, Lu{\'{i}}s and Drummond, Chris and Holte, Robert},
editor = {Gama, Jo{\~{a}}o and Camacho, Rui and Brazdil, Pavel B. and Jorge, Al{\'{i}}pio M{\'{a}}rio and Torgo, Lu{\'{i}}s},
keywords = {"class imbalance"},
mendeley-tags = {"class imbalance"},
pages = {539--546--546},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Machine Learning: ECML 2005}},
url = {http://www.springerlink.com/content/y65851t63086p154/},
volume = {3720},
year = {2005}
}
@article{Naes2011a,
annote = {Could this paper provide a solution for improved modeling of multi-location multi-season crop trait trial-experiments...?},
author = {Naes, T. and Tomic, O. and Mevik, B.-H. and Martens, H.},
issn = {08869383},
journal = {Journal of Chemometrics},
keywords = {Chemometrics,PLS},
mendeley-tags = {Chemometrics,PLS},
month = {jan},
number = {1},
pages = {28--40},
title = {{Path modelling by sequential PLS regression}},
url = {http://doi.wiley.com/10.1002/cem.1357},
volume = {25},
year = {2011}
}
@misc{INAFID2012,
abstract = {Reglamento de la Ley N{\textordmasculine} 29783, Ley de Seguridad y Salud en el Trabajo DECRETO SUPREMO N{\textordmasculine} 005-2012-TR EL PRESIDENTE DE LA REP{\'{U}}BLICA CONSIDERANDO: Que, los derechos a la vida y a la salud se encuentran consagrados en la Constituci{\'{o}}n Pol{\'{i}}tica del Per{\'{u}} y en diversos instrumentos de derechos humanos ratificados por el Per{\'{u}}; Que, a nivel regional, el Per{\'{u}}, como miembro de la Comunidad Andina de Naciones (CAN), cuenta con el Instrumento de Seguridad y Salud en el Trabajo, el cual establece la obligaci{\'{o}}n de los Estados miembros de implementar una pol{\'{i}}tica de prevenci{\'{o}}n de riesgos laborales y vigilar su cumplimiento; el deber de los empleadores de identificar, evaluar, prevenir y comunicar los riesgos en el trabajo a sus trabajadores; y el derecho de los trabajadores a estar informados de los riesgos de las actividades que prestan, entre otros; Que, una pol{\'{i}}tica nacional en seguridad y salud en el trabajo debe crear las condiciones que aseguren el control de los riesgos laborales, mediante el desarrollo de una cultura de la prevenci{\'{o}}n eficaz; en la que los sectores y los actores sociales responsables de crear esas condiciones puedan efectuar una planificaci{\'{o}}n, as{\'{i}} como un seguimiento y control de medidas de seguridad y salud en el trabajo; Que, en este contexto, se ha aprobado la Ley N{\textordmasculine} 29783, Ley de Seguridad y Salud en el Trabajo con el objeto de promover una cultura de prevenci{\'{o}}n de riesgos laborales a trav{\'{e}}s del deber de prevenci{\'{o}}n de los empleadores, el rol de fiscalizaci{\'{o}}n y control del Estado y la participaci{\'{o}}n de los trabajadores y sus organizaciones sindicales, quienes a trav{\'{e}}s del di{\'{a}}logo social, deben velar por la promoci{\'{o}}n, difusi{\'{o}}n y cumplimiento de la normativa sobre la materia;},
author = {INAFID},
booktitle = {Bolet{\'{i}}n Oficial del Estado (citado el 10 de marzo de 2015)},
doi = {https://doi.org/D.S},
number = {01},
title = {{Reglamento de la Ley N{\textordmasculine} 29783, Ley de Seguridad y Salud en el Trabajo.}},
url = {http://www.inabif.gob.pe/portalweb/portal/sst/normativa/ReglamentoLey29783.pdf},
year = {2012}
}
@article{LaRovere2009,
author = {{La Rovere}, R. and Bruggeman, a. and Turkelboom, F. and Aw-Hassan, a. and Thomas, R. and Al-Ahmad, K.},
doi = {10.1177/1070496509333564},
file = {::},
issn = {1070-4965},
journal = {The Journal of Environment {\&} Development},
month = {may},
number = {2},
pages = {107--129},
title = {{Options to Improve Livelihoods and Protect Natural Resources in Dry Environments: The Case of the Khanasser Valley in Syria}},
url = {http://jed.sagepub.com/cgi/doi/10.1177/1070496509333564},
volume = {18},
year = {2009}
}
@article{Levy2002a,
abstract = {The relationship between voting and robust estimation was discussed by Francis Galton in 1907. His two papers in Nature are discussed and reprinted.},
author = {Levy, David M. and Peart, Sandra},
issn = {0048-5829},
journal = {Public Choice},
keywords = {Business and Economics,ensemble},
mendeley-tags = {ensemble},
number = {3},
pages = {357--365--365},
publisher = {Springer Netherlands},
title = {{Galton's Two Papers on Voting as Robust Estimation}},
url = {http://www.springerlink.com/content/pr766273771735xg/},
volume = {113},
year = {2002}
}
@article{Eskandari2009,
author = {Eskandari, Hamdollah and Ghanbari, Ahmad and Javanmard, Abdollah},
file = {::},
keywords = {barley,crude protein,forage quality,maize,mixed cropping,wheat},
number = {1},
pages = {7--13},
title = {{Intercropping of Cereals and Legumes for Forage Production}},
volume = {1},
year = {2009}
}
@book{Daviesa,
abstract = {This paper addresses two types of problems which prove difficult for traditional classifiers: having very limited training data for at least one class, and having classes with a large amount of overlap. Issues discussed will include the (1) use of nearest neighbor methods and neural nets for classification of data which is completely inseparable by linear and quadratic classifiers, (2) dealing with training sets of unequal size from each class},
annote = {{\textless}m:note{\textgreater}Classification with asymmetric internal class structure. ANN, kNN, LDA{\textless}/m:note{\textgreater}},
author = {Davies, P. and Silverstein, B.R.},
booktitle = {1995 International Conference on Acoustics, Speech, and Signal Processing},
pages = {3467--3470},
publisher = {IEEE},
title = {{A comparison of neural nets to statistical stubborn classification problems}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=479732}
}
@article{Schielzeth2009,
abstract = {Mixed-effect models are frequently used to control for the nonindependence of data points, for example, when repeated measures from the same individuals are available. The aim of these models is often to estimate fixed effects and to test their significance. This is usually done by including random intercepts, that is, intercepts that are allowed to vary between individuals. The widespread belief is that this controls for all types of pseudoreplication within individuals. Here we show that this is not the case, if the aim is to estimate effects that vary within individuals and individuals differ in their response to these effects. In these cases, random intercept models give overconfident estimates leading to conclusions that are not supported by the data. By allowing individuals to differ in the slopes of their responses, it is possible to account for the nonindependence of data points that pseudoreplicate slope information. Such random slope models give appropriate standard errors and are easily implemented in standard statistical software. Because random slope models are not always used where they are essential, we suspect that many published findings have too narrow confidence intervals and a substantially inflated type I error rate. Besides reducing type I errors, random slope models have the potential to reduce residual variance by accounting for between-individual variation in slopes, which makes it easier to detect treatment effects that are applied between individuals, hence reducing type II errors as well.},
author = {Schielzeth, Holger and Forstmeier, Wolfgang},
doi = {10.1093/beheco/arn145},
file = {::},
issn = {1465-7279},
journal = {Behavioral ecology : official journal of the International Society for Behavioral Ecology},
month = {jan},
number = {2},
pages = {416--420},
pmid = {19461866},
title = {{Conclusions beyond support: overconfident estimates in mixed models.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2657178{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {20},
year = {2009}
}
@article{Preston2000a,
abstract = {Using a self-administered questionnaire, 149 respondents rated service elements associated with a recently visited store or restaurant on scales that differed only in the number of response categories (ranging from 2 to 11) and on a 101-point scale presented in a different format. On several indices of reliability, validity, and discriminating power, the two-point, three-point, and four-point scales performed relatively poorly, and indices were significantly higher for scales with more response categories, up to about 7. Internal consistency did not differ significantly between scales, but test–retest reliability tended to decrease for scales with more than 10 response categories. Respondent preferences were highest for the 10-point scale, closely followed by the seven-point and nine-point scales. Implications for research and practice are discussed.},
annote = {From Duplicate 1 ( Optimal number of response categories in rating scales: reliability, validity, discriminating power, and respondent preferences - Preston, C )
And Duplicate 2 ( Optimal number of response categories in rating scales: reliability, validity, discriminating power, and respondent preferences - Preston, C )

On several indices of reliability, validity, and discriminating power, the two-point, three-point, and four-point scales performed relatively poorly, and indices were significantly higher for scales with more response categories, up to about 7. Internal consistency did not differ significantly between scales, but test–retest reliability tended to decrease for scales with more than 10 response categories.

From Duplicate 3 ( Optimal number of response categories in rating scales: reliability, validity, discriminating power, and respondent preferences - Preston, C )
And Duplicate 4 ( Optimal number of response categories in rating scales: reliability, validity, discriminating power, and respondent preferences - Preston, C )

undefined},
author = {Preston, C},
issn = {00016918},
journal = {Acta Psychologica},
keywords = {Ordinal,archive,leicester,number of categories,online,research},
mendeley-tags = {Ordinal,number of categories},
month = {mar},
number = {1},
pages = {1--15},
title = {{Optimal number of response categories in rating scales: reliability, validity, discriminating power, and respondent preferences}},
url = {http://dx.doi.org/10.1016/S0001-6918(99)00050-5 http://linkinghub.elsevier.com/retrieve/pii/S0001691899000505},
volume = {104},
year = {2000}
}
@article{Bolisetty2015,
abstract = {
Mohan T. Bolisetty and Gopinath Rajadinakaran contributed equally to this work.
Short-read high-throughput RNA sequencing, though powerful, is limited in its ability to directly measure exon connectivity in mRNAs that contain multiple alternative exons located farther apart than the maximum read length. Here, we use the Oxford Nanopore MinION sequencer to identify 7,899 'full-length' isoforms expressed from four Drosophila genes, Dscam1, MRP, Mhc, and Rdl. These results demonstrate that nanopore sequencing can be used to deconvolute individual isoforms and that it has the potential to be a powerful method for comprehensive transcriptome characterization.},
author = {Bolisetty, Mohan T. and Rajadinakaran, Gopinath and Graveley, Brenton R.},
issn = {1474-760X},
journal = {Genome Biology},
month = {sep},
number = {1},
pages = {204},
title = {{Determining exon connectivity in complex mRNAs by nanopore sequencing}},
url = {http://genomebiology.com/2015/16/1/204},
volume = {16},
year = {2015}
}
@article{Otava2017a,
abstract = {The analysis of transcriptomic experiments with ordered covariates, such as dose-response data, has become a central topic in bioinformatics, in particular in omics studies. Consequently, multiple R packages on CRAN and Bioconductor are designed to analyse microarray data from various perspectives under the assumption of order restriction. We introduce the new R package IsoGene Graphical User Interface (IsoGeneGUI), an extension of the original IsoGene package that includes methods from most of available R packages designed for the analysis of order restricted microarray data, namely orQA, ORIClust, goric and ORCME. The methods included in the new IsoGeneGUI range from inference and estimation to model selection and clustering tools. The IsoGeneGUI is not only the most complete tool for the analysis of order restricted microarray experiments available in R but also it can be used to analyse other types of dose-response data. The package provides all the methods in a user friendly fashion, so analyses can be implemented by users with limited knowledge of R programming.},
author = {Otava, M. and Sengupta, R. and Shkedy, Z. and Lin, D. and Pramana, S. and Verbeke, T. and Haldermans, P. and Hothorn, L.A. and Gerhard, D. and Kuiper, R.M. and Klinglmueller, F. and Kasim, A.},
issn = {20734859},
journal = {R Journal},
number = {1},
title = {{IsoGeneGUI: Multiple approaches for dose-response analysis of microarray data using R}},
volume = {9},
year = {2017}
}
@article{Dixon2009,
abstract = {Four common classification methods are described, Euclidean Distance to Centroids (EDC), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA) and Support Vector Machines (SVM). In many applications of chemometrics e.g. in medicine and biology it is common for there to be unequal sample sizes in different groups. When class sizes are unequal the performance of some of these methods may be biased according to class size. This paper describes approaches for incorporating prior probabilities of class membership using Bayesian approaches to three of the methods LDA, QDA and SVM, either assuming equal probability or assuming that the relative sample sizes relate to the relative probabilities. EDC is used as a benchmark to determine model stabilities. The methods are illustrated by four simulated datasets of different structures and one real dataset consisting of the gas chromatographic profile of mouse urine comparing controls to those on a diet.},
annote = {{\textless}m:note{\textgreater}Asymetric internal class structure{\textless}/m:note{\textgreater}},
author = {Dixon, Sarah J. and Heinrich, Nina and Holmboe, Maria and Schaefer, Michele L. and Reed, Randall R. and Trevejo, Jose and Brereton, Richard G.},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {bayesian methods,classification,linear discriminant analysis,quadratic discriminant analysis,support vector machines},
month = {dec},
number = {2},
pages = {111--120},
title = {{Application of classification methods when group sizes are unequal by incorporation of prior probabilities to three common approaches: Application to simulations and mouse urinary chemosignals}},
url = {http://dx.doi.org/10.1016/j.chemolab.2009.07.016},
volume = {99},
year = {2009}
}
@article{Fisher1936a,
author = {Fisher, Ronald Aylmer},
journal = {Annals of Eugenics, 7: 179-188 (1936)},
keywords = {LDA,Linear Discriminant Analysis},
language = {en},
mendeley-tags = {LDA,Linear Discriminant Analysis},
pages = {179--188},
title = {{The Use of Multiple Measurements in Taxonomic Problems.}},
url = {http://digital.library.adelaide.edu.au/dspace/handle/2440/15227},
volume = {7},
year = {1936}
}
@article{Sidhu2004,
author = {Sidhu, Sharda and Kapoor, Manav},
file = {::},
journal = {Anthropologist},
keywords = {Blood Pressure,Environment,Genes,Genetic,Twin,height,heritability},
number = {3},
pages = {197--200},
title = {{Genetic Analysis of Blood Pressure and Anthropometric Measures Among North Indian Twins}},
url = {http://www.krepublishers.com/02-Journals/T-Anth/Anth-06-0-000-000-2004-Web/Anth-06-3-159-233-2004-Abst-PDF/Anth-06-3-197-200-2004-Sidhu-S/Anth-06-3-197-200-2004-Sidhu-S.pdf},
volume = {6},
year = {2004}
}
@article{Xu,
author = {Xu, X and Economics, SR Jeffrey - Agricultural and undefined 1998},
file = {::},
journal = {Elsevier},
title = {{Efficiency and technical progress in traditional and modern agriculture: evidence from rice production in China}},
url = {http://www.sciencedirect.com/science/article/pii/S0169515098800042}
}
@article{Lord1954a,
annote = {{\textless}m:note{\textgreater}Fredric Lord comments on his previous (1953) commentary to the measurement levels as defined by Stevens (1946).{\textless}/m:note{\textgreater}},
author = {Lord, Frederic M.},
issn = {0003-066X},
journal = {American Psychologist},
keywords = {Measurement{\_}level,admissible{\_}statistics,measurement-statistics{\_}debate},
mendeley-tags = {Measurement{\_}level,admissible{\_}statistics,measurement-statistics{\_}debate},
number = {6},
pages = {264--265},
title = {{Further Comment on "Football Numbers".}},
url = {http://dx.doi.org/10.1037/h0059284},
volume = {9},
year = {1954}
}
@book{Efron1993a,
abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
author = {Efron, Bradley and Tibshirani, Robert J.},
file = {::},
keywords = {bootstrap},
mendeley-tags = {bootstrap},
pages = {436},
publisher = {Chapman {\&} Hall},
title = {{An introduction to the bootstrap}},
url = {http://books.google.com/books?id=gLlpIUxRntoC{\&}pgis=1},
year = {1993}
}
@article{Otava2017,
abstract = {The analysis of transcriptomic experiments with ordered covariates, such as dose-response data, has become a central topic in bioinformatics, in particular in omics studies. Consequently, multiple R packages on CRAN and Bioconductor are designed to analyse microarray data from various perspectives under the assumption of order restriction. We introduce the new R package IsoGene Graphical User Interface (IsoGeneGUI), an extension of the original IsoGene package that includes methods from most of available R packages designed for the analysis of order restricted microarray data, namely orQA, ORIClust, goric and ORCME. The methods included in the new IsoGeneGUI range from inference and estimation to model selection and clustering tools. The IsoGeneGUI is not only the most complete tool for the analysis of order restricted microarray experiments available in R but also it can be used to analyse other types of dose-response data. The package provides all the methods in a user friendly fashion, so analyses can be implemented by users with limited knowledge of R programming.},
author = {Otava, M. and Sengupta, R. and Shkedy, Z. and Lin, D. and Pramana, S. and Verbeke, T. and Haldermans, P. and Hothorn, L.A. and Gerhard, D. and Kuiper, R.M. and Klinglmueller, F. and Kasim, A.},
issn = {20734859},
journal = {R Journal},
number = {1},
title = {{IsoGeneGUI: Multiple approaches for dose-response analysis of microarray data using R}},
volume = {9},
year = {2017}
}
@article{Warton2011,
abstract = {Abstract. The arcsine square root transformation has long been standard procedure when analyzing proportional data in ecology, with applications in data sets containing binomial and non-binomial response variables. Here, we argue that the arcsine transform should not be used in either circumstance. For binomial data, logistic regression has greater interpretability and higher power than analyses of transformed data. However, it is important to check the data for additional unexplained variation, i.e., overdispersion, and to account for it via the inclusion of random effects in the model if found. For non-binomial data, the arcsine transform is undesirable on the grounds of interpretability, and because it can produce nonsensical predictions. The logit transformation is proposed as an alternative approach to address these issues. Examples are presented in both cases to illustrate these advantages, comparing various methods of analyzing proportions including untransformed, arcsine- and logit-transformed linear models and logistic regression (with or without random effects). Simulations demonstrate that logistic regression usually provides a gain in power over other methods.},
author = {Warton, David I and Hui, Francis K. C.},
file = {::},
journal = {Ecology},
keywords = {arcsine transformation,binomial,generalized linear mixed models,logistic regression,logit,overdispersion,power,transformation,type i error},
number = {1},
pages = {3--10},
title = {{The arcsine is asinine: the analysis of proportions in ecology}},
volume = {92},
year = {2011}
}
@article{Libby2010a,
abstract = {Modern institutions face the recurring dilemma of designing accurate evaluation procedures in settings as diverse as academic selection committees, social policies, elections, and figure skating competitions. In particular, it is essential to determine both the number of evaluators and the method for combining their judgments. Previous work has focused on the latter issue, uncovering paradoxes that underscore the inherent difficulties. Yet the number of judges is an important consideration that is intimately connected with the methodology and the success of the evaluation. We address the question of the number of judges through a cost analysis that incorporates the accuracy of the evaluation method, the cost per judge, and the cost of an error in decision. We associate the optimal number of judges with the lowest cost and determine the optimal number of judges in several different scenarios. Through analytical and numerical studies, we show how the optimal number depends on the evaluation rule, the accuracy of the judges, the (cost per judge)/(cost per error) ratio. Paradoxically, we find that for a panel of judges of equal accuracy, the optimal panel size may be greater for judges with higher accuracy than for judges with lower accuracy. The development of any evaluation procedure requires knowledge about the accuracy of evaluation methods, the costs of judges, and the costs of errors. By determining the optimal number of judges, we highlight important connections between these quantities and uncover a paradox that we show to be a general feature of evaluation procedures. Ultimately, our work provides policy-makers with a simple and novel method to optimize evaluation procedures.},
author = {Libby, Eric and Glass, Leon},
editor = {Scalas, Enrico},
file = {::},
issn = {1932-6203},
journal = {PLoS ONE},
keywords = {Biotechnology,Mathematics,Research Article,Science Policy,classifier committee,classifier ensemble},
mendeley-tags = {classifier committee,classifier ensemble},
month = {sep},
number = {9},
pages = {e12642},
publisher = {Public Library of Science},
title = {{The Calculus of Committee Composition}},
url = {http://dx.plos.org/10.1371/journal.pone.0012642},
volume = {5},
year = {2010}
}
@article{Suo2015,
abstract = {MOTIVATION Genome and transcriptome analyses can be used to explore cancers comprehensively, and it is increasingly common to have multiple omics data measured from each individual. Furthermore, there are rich functional data such as predicted impact of mutations on protein coding and gene/protein networks. However, integration of the complex information across the different omics and functional data is still challenging. Clinical validation, particularly based on patient outcomes such as survival, is important for assessing the relevance of the integrated information and for comparing different procedures. RESULTS An analysis pipeline is built for integrating genomic and transcriptomic alterations from whole-exome and RNA sequence data and functional data from protein function prediction and gene interaction networks. The method accumulates evidence for the functional implications of mutated potential driver genes found within and across patients. A driver-gene score (DGscore) is developed to capture the cumulative effect of such genes. To contribute to the score, a gene has to be frequently mutated, with high or moderate mutational impact at protein level, exhibiting an extreme expression and functionally linked to many differentially expressed neighbors in the functional gene network. The pipeline is applied to 60 matched tumor and normal samples of the same patient from The Cancer Genome Atlas breast-cancer project. In clinical validation, patients with high DGscores have worse survival than those with low scores (P = 0.001). Furthermore, the DGscore outperforms the established expression-based signatures MammaPrint and PAM50 in predicting patient survival. In conclusion, integration of mutation, expression and functional data allows identification of clinically relevant potential driver genes in cancer. AVAILABILITY AND IMPLEMENTATION The documented pipeline including annotated sample scripts can be found in http://fafner.meb.ki.se/biostatwiki/driver-genes/. CONTACT yudi.pawitan@ki.se SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.},
author = {Suo, Chen and Hrydziuszko, Olga and Lee, Donghwan and Pramana, Setia and Saputra, Dhany and Joshi, Himanshu and Calza, Stefano and Pawitan, Yudi},
doi = {10.1093/bioinformatics/btv164},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
pmid = {25810432},
title = {{Integration of somatic mutation, expression and functional data reveals potential driver genes predictive of breast cancer survival}},
year = {2015}
}
@article{You2017,
abstract = {Cancer incidence increase has multiple aetiologies. Mutant alleles accumulation in populations may be one of them due to strong heritability of many cancers. The opportunity for the operation of natural selection has decreased in the past {\~{}}150 years because of reduction in mortality and fertility. Mutation-selection balance may have been disturbed in this process and genes providing background for some cancers may have been accumulating in human gene pools. Worldwide, based on the WHO statistics for 173 countries the index of the opportunity for selection is strongly inversely correlated with cancer incidence in peoples aged 0–49 years and in people of all ages. This relationship remains significant when gross domestic product per capita (GDP), life expectancy of older people (e50), obesity, physical inactivity, smoking and urbanization are kept statistically constant for fifteen (15) of twenty-seven (27) individual cancers incidence rates. Twelve (12) cancers which are not correlated with relaxed natural selection after considering the six potential confounders are largely attributable to external causes like viruses and toxins. Ratios of the average cancer incidence rates of the 10 countries with lowest opportunities for selection to the average cancer incidence rates of the 10 countries with highest opportunities for selection are 2.3 (all cancers at all ages), 2.4 (all cancers in 0–49 years age group), 5.7 (average ratios of strongly genetically based cancers) and 2.1 (average ratios of cancers with less genetic background).},
author = {You, Wenpeng and Henneberg, Maciej},
doi = {10.1111/eva.12523},
journal = {Evolutionary Applications},
keywords = {biological state index,cancer heritability,life expectancy,mutations},
month = {aug},
title = {{Cancer incidence increasing globally: The role of relaxed natural selection}},
url = {http://doi.wiley.com/10.1111/eva.12523},
year = {2017}
}
@article{Marquez2018,
abstract = {This document presents a performance analysis on network data traffic channel mathematical modeling using polynomial regression applied to Potential Polynomials of degree one (P1P) to propose an easier computational alternative for traditional methods and even those whose uses neural networks. It proposes an alternative to traditional methods that use neural networks. There are several studies that look into modeling and traffic prediction as auto-regression in movement average (ARIMA) or Wavelet based. In this sense, this paper analyzes the Mean Absolute Error (MAE) and the Mean Square Error (MSE) to compare P1P polynomial regression other mentioned methods. Proofs were realized with a set of 100 sample signals from different network traffics analyzing TCP and UDP packets to compute average MAE and MSE values. Finally, graphics will demonstrate the statistical performance of the proposed method, and comparative tables with different modeling and predictor algorithms will be presented.},
author = {Marquez, Pablo and Pinos, David and Juan, Inga Ortega},
doi = {10.1109/EIConRus.2018.8317034},
isbn = {9781538643396},
journal = {Proceedings of the 2018 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering, ElConRus 2018},
keywords = {ARIMA,MWM,P1P,Traffic Prediction},
pages = {77--82},
publisher = {IEEE},
title = {{Performance comparison in network traffic prediction for polynomial regression to P1P versus ARIMA and MWM}},
volume = {2018-Janua},
year = {2018}
}
@article{Wild2017a,
abstract = {BACKGROUND Understanding the genetic architecture of cardiac structure and function may help to prevent and treat heart disease. This investigation sought to identify common genetic variations associated with inter-individual variability in cardiac structure and function. METHODS A GWAS meta-analysis of echocardiographic traits was performed, including 46,533 individuals from 30 studies (EchoGen consortium). The analysis included 16 traits of left ventricular (LV) structure, and systolic and diastolic function. RESULTS The discovery analysis included 21 cohorts for structural and systolic function traits (n = 32,212) and 17 cohorts for diastolic function traits (n = 21,852). Replication was performed in 5 cohorts (n = 14,321) and 6 cohorts (n = 16,308), respectively. Besides 5 previously reported loci, the combined meta-analysis identified 10 additional genome-wide significant SNPs: rs12541595 near MTSS1 and rs10774625 in ATXN2 for LV end-diastolic internal dimension; rs806322 near KCNRG, rs4765663 in CACNA1C, rs6702619 near PALMD, rs7127129 in TMEM16A, rs11207426 near FGGY, rs17608766 in GOSR2, and rs17696696 in CFDP1 for aortic root diameter; and rs12440869 in IQCH for Doppler transmitral A-wave peak velocity. Findings were in part validated in other cohorts and in GWAS of related disease traits. The genetic loci showed associations with putative signaling pathways, and with gene expression in whole blood, monocytes, and myocardial tissue. CONCLUSION The additional genetic loci identified in this large meta-analysis of cardiac structure and function provide insights into the underlying genetic architecture of cardiac structure and warrant follow-up in future functional studies. FUNDING For detailed information per study, see Acknowledgments.},
author = {Wild, Philipp S. and Felix, Janine F. and Schillert, Arne and Teumer, Alexander and Chen, Ming Huei and Leening, Maarten J.G. and V{\"{o}}lker, Uwe and Gro{\ss}mann, Vera and Brody, Jennifer A. and Irvin, Marguerite R. and Shah, Sanjiv J. and Pramana, Setia and Lieb, Wolfgang and Schmidt, Reinhold and Stanton, Alice V. and Malzahn, D{\"{o}}rthe and Smith, Albert Vernon and Sundstr{\"{o}}m, Johan and Minelli, Cosetta and Ruggiero, Daniela and Lyytik{\"{a}}inen, Leo Pekka and Tiller, Daniel and Smith, J. Gustav and Monnereau, Claire and {Di Tullio}, Marco R. and Musani, Solomon K. and Morrison, Alanna C. and Pers, Tune H. and Morley, Michael and Kleber, Marcus E. and Aragam, Jayashri and Benjamin, Emelia J. and Bis, Joshua C. and Bisping, Egbert and Broeckel, Ulrich and Cheng, Susan and Deckers, Jaap W. and {Del Greco M}, Fabiola and Edelmann, Frank and Fornage, Myriam and Franke, Lude and Friedrich, Nele and Harris, Tamara B. and Hofer, Edith and Hofman, Albert and Huang, Jie and Hughes, Alun D. and K{\"{a}}h{\"{o}}nen, Mika and Kruppa, Jochen and Lackner, Karl J. and Lannfelt, Lars and Laskowski, Rafael and Launer, Lenore J. and Leosdottir, Margr{\'{e}}t and Lin, Honghuang and Lindgren, Cecilia M. and Loley, Christina and MacRae, Calum A. and Mascalzoni, Deborah and Mayet, Jamil and Medenwald, Daniel and Morris, Andrew P. and M{\"{u}}ller, Christian and M{\"{u}}ller-Nurasyid, Martina and Nappo, Stefania and Nilsson, Peter M. and Nuding, Sebastian and Nutile, Teresa and Peters, Annette and Pfeufer, Arne and Pietzner, Diana and Pramstaller, Peter P. and Raitakari, Olli T. and Rice, Kenneth M. and Rivadeneira, Fernando and Rotter, Jerome I. and Ruohonen, Saku T. and Sacco, Ralph L. and Samdarshi, Tandaw E. and Schmidt, Helena and Sharp, Andrew S.P. and Shields, Denis C. and Sorice, Rossella and Sotoodehnia, Nona and Stricker, Bruno H. and Surendran, Praveen and Thom, Simon and T{\"{o}}glhofer, Anna M. and Uitterlinden, Andr{\'{e}} G. and Wachter, Rolf and V{\"{o}}lzke, Henry and Ziegler, Andreas and M{\"{u}}nzel, Thomas and M{\"{a}}rz, Winfried and Cappola, Thomas P. and Hirschhorn, Joel N. and Mitchell, Gary F. and Smith, Nicholas L. and Fox, Ervin R. and Dueker, Nicole D. and Jaddoe, Vincent W.V. and Melander, Olle and Russ, Martin and Lehtim{\"{a}}ki, Terho and Ciullo, Marina and Hicks, Andrew A. and Lind, Lars and Gudnason, Vilmundur and Pieske, Burkert and Barron, Anthony J. and Zweiker, Robert and Schunkert, Heribert and Ingelsson, Erik and Liu, Kiang and Arnett, Donna K. and Psaty, Bruce M. and Blankenberg, Stefan and Larson, Martin G. and Felix, Stephan B. and Franco, Oscar H. and Zeller, Tanja and Vasan, Ramachandran S. and D{\"{o}}rr, Marcus},
doi = {10.1172/JCI84840},
isbn = {0021-9738},
issn = {15588238},
journal = {Journal of Clinical Investigation},
pmid = {28394258},
title = {{Large-scale genome-wide analysis identifies genetic variants associated with cardiac structure and function}},
year = {2017}
}
@article{Grzeskowiak2014a,
abstract = {Natural selection imposed by pathogens is a strong and pervasive evolutionary force structuring genetic diversity within their hosts' genomes and populations. As a model system for understanding the genomic impact of host-parasite coevolution, we have been studying the evolutionary dynamics of disease resistance genes in wild relatives of the cultivated tomato species. In this study, we investigated the sequence variation and evolutionary history of three linked genes involved in pathogen resistance in populations of Solanum peruvianum ( Pto, Fen, and Prf). These genes encode proteins, which form a multimeric complex and together activate defense responses. We used standard linkage disequilibrium, as well as partitioning of linkage disequilibrium components across populations and correlated substitution analysis to identify amino acid positions that are candidates for coevolving sites between Pto/Fen and Prf. These candidates were mapped onto known and predicted structures of Pto, Fen and Prf to visualize putative coevolving regions between proteins. We discuss the functional significance of these coevolving pairs in the context of what is known from previous structure-function studies of Pto, Fen and Prf.},
annote = {NULL},
author = {Grzeskowiak, Lukasz and Stephan, Wolfgang and Rose, Laura E.},
doi = {10.1016/j.meegid.2014.06.019},
issn = {15677257},
journal = {Infection, Genetics and Evolution},
keywords = {Coadaptation,Crop improvement,Disease resistance,Host-parasite interactions,Pto,Solanum peruvianum},
pmid = {24997333},
title = {{Epistatic selection and coadaptation in the Prf resistance complex of wild tomato}},
year = {2014}
}
@article{Spasial2015,
author = {Spasial, Regresi},
number = {September},
pages = {8--10},
title = {{1st ISCO : Konferensi Nasional Statistika 1st ISCO : Konferensi Nasional Statistika}},
year = {2015}
}
@article{DavidOptitz1999,
annote = {{\textless}m:note{\textgreater}undefined{\textless}/m:note{\textgreater}},
author = {{David Optitz}},
journal = {Journal of Artificial Intelligence Research},
keywords = {"Classifier Ensemble"},
pages = {169--198},
title = {{Popular Ensemble Methods: An Empirical Study}},
url = {http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume11/opitz99a-html/paper.html},
volume = {11},
year = {1999}
}
@techreport{CenterforWorkforceStudies2018,
author = {{Center for Workforce Studies}},
number = {April},
pages = {1--252},
title = {{Physician Supply and Demand Through 2030: Key Findings}},
year = {2018}
}
@article{Smith2002,
author = {Smith, Eric P and El-shaarawi, Abdel H and Piegorsch, Walter W},
file = {::},
journal = {Environmetrics},
pages = {141--148},
title = {{BACI design BACI design}},
volume = {1},
year = {2002}
}
@article{Roziqin2017,
abstract = {A prediction of dengue fever cases by using a predictor of rainfall, the rain days, the house index, and the larva-free number has been done in Jember Regency. The evaluation was done by comparing and calculating the deviation value of the predicted number of cases, as a result of the prediction, to the number of actual cases. This prediction simulation of the number of dengue fever cases is using two regression methods, those are Montecarlo linear regression and dynamic polynomial regression. The MSE (mean square error) test was done to find out which regression is the best in predicting dengue fever cases. The data processing result of the two regressions shows that the polynomial dynamic is able to predict well as compared to Montecarlo linear regression, with error level up to 1{\%}. {\textcopyright} 2016 IEEE.},
author = {Roziqin, Mochammad Choirur and Basuki, Achmad and Harsono, Tri},
doi = {10.1109/KCIC.2016.7883649},
isbn = {9781509052318},
journal = {2016 International Conference on Knowledge Creation and Intelligent Computing, KCIC 2016},
keywords = {Jember,dengue fever,prediction,regression},
pages = {213--218},
publisher = {IEEE},
title = {{A comparison of Montecarlo linear and dynamic polynomial regression in predicting dengue fever case}},
year = {2017}
}
@article{Kapoor2012,
author = {Kapoor, Manav and Wang, Jen-Chyong and Bertelsen, Sarah and Bucholz, Kathy and Budde, John P. and Hinrichs, Anthony and Agrawal, Arpana and Brooks, Andrew and Chorlian, David and Dick, Danielle and Hesselbrock, Victor and Foroud, Tatiana and Kramer, John and Kuperman, Samuel and Manz, Niklas and Nurnberger, John and Porjesz, Bernice and Rice, John and Tischfield, Jay and Xuei, Xiaoling and Schuckit, Marc and Edenberg, Howard J. and Bierut, Laura J. and Goate, Alison M.},
editor = {{Le Foll}, Bernard},
file = {::},
issn = {1932-6203},
journal = {PLoS ONE},
month = {mar},
number = {3},
pages = {e33513},
title = {{Variants Located Upstream of CHRNB4 on Chromosome 15q25.1 Are Associated with Age at Onset of Daily Smoking and Habitual Smoking}},
url = {http://dx.plos.org/10.1371/journal.pone.0033513},
volume = {7},
year = {2012}
}
@article{Prelipcean2016,
abstract = {Rooted in the phylosophy of point- and segment-based approaches for transportation mode segmentation of trajectories, the measures that researchers have
adopted to evaluate the quality of the results (1) are incomparable across approaches, hence slowing the progress in the field, and (2) do not provide insight
about the quality of the continuous transportation mode segmentation. To ad-
dress these problems, this paper proposes new error measures that can be applied
to measure how well a continuous transportation mode segmentation model performs. The error measures introduced are based on aligning multiple inferred
continuous intervals to ground truth intervals, and measure the cardinality of the
alignment and the spatial and temporal discrepancy between the corresponding aligned segments. The utility of this new way of computing errors is shown
by evaluating the segmentation of three generic transportation mode segmentation approaches (implicit, explicit–holistic and explicit–consensus-based trans-
port mode segmentation), which can be implemented in a thick client architecture. Empirical evaluations on a large real-word dataset reveal the superiority of
explicit–consensus-based transport mode segmentation, which can be attributed
to the explicit modeling of segments and transitions that allows for a meaningful
decomposition of the complex learning task.},
author = {Prelipcean, Adrian C and Gidofalvi, Gyozo and Susilo, Yusak O},
doi = {10.1080/13658816.2015.1137297},
file = {::},
journal = {International Journal of Geographical Information Science},
keywords = {Continuous Model Evaluation,Error Analysis,Interval Algebra,Trajectory Data Mining,Transportation Mode Segmentation and Detection},
mendeley-tags = {Continuous Model Evaluation,Error Analysis,Interval Algebra,Trajectory Data Mining,Transportation Mode Segmentation and Detection},
title = {{Measures of transport mode segmentation of trajectories}},
url = {http://www.tandfonline.com/doi/full/10.1080/13658816.2015.1137297},
year = {2016}
}
@article{Hassan2011,
author = {Hassan, Zakiya Ahmed and Al-mezori, Hassan and Duhoky, Mosleh M S},
file = {::},
keywords = {1,2011,9-15,a,affects growth and quality,agric 27,characteristics of corn and,citation,corn,duhoky,fertilizer,h,hassan,intercropping,intercropping treatments and nitrogen,m,mezori and m,mixed cropping,multiple cropping,nitrogen,peanut,s,sarhad j,z},
number = {1},
pages = {9--15},
title = {{INTERCROPPING TREATMENTS AND NITROGEN FERTILIZER AFFECTS GROWTH AND QUALITY CHARACTERISTICS OF CORN AND PEANUT}},
volume = {27},
year = {2011}
}
@article{Nannen2003,
author = {Nannen, Volker},
file = {::},
institution = {Rijksuniversiteit Groningen},
journal = {Computer},
publisher = {Citeseer},
title = {{The Paradox of Overfitting}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1.4231{\&}rep=rep1{\&}type=pdf},
volume = {501},
year = {2003}
}
@article{Bachmann2014,
abstract = {Systemic inflammation and sequestration of parasitized erythrocytes are central processes in the pathophysiology of severe Plasmodium falciparum childhood malaria. However, it is still not understood why some children are more at risks to develop malaria complications than others. To identify human proteins in plasma related to childhood malaria syndromes, multiplex antibody suspension bead arrays were employed. Out of the 1,015 proteins analyzed in plasma from more than 700 children, 41 differed between malaria infected children and community controls, whereas 13 discriminated uncomplicated malaria from severe malaria syndromes. Markers of oxidative stress were found related to severe malaria anemia while markers of endothelial activation, platelet adhesion and muscular damage were identified in relation to children with cerebral malaria. These findings suggest the presence of generalized vascular inflammation, vascular wall modulations, activation of endothelium and unbalanced glucose metabolism in severe malaria. The increased levels of specific muscle proteins in plasma implicate potential muscle damage and microvasculature lesions during the course of cerebral malaria.},
author = {Bachmann, Julie and Burt{\'{e}}, Florence and Pramana, Setia and Conte, Ianina and Brown, Biobele J. and Orimadegun, Adebola E. and Ajetunmobi, Wasiu A. and Afolabi, Nathaniel K. and Akinkunmi, Francis and Omokhodion, Samuel and Akinbami, Felix O. and Shokunbi, Wuraola A. and Kampf, Caroline and Pawitan, Yudi and Uhl{\'{e}}n, Mathias and Sodeinde, Olugbemiro and Schwenk, Jochen M. and Wahlgren, Mats and Fernandez-Reyes, Delmiro and Nilsson, Peter},
doi = {10.1371/journal.ppat.1004038},
isbn = {5212011337},
issn = {15537374},
journal = {PLoS Pathogens},
pmid = {24743550},
title = {{Affinity Proteomics Reveals Elevated Muscle Proteins in Plasma of Children with Cerebral Malaria}},
year = {2014}
}
@inproceedings{AlBargi2017,
abstract = {{\textcopyright} The Authors, published by EDP Sciences, 2017. Road crossings are considered as an unavoidable part of walking in which the desirable route of pedestrians interacts with vehicles. These interactions may expose the pedestrians to risks or delays. In Malaysia, road accident statistics show that pedestrian casualties are fairly high. Inappropriate gap acceptance when pedestrians cross roads is a main contributing element to this situation. In this context, the purpose of this study was to develop realistic models for pedestrian road crossing behaviour using the regression technique for mid-block street crossing. A choice model was produced to capture the decision making process of pedestrians whereas rejected or accepted vehicular gaps was based on the discrete choice theory. Gap acceptance data under real mix traffic conditions was collected using video camera on a typical unsignalised two lane one way urban street section in the city center of Kuala Lumpur, Malaysia. The lognormal regression model developed for the crossing behaviour model shows that traffic speed, pedestrian waiting time, gender, crossing distance, age group, frequency of attempts and pedestrian number are the significant factors which are able to predict 77.0{\%} of variance or changes in accepted gap size at 0.05 significance level. Higher traffic speed, lower waiting time, being a male, wider crossing distance, older age group, lower frequency of attempts and higher number of pedestrian were found to influence pedestrians to accept a bigger gap size. The binary logistic regression developed for the crossing choice model was found to be influenced by traffic speed, driver yield, pedestrian number and age group. Furthermore, lower traffic speed, willingness of drivers to slow down, more pedestrian crossings at the same time and a younger age group lead to a higher chance or probability of crossing roads. The model was validated again using 100 isolated samples and an accuracy of 98{\%} was obtained compared to the calibrated model which yielded an accuracy of 98.9{\%}.},
author = {{Al Bargi}, W.A. and {David Daniel}, B. and Prasetijo, J. and Rohani, M.M. and {Mohamad Nor}, S.N.},
booktitle = {MATEC Web of Conferences},
doi = {10.1051/matecconf/201710308003},
issn = {2261236X},
title = {{Crossing Behaviour of Pedestrians Along Urban Streets in Malaysia}},
volume = {103},
year = {2017}
}
@article{Siti,
abstract = {See, stats, and : https : / / www . researchgate. net / publication / 271447069 Proc13thConf - FAST Data CITATIONS 0 READS 32 12 , including : Some : Big Biclustering project Widyo Statistics 4 SEE Setia Karolinska 41 SEE All . The . All - text and , letting . ABSTRACT Performing standard and advanced statistical analysis using statistical packages and interpreting the results or outputs is still a hurdle faced by beginners . Knowledge related to the use of the statistical packages are often derived from tutorials or books and the internet , which is very broad and dispersed . Furthermore , there was no good knowledge management on statistical analysis that can be used as a reference by the beginners . The results of the analysis still become personal knowledge , since no media can spread them . To overcome the limitation , a web - based forum named FAST is built to provide a discussion board that make the dissemination of knowledge to be faster . In addition , this forum also provides a web - based statistical applications built with the shiny framework as the user interface and R as back - end - program that can be used to analyze the data that has been provided or uploaded by the user . FAST currently provides several statistical analysis such as regression (linear , logistic , ridge , and tobit regression) , ARIMA , survival data analysis , as well as cluster analysis . Users can place the results of the analysis conducted on the application to the discussion forum and gallery of analysis to have feedback and discussion from other users . In addition , users can also create reports of the results of the analysis conducted .},
author = {Siti, Dwiyana and Dalimunthe, Meilany and Tomika, Debi and Rahmawati, Eka Miftakhul and Burhan, Muchriana and Fauzi, Yayan and Romzy, Muchammad and Arcana, I Made and Machdi, Imam and Bustaman, Usman and Buana, Widyo Pura and Pramana, Setia},
keywords = {Internet forum,R,shiny framework,web - based statistical analysis},
pages = {185--200},
title = {{FAST : A WEB - BASED STATISTICAL ANALYSIS FORUM}},
volume = {27}
}
@article{Rose2011,
abstract = {Studies combining comparative genomics and information on biochemical pathways have revealed that protein evolution can be affected by the amount of pleiotropy associated with a particular gene. The amount of pleiotropy, in turn, can be a function of the position at which a gene operates in a pathway and the pathway structure. Genes that serve as convergence points and have several partners (so-called hubs) often show the greatest constraint and hence the slowest rate of protein evolution. In this article, we have studied five genes (Pto, Fen, Rin4, Prf and Pfi) in a defence signalling network in a wild tomato species, Solanum peruvianum. These proteins operate together and contribute to bacterial resistance in tomato. We predicted that Prf (and possibly Pfi), which serves as a convergence point for upstream signals, should show greater evolutionary constraint. However, we found instead that two of the genes which potentially interact with pathogen ligands, Rin4 and Fen, have evolved under strong evolutionary constraint, whereas Prf and Pfi, which probably function further downstream in the network, show evidence of balancing selection. This counterintuitive observation may be probable in pathogen defence networks, because pathogens may target positions throughout resistance networks to manipulate or nullify host resistance, thereby leaving a molecular signature of host-parasite co-evolution throughout a single network.},
annote = {NULL},
author = {Rose, Laura E and Grzeskowiak, Lukasz and H{\"{o}}rger, Anja C and Groth, Martin and Stephan, Wolfgang},
doi = {10.1111/j.1364-3703.2011.00720.x},
isbn = {1364-3703},
issn = {1364-3703},
journal = {Molecular plant pathology},
keywords = {Disease Resistance,Disease Resistance: genetics,Disease Resistance: physiology,Evolution, Molecular,Genetic Variation,Genetic Variation: genetics,Lycopersicon esculentum,Lycopersicon esculentum: genetics,Lycopersicon esculentum: metabolism,Lycopersicon esculentum: microbiology,Lycopersicon esculentum: parasitology,Plant Proteins,Plant Proteins: genetics,Plant Proteins: metabolism},
number = {9},
pages = {1--7},
pmid = {21726387},
title = {{Targets of selection in a disease resistance network in wild tomatoes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21726387},
volume = {12},
year = {2011}
}
@book{LudmilaI.Kuncheva2004,
address = {Hoboken, New Jersey},
annote = {{\textless}m:note{\textgreater}undefined{\textless}/m:note{\textgreater}},
author = {{Ludmila I. Kuncheva}},
keywords = {"classifier ensembles"},
language = {English},
mendeley-tags = {"classifier ensembles"},
pages = {376},
publisher = {John Wiley {\&} Sons},
title = {{Combining Pattern Classifiers: Methods and Algorithms}},
year = {2004}
}
@article{YEN2009,
abstract = {For classification problem, the training data will significantly influence the classification accuracy. However, the data in real-world applications often are imbalanced class distribution, that is, most of the data are in majority class and little data are in minority class. In this case, if all the data are used to be the training data, the classifier tends to predict that most of the incoming data belongs to the majority class. Hence, it is important to select the suitable training data for classification in the imbalanced class distribution problem. In this paper, we propose cluster-based under-sampling approaches for selecting the representative data as training data to improve the classification accuracy for minority class and investigate the effect of under-sampling methods in the imbalanced class distribution environment. The experimental results show that our cluster-based under-sampling approaches outperform the other under-sampling techniques in the previous studies.},
annote = {{\textless}m:note{\textgreater}Imbalanced training data distribution, unbalanced classification{\textless}/m:note{\textgreater}},
author = {YEN, S and LEE, Y},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {"unbalanced classification",classification,data mining,imbalanced data distribution,under-sampling},
mendeley-tags = {"unbalanced classification"},
month = {apr},
number = {3},
pages = {5718--5727},
title = {{Cluster-based under-sampling approaches for imbalanced data distributions}},
url = {http://dx.doi.org/10.1016/j.eswa.2008.06.108},
volume = {36},
year = {2009}
}
@article{Johnson2004,
abstract = {Recently, researchers in several areas of ecology and evolution have begun to change the way in which they analyze data and make biological inferences. Rather than the traditional null hypothesis testing approach, they have adopted an approach called model selection, in which several competing hypotheses are simultaneously confronted with data. Model selection can be used to identify a single best model, thus lending support to one particular hypothesis, or it can be used to make inferences based on weighted support from a complete set of competing models. Model selection is widely accepted and well developed in certain fields, most notably in molecular systematics and mark-recapture analysis. However, it is now gaining support in several other areas, from molecular evolution to landscape ecology. Here, we outline the steps of model selection and highlight several ways that it is now being implemented. By adopting this approach, researchers in ecology and evolution will find a valuable alternative to traditional null hypothesis testing, especially when more than one hypothesis is plausible.},
author = {Johnson, Jerald B and Omland, Kristian S},
doi = {10.1016/j.tree.2003.10.013},
file = {::},
issn = {0169-5347},
journal = {Trends in ecology {\&} evolution},
month = {feb},
number = {2},
pages = {101--8},
pmid = {16701236},
title = {{Model selection in ecology and evolution.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16701236},
volume = {19},
year = {2004}
}
@article{Towner2007,
author = {Towner, Mary C. and Luttbeg, Barney},
doi = {10.1002/evan.20134},
file = {::},
issn = {10601538},
journal = {Evolutionary Anthropology: Issues, News, and Reviews},
keywords = {akaike information,bayesian statistics,human behavioral ecology,inference,likelihood statistics,model comparison,null hypothesis testing,p-values,significance,statistical,testing},
month = {jun},
number = {3},
pages = {107--118},
title = {{Alternative statistical approaches to the use of data as evidence for hypotheses in human behavioral ecology}},
url = {http://doi.wiley.com/10.1002/evan.20134},
volume = {16},
year = {2007}
}
@article{Lin2018,
author = {Lin, Yuan and Yu, Hongzhi},
title = {{Sentiment analysis : a comparison of deep learning neural network algorithm with SVM and naϊve Bayes for Indonesian text Sentiment analysis : a comparison of deep learning neural network algorithm with SVM and naϊve Bayes for Indonesian text}},
year = {2018}
}
@misc{Schapire1990a,
abstract = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.},
annote = {{\textless}m:note{\textgreater}Early paper on combining classifiers, so-called classifier ensemble or committee{\textless}/m:note{\textgreater}},
author = {Schapire, Robert E.},
booktitle = {Machine Learning},
issn = {0885-6125},
keywords = {"ensemble classifier","ensemble",Computer Science},
mendeley-tags = {"ensemble classifier","ensemble"},
number = {2},
pages = {197--227--227},
publisher = {Springer Netherlands},
title = {{The Strength of Weak Learnability}},
url = {http://www.springerlink.com/content/k86t840228382v28/},
volume = {5},
year = {1990}
}
@article{Vu2016,
abstract = {Molecular classification of breast cancer into clinically relevant subtypes helps improve prognosis and adjuvant-treatment decisions. The aim of this study is to provide a better characterization of the molecular subtypes by providing a comprehensive landscape of subtype-specific isoforms including coding, long non-coding RNA and microRNA transcripts. Isoform-level expression of all coding and non-coding RNAs is estimated from RNA-sequence data of 1168 breast samples obtained from The Cancer Genome Atlas (TCGA) project. We then search the whole transcriptome systematically for subtype-specific isoforms using a novel algorithm based on a robust quasi-Poisson model. We discover 5451 isoforms specific to single subtypes. A total of 27{\%} of the subtype-specific isoforms have better accuracy in classifying the intrinsic subtypes than that of their corresponding genes. We find three subtype-specific miRNA and 707 subtype-specific long non-coding RNAs. The isoforms from long non-coding RNAs also show high performance for separation between Luminal A and Luminal B subtypes with an AUC of 0.97 in the discovery set and 0.90 in the validation set. In addition, we discover 1500 isoforms preferentially co-expressed in two subtypes, including 369 isoforms co-expressed in both Normal-like and Basal subtypes, which are commonly considered to have distinct ER-receptor status. Finally, analyses at protein level reveal four subtype-specific proteins and two subtype co-expression proteins that successfully validate results from the isoform level.},
author = {Vu, Trung Nghia and Pramana, Setia and Calza, Stefano and Suo, Chen and Lee, Donghwan and Pawitan, Yudi},
keywords = {RNA sequencing,breast cancer,non-coding RNAs,subtype co-expression,subtype-specific isoforms},
title = {{Comprehensive landscape of subtype-specific coding and non- coding RNA transcripts in breast cancer}},
year = {2016}
}
@article{Nadeem,
author = {Nadeem, Muhammad and Ansar, M and Anwar, Adeel and Hussain, Ashiq},
file = {::},
journal = {Agriculture},
keywords = {avena sativa,hordeum vulgare,mixed cropping,pakistan,performance,triticum aestivum,vicia sativa},
number = {1},
pages = {181--192},
title = {{PERFORMANCE OF WINTER CEREAL-LEGUMES FODDER MIXTURES AND THEIR PURE STAND AT DIFFERENT GROWTH STAGES UNDER RAINFED CONDITIONS OF}},
volume = {48}
}
@article{Piquerez2014,
abstract = {One of the great challenges for food security in the 21st century is to improve yield stability through the development of disease-resistant crops. Crop research is often hindered by the lack of molecular tools, growth logistics, generation time and detailed genetic annotations, hence the power of model plant species. Our knowledge of plant immunity today has been largely shaped by the use of models, specifically through the use of mutants. We examine the importance of Arabidopsis and tomato as models in the study of plant immunity and how they help us in revealing a detailed and deep understanding of the various layers contributing to the immune system. Here we describe examples of how knowledge from models can be transferred to economically important crops resulting in new tools to enable and accelerate classical plant breeding. We will also discuss how models, and specifically transcriptomics and effectoromics approaches, have contributed to the identification of core components of the defense response which will be key to future engineering of durable and sustainable disease resistance in plants.},
author = {Piquerez, Sophie J M and Harvey, Sarah E and Beynon, Jim L and Ntoukakis, Vardis},
doi = {10.3389/fpls.2014.00671},
isbn = {1664-462x},
issn = {1664-462X},
journal = {Frontiers in plant science},
keywords = {Arabidopsis,Crop engineering,Disease resistance,Food security,Model,Tomato},
pmid = {25520730},
title = {{Improving crop disease resistance: lessons from research on Arabidopsis and tomato.}},
year = {2014}
}
@article{Vasilakoglou2008,
author = {Vasilakoglou, I. and Dhima, K. and Lithourgidis, a. and Eleftherohorinos, I.},
doi = {10.1017/S0014479708006728},
file = {::},
issn = {0014-4797},
journal = {Experimental Agriculture},
month = {oct},
number = {04},
pages = {509},
title = {{Competitive Ability of Winter Cereal–Common Vetch Intercrops Against Sterile Oat}},
url = {http://www.journals.cambridge.org/abstract{\_}S0014479708006728},
volume = {44},
year = {2008}
}
@article{Cortes1995,
abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
annote = {{\textless}m:note{\textgreater}Support Vector Machine (SVM) introduced here.{\textless}/m:note{\textgreater}},
author = {Cortes, Corinna and Vapnik, Vladimir},
file = {::},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Computer Science,SVM,Support Vector Machine},
mendeley-tags = {SVM,Support Vector Machine},
month = {sep},
number = {3},
pages = {273--297},
publisher = {Springer Netherlands},
title = {{Support-vector networks}},
url = {http://www.springerlink.com/content/k238jx04hm87j80g/},
volume = {20},
year = {1995}
}
@article{Koufteros2005,
author = {Koufteros, Xenophon and Vonderembse, Mark and Jayaram, Jayanth},
doi = {10.1111/j.1540-5915.2005.00067.x},
file = {::},
issn = {0011-7315},
journal = {Decision Sciences},
keywords = {Contingency Theory,Integration,New Product Development,Structural Equation Modeling},
month = {feb},
number = {1},
pages = {97--133},
publisher = {Wiley/Blackwell (10.1111)},
title = {{Internal and External Integration for Product Development: The Contingency Effects of Uncertainty, Equivocality, and Platform Strategy}},
url = {http://doi.wiley.com/10.1111/j.1540-5915.2005.00067.x},
volume = {36},
year = {2005}
}
@article{Lozano2008,
annote = {{\textless}m:note{\textgreater}The number of response options employed ranges from two to nine. The results show that as the number of response alternatives increases, both reliability and validity improve. The optimum number of alternatives is between four and seven.{\textless}/m:note{\textgreater}},
author = {Lozano, Luis M. and Garc{\'{i}}a-Cueto, Eduardo and Mu{\~{n}}iz, Jos{\'{e}}},
issn = {1614-1881},
journal = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
keywords = {number of categories,ordinal},
mendeley-tags = {number of categories,ordinal},
month = {jan},
number = {2},
pages = {73--79},
title = {{Effect of the Number of Response Categories on the Reliability and Validity of Rating Scales}},
url = {http://dx.doi.org/10.1027/1614-2241.4.2.73},
volume = {4},
year = {2008}
}
@article{Science2007,
author = {Science, Biological},
file = {::},
journal = {Network},
keywords = {intercropping,mixture ratio,oat,vetch},
number = {2},
pages = {14--19},
title = {{THE ROLE OF INTERCROPPING ON YIELD POTENTIAL OF COMMON VETCH ( Vicia sativa L .)/ OAT ( Avena sativa L .) CULTIVATED}},
volume = {2},
year = {2007}
}
@article{Tang1999,
annote = {{\textless}m:note{\textgreater}... the optimal scale for maximizing confidence in relevance judgments has approximately seven points.{\textless}/m:note{\textgreater}},
author = {Tang, Rong and Shaw,, William M. and Vevea, Jack L.},
issn = {0002-8231},
journal = {Journal of the American Society for Information Science},
keywords = {categories,ordinal},
language = {en},
mendeley-tags = {categories,ordinal},
month = {jan},
number = {3},
pages = {254--264},
title = {{Towards the identification of the optimal number of relevance categories}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-4571(1999)50:3{\%}3C254::AID-ASI8{\%}3E3.0.CO;2-Y/abstract},
volume = {50},
year = {1999}
}
@inproceedings{MiekeNurmalasariandSetiaPramana2014,
abstract = {See, stats, and : https : / / www . researchgate. net / publication / 272565757 ANALYSIS BELGIUM TECHNIQUES Conference CITATIONS 0 READS 69 2 , including : Some : Big Biclustering project Setia Karolinska 48 SEE All . The.ABSTRACTAnappendectomyisthesurgicalremovalofthevermiformappendixnormallyperformedasanemergency,whenthepatientissufferingfromacuteappendicitis.Theanalysisofappendectomyrelatedtothegeographicaldistributionofdiseasehospital-admissionsinBelgiumwasstillunderstudied.Thisstudyisaimedtoidentifygeographicaldifferencesinmedicalpractices,andtoinvestigatespatialandtemporaldistributiononappendectomyandincidentalappendectomyincidencerateinBelgiumforperiod2001to2006.Twodifferentmethodswereappliedtoidentifypossiblehighincidenceregions,usedmapsofthenon-smoothedSIRsandBayesianmethodstosmoothSIRs.UsingBayesianmethod,thebestmodel(basedonthesmallestvalueofDIC)forappendectomyandincidentalappendectomycasesarePoisson-LognormalandPoisson-Gamma,respectively.Therangeofmeanrelativerisksforappendectomycaseswassmoothedbetween0.81and1.32,andtherangeofmeanrelativerisksforincidentalappendectomywassmoothedbetween0.32and2.67.Basedonthesemodels,wecanconcludethatthemodelofsmoothedSIRs(meanrelativerisks)ofappendectomyandincidentalappendectomycasesamongdistrictsinBelgiumfor2001-2006periodsarenotrelatedwiththeenvironment.},
author = {{Mieke Nurmalasari and Setia Pramana}},
title = {{ANALYSIS OF APPENDECTOMY IN BELGIUM USING DISEASE MAPPING TECHNIQUES}},
volume = {27},
year = {2014}
}
@article{Shook1989,
abstract = {Approaches to disease control are prioritized. Genetic improvement could reduce need for treatment and culling but would not reduce the need for proper management and sanitation. Results of several studies indicate that disease incidence and cost increases with selection for milk yield. The large array of disease resistance mechanisms in animals suggests a large number of loci are involved in disease resistance. A few loci, e.g., the major histocompatibility complex, may account for a major portion of genetic variance in disease. Rate of genetic gain from selection for a major locus alone or in combination with performance is discussed. Four criteria for including traits in a breeding program are outlined, and each is discussed with respect to disease. In spite of low heritabilities for disease traits, genetic variation for disease incidence is economically important and justifies including disease in breeding programs. An industry-wide standard for recording and accumulating field data for disease is lacking. Institutional relationships among segments of the animal breeding and animal health industries are needed to facilitate genetic improvement for disease resistance.},
annote = {NULL},
author = {Shook, G E},
doi = {10.3168/jds.S0022-0302(89)79242-0},
institution = {Dairy Science Department, University of Wisconsin, Madison 53706.},
issn = {0022-0302},
journal = {Journal of dairy science},
number = {5},
pages = {1349--1362},
pmid = {2663944},
title = {{Selection for disease resistance.}},
volume = {72},
year = {1989}
}
@article{Scott1986a,
abstract = {There has been a great deal of interest in recent years in fitting logistic and log-linear models to tables of population counts estimated from survey data. Since maximum likelihood methods are not available in general for complex survey designs, most work has concentrated on adapting standard methods developed for multinominal sampling. Maximum likelihood methods have been developed for some special designs, however, and we might expect ad hoc methods to be less efficient in these cases. We compare the two approaches in the important special case of fitting logistic regression models under case-control or choice-based sampling, where the population is stratified by values of the (categorical) response variable.},
author = {Scott, A. J. and Wild, C. J.},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
number = {2},
pages = {170 -- 182},
title = {{Fitting Logistic Models Under Case-Control or Choice Based Sampling}},
url = {http://www.jstor.org/stable/2345712},
volume = {48},
year = {1986}
}
